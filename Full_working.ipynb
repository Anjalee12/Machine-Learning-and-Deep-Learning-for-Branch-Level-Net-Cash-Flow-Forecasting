{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15a4e3-a812-4a8c-b0b1-d2014064ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, TimeSeriesSplit,\n",
    "                                     RandomizedSearchCV, GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import (Dense, LSTM, Dropout, Embedding, Reshape, Concatenate,\n",
    "                                     RepeatVector, Bidirectional, Layer, Add, LayerNormalization,\n",
    "                                     Multiply, Lambda, MultiHeadAttention, GlobalAveragePooling1D,\n",
    "                                     BatchNormalization, Conv1D, Activation, ReLU)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperModel, RandomSearch, Objective\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "from tcn import TCN\n",
    "import pickle\n",
    "import optuna\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Add, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1b871-c9e9-4429-94f0-e3f6e4c2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c471c27-6a39-49bc-a64b-02c8ddb35751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0159fd-637c-40d0-98c4-ec5b6077b15c",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a5f33-f6a3-4e03-99fa-b623f212c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TXNDATE to datetime format\n",
    "df['TXNDATE'] = pd.to_datetime(df['TXNDATE'], dayfirst=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40b12a-d592-4571-b1b0-699ce018e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in ascending order\n",
    "df = df.sort_values(by=['BRANCHID', 'TXNDATE']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81683979-257d-4796-acfb-68cfbafad89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week (0 = Monday, 6 = Sunday)\n",
    "df['DayOfWeek'] = df['TXNDATE'].dt.dayofweek\n",
    "\n",
    "# Is weekend? (Saturday = 5, Sunday = 6)\n",
    "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Month\n",
    "df['Month'] = df['TXNDATE'].dt.month\n",
    "\n",
    "# Year\n",
    "df['Year'] = df['TXNDATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebcdd7-a750-41fd-becf-5b9b7e214e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View new features\n",
    "df[['TXNDATE', 'DayOfWeek', 'IsWeekend', 'Month', 'Year']].head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab6bc2-de47-4f34-aa11-676ec4d6f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load holiday list from Excel\n",
    "holidays_df = pd.read_csv(\"holidays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47e899-a6b6-407b-95a6-f1170d5c2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'DATE' column to datetime format\n",
    "holidays_df['DATE'] = pd.to_datetime(holidays_df['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974495af-e29e-44d8-ae91-25c094052942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a holiday list as a set for faster lookup\n",
    "holiday_dates = set(holidays_df['DATE'])\n",
    "\n",
    "# Create 'IsHoliday' column: 1 if date is a holiday, else 0\n",
    "df['IsHoliday'] = df['TXNDATE'].isin(holiday_dates).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcc778-7a69-4663-ae3e-a08b66b900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TXNDATE', 'IsHoliday']].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db195429-1bba-44a4-b66a-96ed1f48b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'IsNonWorkingDay' column: 1 if date is a NonWorking Day, else 0\n",
    "df['IsNonWorkingDay'] = ((df['IsWeekend'] == 1) | (df['IsHoliday'] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f598790-b29c-44f9-9f05-7591a5e795d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee25a1-84f2-4edf-a3f6-490e80cea5c7",
   "metadata": {},
   "source": [
    "### Lag Features (Past Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a38cbe-6543-4e86-9448-f249c6aef51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lag1_Credit'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].shift(1)\n",
    "df['Lag1_Debit'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].shift(1)\n",
    "df['Lag1_Customers'] = df.groupby('BRANCHID')['CUSTOMER'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad835bf-9b7d-481c-b0a2-0abb4f957f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [1,2, 3, 7, 14]:\n",
    "    df[f'Lag{lag}_Credit'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].shift(lag)\n",
    "    df[f'Lag{lag}_Debit'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].shift(lag)\n",
    "    df[f'Lag{lag}_Customers'] = df.groupby('BRANCHID')['CUSTOMER'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b568e06-9607-416e-8f06-7f4b8d9b596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TOTALCREDITAMOUNT', 'TOTALDEBITAMOUNT', 'CUSTOMER',\n",
    "    'Lag1_Credit', 'Lag1_Debit', 'Lag1_Customers', \n",
    "    'Lag2_Credit', 'Lag2_Debit', 'Lag2_Customers' ,\n",
    "    'Lag3_Credit', 'Lag3_Debit', 'Lag3_Customers' ,\n",
    "    'Lag7_Credit', 'Lag7_Debit', 'Lag7_Customers',\n",
    "    'Lag14_Credit', 'Lag14_Debit', 'Lag14_Customers'\n",
    "   ]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753093e-475b-4f1c-8b1d-1d909a431f46",
   "metadata": {},
   "source": [
    "### Rolling Statistics (Short-term & Long-term Trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e47c4-7216-4914-a642-02851a55f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling features for debits\n",
    "df['Rolling7_DebitMean'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    "df['Rolling7_DebitStd'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(window=7).std())\n",
    "\n",
    "# Rolling features for customers\n",
    "df['Rolling7_CustomersMean'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    "df['Rolling7_CustomersStd'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(window=7).std())\n",
    "\n",
    "# Rolling features for credit\n",
    "df['Rolling7_CreditMean'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    "df['Rolling7_CreditStd'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(window=7).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c6176-267c-41ca-99ee-5d0655cbd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6421828-9e5c-49f9-aa45-ef94aab58198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-day rolling averages\n",
    "df['Rolling3_CreditMean'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "df['Rolling3_DebitMean'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "df['Rolling3_CustomersMean'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "\n",
    "# 3-day rolling standard deviation\n",
    "df['Rolling3_CustomersStd'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(3).std())\n",
    "df['Rolling3_CreditStd'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(3).std())\n",
    "df['Rolling3_DebitStd'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(3).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b0bdb-a391-4bfb-b962-37312b42e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30-day rolling averages\n",
    "df['Rolling30_CreditMean'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(30).mean())\n",
    "df['Rolling30_DebitMean'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(30).mean())\n",
    "df['Rolling30_CustomersMean'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(30).mean())\n",
    "\n",
    "# 30-day rolling standard deviation\n",
    "df['Rolling30_CustomersStd'] = df.groupby('BRANCHID')['CUSTOMER'].transform(lambda x: x.shift(1).rolling(30).std())\n",
    "df['Rolling30_CreditStd'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].transform(lambda x: x.shift(1).rolling(30).std())\n",
    "df['Rolling30_DebitStd'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].transform(lambda x: x.shift(1).rolling(30).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df6427-c63e-4e0a-8580-e92564428d69",
   "metadata": {},
   "source": [
    "### Combined Features (Lag & Rolling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b5739-302c-4667-a8e7-0d76d3672a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Credit', 'Debit', 'Customers']\n",
    "rolling_windows = [3, 7]\n",
    "\n",
    "for target in target_cols:\n",
    "    lag_col = f'Lag1_{target}'\n",
    "    \n",
    "    for window in rolling_windows:\n",
    "        mean_col = f'Rolling{window}_{target}Mean'\n",
    "        std_col = f'Rolling{window}_{target}Std'\n",
    "        \n",
    "        # Difference from mean\n",
    "        df[f'{lag_col}_vs_Mean{window}'] = df[lag_col] - df[mean_col]\n",
    "        \n",
    "        # Ratio to mean (avoid division by 0)\n",
    "        df[f'{lag_col}_Ratio_Mean{window}'] = df[lag_col] / (df[mean_col] + 1e-6)\n",
    "        \n",
    "        # Z-score (standardized deviation)\n",
    "        df[f'{lag_col}_ZScore{window}'] = (df[lag_col] - df[mean_col]) / (df[std_col] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fe299-10f8-4829-81e4-0763fa74ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef688148-ece9-4f1f-98a5-ab823fc20c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by date per branch\n",
    "df = df.sort_values(['BRANCHID', 'TXNDATE'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d1e8-d1d5-4c00-a133-6882a21acd9b",
   "metadata": {},
   "source": [
    "### Ratios & Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfdf3e-04bb-4ac5-84ed-a026c2f3ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much of the credit was withdrawn\n",
    "df['Debit_to_Credit_Ratio'] = df['TOTALDEBITAMOUNT'] / (df['TOTALCREDITAMOUNT'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f4dce-b9e9-4bb0-a48b-5d13e5d5ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cash in or out per customers visited\n",
    "df['CashOut_Per_Customer'] = df['TOTALDEBITAMOUNT'] / (df['CUSTOMER'] + 1)\n",
    "df['CashIn_Per_Customer'] = df['TOTALCREDITAMOUNT'] / (df['CUSTOMER'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632656b-a8c3-4a84-95eb-207397bc09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Net Cash Flow (Shows overall surplus or deficit of cash that day) - Response variable\n",
    "df['NetCashFlow'] = df['TOTALCREDITAMOUNT'] - df['TOTALDEBITAMOUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27996da5-c121-4372-9805-9381ecb9d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Cash Flow per Customer\n",
    "df['NetCashFlow_Per_Customer'] = df['NetCashFlow'] / (df['CUSTOMER'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7727068-f653-4a62-b6cc-7b3299abe0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-over-Day Deltas\n",
    "df['Delta_Debit'] = df.groupby('BRANCHID')['TOTALDEBITAMOUNT'].diff(1)\n",
    "df['Delta_Credit'] = df.groupby('BRANCHID')['TOTALCREDITAMOUNT'].diff(1)\n",
    "df['Delta_Customers'] = df.groupby('BRANCHID')['CUSTOMER'].diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdccf3-547a-4830-a723-f4f79e35f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Delta vs Mean (Z-score type feature) : helps to detect spikes or drops\n",
    "df['Debit_ZScore'] = (df['TOTALDEBITAMOUNT'] - df['Rolling7_DebitMean']) / (df['Rolling7_DebitStd'] + 1)\n",
    "df['Credit_ZScore'] = (df['TOTALCREDITAMOUNT'] - df['Rolling7_CreditMean']) / (df['Rolling7_CreditStd'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78d0ff-5597-4759-b538-4371735e742a",
   "metadata": {},
   "source": [
    "### Time Positioning Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cdc8a-1c10-4ab6-872f-3402eb088c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IsMonthStart'] = df['TXNDATE'].dt.is_month_start.astype(int)\n",
    "df['IsMonthEnd'] = df['TXNDATE'].dt.is_month_end.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c55b59-c8da-4b31-b6d6-0083158a9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and last 5 days of the month\n",
    "\n",
    "# Ensure TXNDATE is datetime\n",
    "df['TXNDATE'] = pd.to_datetime(df['TXNDATE'])\n",
    "\n",
    "# Day of the month\n",
    "df['DayOfMonth'] = df['TXNDATE'].dt.day\n",
    "\n",
    "# Number of days in the month\n",
    "df['DaysInMonth'] = df['TXNDATE'].dt.days_in_month\n",
    "\n",
    "# Is it in the first 5 days of the month?\n",
    "df['IsFirst5Days'] = (df['DayOfMonth'] <= 5).astype(int)\n",
    "\n",
    "# Is it in the last 5 days of the month?\n",
    "df['IsLast5Days'] = (df['DayOfMonth'] > (df['DaysInMonth'] - 5)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652c1e6-fe1c-46b4-b645-7db42148a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inflation dataset\n",
    "inflation = pd.read_csv(\"inflation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9664e5f-ebed-4ff2-bb41-a29d7085f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inflation 'Month' is zero-padded to match datetime format\n",
    "inflation['Month'] = inflation['Month'].astype(int)\n",
    "inflation['Year'] = inflation['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60743fa-4bd5-49ef-b72e-389691a4f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the inflation data into main dataframe\n",
    "df = df.merge(inflation, on=['Year', 'Month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d7ea7-4e9d-4bf8-a5ae-a4d40bd11eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing inflation values\n",
    "missing_inflation = df[df['NCPI_Index'].isna()]\n",
    "print(f\"Rows with missing inflation data: {len(missing_inflation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb88af-46c2-487a-8331-53dc5cbd00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6ce6ce4-24a6-42df-8bf6-a715e6c73659",
   "metadata": {},
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('processed_branch_cash_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713287ed-1705-4175-9a64-85428a6a60fb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a292fb-c935-458e-9cca-6bbf09f6425c",
   "metadata": {},
   "source": [
    "### 1. Date Parsing & Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7cdeb-3a76-4f46-9281-da65dfae59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TXNDATE\"] = pd.to_datetime(df[\"TXNDATE\"], format=\"%d/%m/%Y\")\n",
    "df.sort_values([\"BRANCHID\", \"TXNDATE\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918356c-e142-456c-96a9-b47fe5b309db",
   "metadata": {},
   "source": [
    "### 2. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fa0ac-1275-4e59-8aa3-f51d9217961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "for col in [\"BRANCH\", \"DISTRICT\", \"PROVINCE\", \"CODE\"]:\n",
    "    df_[col] = LabelEncoder().fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb0dac-d144-4694-84c8-dd3c80121586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf4f7d-d1c2-4ea3-bfe8-145aaeecb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99143e3b-7701-4aaf-8db3-2c034df6f073",
   "metadata": {},
   "source": [
    "### 3. Handle Missing Values "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f45fe59-6465-4f8f-86b3-f2bbcb3de70c",
   "metadata": {},
   "source": [
    "drop rows with NA in rolling/lag/z-score columns, which are usually at the start of each BRANCHID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d69f77-a19a-4327-94b5-4294204254a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0a3d2-99ca-4367-8ba6-4767c5b6a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d9b00-d631-4dd4-8597-6fea3d5cc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_lag_cols = [\n",
    "    \"Lag1_Credit\",\t\"Lag1_Debit\",\t\"Lag1_Customers\",\t\"Lag2_Credit\",\t\"Lag2_Debit\",\t\"Lag2_Customers\",\t\n",
    "    \"Lag3_Credit\",\t\"Lag3_Debit\",\t\"Lag3_Customers\",\t\"Lag7_Credit\",\t\"Lag7_Debit\",\t\"Lag7_Customers\",\t\n",
    "    \"Lag14_Credit\",\t\"Lag14_Debit\",\t\"Lag14_Customers\",\n",
    "    \"Rolling7_DebitMean\", \"Rolling7_DebitStd\", \"Rolling7_CustomersMean\", \"Rolling7_CustomersStd\", \"Rolling7_CreditMean\", \"Rolling7_CreditStd\",\n",
    "    \"Rolling3_CreditMean\", \"Rolling3_DebitMean\", \"Rolling3_CustomersMean\", \"Rolling3_CustomersStd\", \"Rolling3_CreditStd\", \"Rolling3_DebitStd\",\n",
    "    \"Rolling30_CreditMean\", \"Rolling30_DebitMean\", \"Rolling30_CustomersMean\", \"Rolling30_CustomersStd\", \"Rolling30_CreditStd\", \"Rolling30_DebitStd\",\n",
    "    \"Lag1_Credit_vs_Mean3\",\t\"Lag1_Credit_Ratio_Mean3\",\t\"Lag1_Credit_ZScore3\",\t\"Lag1_Credit_vs_Mean7\",\t\"Lag1_Credit_Ratio_Mean7\",\t\n",
    "    \"Lag1_Credit_ZScore7\",\t\"Lag1_Debit_vs_Mean3\",\t\"Lag1_Debit_Ratio_Mean3\",\t\"Lag1_Debit_ZScore3\",\t\"Lag1_Debit_vs_Mean7\",\t\"Lag1_Debit_Ratio_Mean7\",\t\n",
    "    \"Lag1_Debit_ZScore7\",\t\"Lag1_Customers_vs_Mean3\",\t\"Lag1_Customers_Ratio_Mean3\",\t\"Lag1_Customers_ZScore3\",\t\"Lag1_Customers_vs_Mean7\",\t\n",
    "    \"Lag1_Customers_Ratio_Mean7\",\t\"Lag1_Customers_ZScore7\", \"Delta_Debit\", \"Delta_Credit\", \"Delta_Customers\", \"Debit_ZScore\", \"Credit_ZScore\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0ec72-2e48-4c68-98af-a88e9049134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_.dropna(subset=rolling_lag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2427ac-b78f-4e4f-a849-15852a90f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab9019-311c-47e8-86e7-9c8fc80a7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fc23e-0d58-42e9-adc6-7d5bc1071170",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82879493-6a72-4e00-8f79-a19c3132044d",
   "metadata": {},
   "source": [
    "Training: 1st April 2022 to 31st Dec 2024\n",
    "\n",
    "Test/Validation: 1st Jan 2025 to 31st March 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccf485-c441-4c23-9180-ef6a8fc4b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "df_cleaned[\"TXNDATE\"] = pd.to_datetime(df_cleaned[\"TXNDATE\"])\n",
    "\n",
    "split_date = pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "train_df = df_cleaned[df_cleaned[\"TXNDATE\"] < split_date].copy()\n",
    "test_df = df_cleaned[df_cleaned[\"TXNDATE\"] >= split_date].copy()\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f420a9-b067-4ee5-82cc-8213467a6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target \n",
    "target_col = \"NetCashFlow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d127f4-2bd6-46e8-a2de-f6b3d514d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the \"NetCashFlow\"\n",
    "print(df_cleaned[\"NetCashFlow\"].describe())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0e3fee6-4c24-4546-938a-1ed94b4ec677",
   "metadata": {},
   "source": [
    "| Statistic       | Value              | Interpretation                                        |\n",
    "| --------------- | ------------------ | ----------------------------------------------------- |\n",
    "| **Count**       | 165,707            | Rows in your dataset                                  |\n",
    "| **Mean**        | 365,607 LKR        | Average net cash flow                                 |\n",
    "| **Std Dev**     | 2,899,138 LKR      | Extremely high spread — indicates huge variability    |\n",
    "| **Min**         | -202.5 million LKR | Massive outlier                                       |\n",
    "| **25% (Q1)**    | -64,289 LKR        | 25% of values are below this (many negatives)         |\n",
    "| **Median (Q2)** | 72,850 LKR         | 50% are below this — much smaller than mean           |\n",
    "| **75% (Q3)**    | 719,028 LKR        | 75% are below this                                    |\n",
    "| **Max**         | +135.5 million LKR | Another massive outlier                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe433f2a-ad33-447d-8d56-68c4eda438a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\", \"NetCashFlow_Per_Customer\", \"TOTALTXNAMOUNT\"]\n",
    "feature_cols = [col for col in train_df.columns if col not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa0958-8994-418f-add3-749bed85bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test1 = test_df[feature_cols]\n",
    "y_test = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde2552-6230-4791-8ce9-0b97905c068e",
   "metadata": {},
   "source": [
    "## Correlation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffb264-9cc7-457e-b2b9-aa572745e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix on X_train1\n",
    "cor_matrix = X_train1.corr().abs()\n",
    "\n",
    "# Identify pairs with high correlation\n",
    "high_corr_pairs = []\n",
    "for i in range(len(cor_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if cor_matrix.iloc[i, j] > 0.8:\n",
    "            f1 = cor_matrix.columns[i]\n",
    "            f2 = cor_matrix.columns[j]\n",
    "            high_corr_pairs.append((f1, f2, cor_matrix.iloc[i, j]))\n",
    "\n",
    "# Sort and display\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature_1', 'Feature_2', 'Correlation'])\n",
    "high_corr_df = high_corr_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "print(\"Highly Correlated Feature Pairs (|correlation| > 0.8):\")\n",
    "print(high_corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fb194-4f4e-4dd8-9137-06df23fc026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation of each feature with target\n",
    "feature_target_corr = X_train1.corrwith(y_train).abs()\n",
    "\n",
    "# Drop the feature with lower correlation to target\n",
    "to_drop = []\n",
    "for f1, f2, _ in high_corr_pairs:\n",
    "    if f1 in to_drop or f2 in to_drop:\n",
    "        continue  \n",
    "    if feature_target_corr[f1] < feature_target_corr[f2]:\n",
    "        to_drop.append(f1)\n",
    "    else:\n",
    "        to_drop.append(f2)\n",
    "\n",
    "print(\"\\nFeatures to drop due to high multicollinearity:\")\n",
    "print(to_drop)\n",
    "\n",
    "# Drop from training and test sets\n",
    "X_train = X_train1.drop(columns=to_drop)\n",
    "X_test = X_test1.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb60e27-e957-4eb8-9ea9-e519f4355286",
   "metadata": {},
   "source": [
    "## Fitting and Testing the Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03852f14-064d-4d59-b8a0-3f2103a44bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic XGBoost model\n",
    "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78697ce6-0a6e-4f30-bf5e-56f58b9b3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Basic Model Evaluation on Test Set:\")\n",
    "print(f\"  RMSE: {rmse:,.2f}\")\n",
    "print(f\"  MAE : {mae:,.2f}\")\n",
    "print(f\"  R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa84d8f1-9463-44b1-8257-bb73f7a232c2",
   "metadata": {},
   "source": [
    "Basic XGB Model Results:\n",
    "  RMSE: 1,106,413.84\n",
    "  MAE : 113,193.56\n",
    "  R²  : 0.9210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e9cd3-8376-4ea6-b4b8-1963819425c5",
   "metadata": {},
   "source": [
    "# Randomized search using all featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b0305-96fd-48fd-8adc-2f8c526285bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 950, 1200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],   \n",
    "    'reg_lambda': [1, 5, 10]    \n",
    "}\n",
    "\n",
    "# TimeSeriesSplit (on training set only)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Model\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "     random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Final Test Set Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "\n",
    "print(\"\\n Best Parameters from CV:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "032837bb-e7fe-4852-b745-7984001dbe43",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    " Final Test Set Results:\n",
    " RMSE: 967,309.49\n",
    " MAE : 90,820.51\n",
    " R²  : 0.9396\n",
    "\n",
    " Best Parameters from CV:\n",
    "{'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 1200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b41acf-0c7b-4187-a7e4-0e3d63ccabad",
   "metadata": {},
   "source": [
    "### Improved randomised search - All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936358e-18cc-4307-a612-1105ec85a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(800, 1500),             \n",
    "    'learning_rate': uniform(0.01, 0.1),             \n",
    "    'max_depth': randint(3, 8),                     \n",
    "    'subsample': uniform(0.6, 0.4),                  \n",
    "    'colsample_bytree': uniform(0.6, 0.4),           \n",
    "    'reg_alpha': uniform(0, 1),                      \n",
    "    'reg_lambda': uniform(1, 9)                      \n",
    "}\n",
    "\n",
    "# TimeSeriesSplit (on training set only)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Model\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "     random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Final Test Set Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "\n",
    "print(\"\\n Best Parameters from CV:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24367639-3b7e-44b1-9580-397fbe79d365",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    " Final Test Set Results:\n",
    " RMSE: 967,309.49\n",
    " MAE : 90,820.51\n",
    " R²  : 0.9396\n",
    "\n",
    " Best Parameters from CV:\n",
    "{'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 1200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04692a-5f9c-4f23-aac1-834ef956cc56",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with GridSearchCV (Considering all features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcb5c3-ef1a-4d4f-8196-e5366c9b4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [1150, 1200, 1250],\n",
    "    'max_depth': [4,5,6],\n",
    "    'learning_rate': [0.05,0.07],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha': [ 0.1, 1],   \n",
    "    'reg_lambda': [1, 5]    \n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  \n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"\\n Final Evaluation on Test Set:\")\n",
    "print(f\"   RMSE: {rmse_best:,.2f}\")\n",
    "print(f\"   MAE : {mae_best:,.2f}\")\n",
    "print(f\"   R²  : {r2_best:.4f}\")\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f91d73ca-418a-4cdb-a609-9a8f9cdaf248",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
    "\n",
    " Final Evaluation on Test Set:\n",
    "   RMSE: 967,208.17\n",
    "   MAE : 90,450.14\n",
    "   R²  : 0.9397\n",
    "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1250, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a23ae0-a3fd-4e4a-a627-c149f2ceafdf",
   "metadata": {},
   "source": [
    "### Optimal Model - All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344dd1e7-d8ce-40d5-82a2-35f286db8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [1250],\n",
    "    'max_depth': [5],\n",
    "    'learning_rate': [0.05],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha': [1],   \n",
    "    'reg_lambda': [1]    \n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  \n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"\\n Final Evaluation on Test Set:\")\n",
    "print(f\"   RMSE: {rmse_best:,.2f}\")\n",
    "print(f\"   MAE : {mae_best:,.2f}\")\n",
    "print(f\"   R²  : {r2_best:.4f}\")\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "479ce549-7685-4ba6-a8c9-59260593c39c",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "\n",
    " Final Evaluation on Test Set:\n",
    "   RMSE: 967,208.17\n",
    "   MAE : 90,450.14\n",
    "   R²  : 0.9397\n",
    "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1250, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445a2ea-f685-4b0c-aa7b-fceabbbfa40e",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be319af0-332d-41e6-abaf-41181347172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Basic Model Evaluation on Test Set:\")\n",
    "print(f\"  RMSE: {rmse:,.2f}\")\n",
    "print(f\"  MAE : {mae:,.2f}\")\n",
    "print(f\"  R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99862cf4-03b7-4925-9de4-c0473033e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(model, max_num_features=30, importance_type='gain', height=0.5)\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e549b-2f0b-4cf0-b8a7-322c21ab9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted features by importance\n",
    "importance_dict = model.get_booster().get_score(importance_type='gain')\n",
    "important_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "top_features = [feature for feature, score in important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c10188-ee95-452c-b8cc-552f5e8efbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_dict = model.get_booster().get_score(importance_type='gain')\n",
    "important_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Features with their importance scores\n",
    "print(\"Feature Importances (by Gain):\")\n",
    "for feature, score in important_features:\n",
    "    print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bf19e-876c-42ce-9228-1526e6198b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gain = sum(importance_dict.values())\n",
    "normalized_importance = [(feature, score / total_gain) for feature, score in important_features]\n",
    "\n",
    "print(\"Normalized Feature Importances (by Gain):\")\n",
    "for feature, score in normalized_importance:\n",
    "    print(f\"{feature}: {score:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30cb7f-1842-4cc1-ab48-994b77734f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores = dict(normalized_importance)\n",
    "\n",
    "# Select features above a chosen importance threshold (0.7%)\n",
    "selected_features = [feature for feature, score in normalized_scores.items() if score >= 0.007]\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model_selected = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred_selected = model_selected.predict(X_test_selected)\n",
    "rmse_sel = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "mae_sel = mean_absolute_error(y_test, y_pred_selected)\n",
    "r2_sel = r2_score(y_test, y_pred_selected)\n",
    "\n",
    "print(\"\\n Evaluation after Feature Selection:\")\n",
    "print(f\"   RMSE: {rmse_sel:,.2f}\")\n",
    "print(f\"   MAE : {mae_sel:,.2f}\")\n",
    "print(f\"   R²  : {r2_sel:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29c4c980-a30a-4dd3-aaed-ca3dd3c93bc7",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1550, 'reg_alpha': 0.1, 'reg_lambda': 10, 'subsample': 1.0}\n",
    "\n",
    " Final Evaluation on Test Set:\n",
    "   RMSE: 1,114,324.20\n",
    "   MAE : 89,520.17\n",
    "   R²  : 0.9199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c22ce-4677-4df2-9ec5-43ef4b799044",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores = dict(normalized_importance)\n",
    "\n",
    "# Select features above a chosen importance threshold (0.5%)\n",
    "selected_features = [feature for feature, score in normalized_scores.items() if score >= 0.005]\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model_selected = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred_selected = model_selected.predict(X_test_selected)\n",
    "rmse_sel = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "mae_sel = mean_absolute_error(y_test, y_pred_selected)\n",
    "r2_sel = r2_score(y_test, y_pred_selected)\n",
    "\n",
    "print(\"\\n Evaluation after Feature Selection:\")\n",
    "print(f\"   RMSE: {rmse_sel:,.2f}\")\n",
    "print(f\"   MAE : {mae_sel:,.2f}\")\n",
    "print(f\"   R²  : {r2_sel:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289f2975-52a2-414e-961f-92b1d63126b7",
   "metadata": {},
   "source": [
    "Evaluation after Feature Selection:\n",
    "   RMSE: 1,095,062.16\n",
    "   MAE : 104,392.45\n",
    "   R²  : 0.9226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5040f-dc21-4a34-be37-ba1984ef40af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSelected Features:\")\n",
    "for f in selected_features:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ae6f7-043b-46d8-84e6-c7bfcf1f72af",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with RandomizedSearchCV (Only considering selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da965e6-dcde-4c03-81bb-93d3905afda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Expanded but controlled hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 750, 1200],\n",
    "    'learning_rate': [0.075, 0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],   \n",
    "    'reg_lambda': [1, 5, 10]    \n",
    "}\n",
    "\n",
    "#  Time-aware cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#  XGBoost regressor with fixed objective and random seed\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "#  Randomized search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_selected, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Final Test Set Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "\n",
    "print(\"\\n Best Parameters from CV:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db867d2b-80b2-4e38-864f-a15cec206361",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
    "\n",
    " Final Test Set Results:\n",
    " RMSE: 1,035,971.22\n",
    " MAE : 98,018.38\n",
    " R²  : 0.9308\n",
    "\n",
    " Best Parameters from CV:\n",
    "{'subsample': 1.0, 'reg_lambda': 5, 'reg_alpha': 0, 'n_estimators': 1200, 'max_depth': 5, 'learning_rate': 0.075, 'colsample_bytree': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9fd00-7808-43ef-b96a-427b43feac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Expanded hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1500, 1750,2000],\n",
    "    'learning_rate': [0.85, 0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],  \n",
    "    'reg_lambda': [1, 5, 10]    \n",
    "}\n",
    "\n",
    "#  Time-aware cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#  XGBoost regressor with fixed objective and random seed\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "#  Randomized search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train_selected, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Final Test Set Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "\n",
    "print(\"\\n Best Parameters from CV:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32b57a12-9c7c-4a22-8a02-05e31dd692a9",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    " Final Test Set Results:\n",
    " RMSE: 987,548.73\n",
    " MAE : 88,474.73\n",
    " R²  : 0.9371\n",
    "\n",
    " Best Parameters from CV:\n",
    "{'subsample': 1.0, 'reg_lambda': 10, 'reg_alpha': 0.1, 'n_estimators': 1500, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761d627-1176-4914-97ea-699bd9cb6fac",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV (Only considering selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3692222-7815-4929-a98f-36c69f2e01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [1450, 1500, 1550],\n",
    "    'max_depth': [5, 7],\n",
    "    'learning_rate': [ 0.05, 0.1, 0.15],\n",
    "    'subsample': [ 1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'reg_alpha': [0.1, 1],   \n",
    "    'reg_lambda': [5, 10]\n",
    "}\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Set Up Grid Search\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  \n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1  \n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_model.predict(X_test_selected)\n",
    "\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"\\n Final Evaluation on Test Set:\")\n",
    "print(f\"   RMSE: {rmse_best:,.2f}\")\n",
    "print(f\"   MAE : {mae_best:,.2f}\")\n",
    "print(f\"   R²  : {r2_best:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3193bbe-3f8b-4962-aadb-500a6c0167bb",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
    "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1550, 'reg_alpha': 0.1, 'reg_lambda': 10, 'subsample': 1.0}\n",
    "\n",
    " Final Evaluation on Test Set:\n",
    "   RMSE: 987,557.80\n",
    "   MAE : 88,086.85\n",
    "   R²  : 0.9371"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a7ff1-f143-4506-9fab-13be976da34f",
   "metadata": {},
   "source": [
    "### Optimal Model - Selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0191ae6-535c-4585-9321-6e6a8bc0ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized parameters\n",
    "params = {\n",
    "    'n_estimators': 1550,\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 10,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Instantiate model with optimized params\n",
    "best_model = XGBRegressor(**params)\n",
    "\n",
    "# Fit on selected features and target\n",
    "best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_best = best_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "mae = mean_absolute_error(y_test, y_pred_best)\n",
    "r2 = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"Final XGBoost Model Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10316242-0107-4963-8c54-2a3458543a13",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1550, 'reg_alpha': 0.1, 'reg_lambda': 10, 'subsample': 1.0}\n",
    "\n",
    " Final Evaluation on Test Set:\n",
    "   RMSE: 987,557.80\n",
    "   MAE : 88,086.85\n",
    "   R²  : 0.9371"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972cdab-58ec-4c9e-865b-ad55f0fa7412",
   "metadata": {},
   "source": [
    "### Saving the optimal XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a62c7-76e7-4adc-8569-e13552e35230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the XGBoost model\n",
    "with open(\"xgb_best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save selected feature names\n",
    "with open(\"xgb_selected_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train_selected.columns.tolist(), f)\n",
    "\n",
    "print(\"XGBoost model and selected features saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcdef3-4260-4919-a867-b7d392507e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Load selected features\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "# Prepare test data\n",
    "X_test_selected = X_test[selected_features]\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final XGBoost Model Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db683e15-64f6-4c0f-8186-83b511fcfcd7",
   "metadata": {},
   "source": [
    "Final XGBoost Model Results:\n",
    " RMSE: 987,557.80\n",
    " MAE : 88,086.85\n",
    " R²  : 0.9371"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873d7c4-8d99-4c87-834f-1ecc80d1fd85",
   "metadata": {},
   "source": [
    "### Branchwise MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a794b59-f738-4a15-850f-b0448711ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BRANCHID values for the test set\n",
    "branch_ids_test = df_cleaned.loc[X_test_selected.index, \"BRANCHID\"].values\n",
    "\n",
    "# DataFrame with predictions and actuals\n",
    "results_df = X_test_selected.copy()\n",
    "results_df[\"Actual\"] = y_test.values\n",
    "results_df[\"Predicted\"] = y_pred_best\n",
    "results_df[\"BRANCHID\"] = branch_ids_test\n",
    "\n",
    "# Compute absolute error per row\n",
    "results_df[\"AbsError\"] = abs(results_df[\"Actual\"] - results_df[\"Predicted\"])\n",
    "\n",
    "# Compute per-branch metrics: MAE, average actual, and % MAE\n",
    "relative_mae = results_df.groupby(\"BRANCHID\").agg(\n",
    "    MAE=(\"AbsError\", \"mean\"),\n",
    "    AvgCashFlow=(\"Actual\", \"mean\")\n",
    ").assign(\n",
    "    MAE_Percent=lambda x: 100 * x[\"MAE\"] / x[\"AvgCashFlow\"]\n",
    ").reset_index()\n",
    "\n",
    "# Display Top 10 branches with highest % MAE\n",
    "print(\"\\n Branch-wise MAE (% of Avg Daily Cash Flow) – Top 10:\")\n",
    "print(relative_mae.sort_values(\"MAE_Percent\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024a047-1736-480c-8909-82f220d49676",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mae_branches = branch_mae.sort_values(\"MAE\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_mae_branches[\"BRANCHID\"].astype(str), top_mae_branches[\"MAE\"], color=\"steelblue\")\n",
    "plt.xlabel(\"Branch ID\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Top 10 Branches by MAE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fad430-8418-4b02-a023-7233524b92d8",
   "metadata": {},
   "source": [
    "## Prediction Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69affb73-8211-46ec-b50c-568b9544e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine true and predicted values into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_best).reset_index(drop=True)  \n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# Filter for BranchID = 1\n",
    "branch_df = test_df[test_df['BRANCHID'] == 1].sort_values('TXNDATE')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Predicted\", color=\"dodgerblue\", linestyle='--')\n",
    "plt.title(\"Actual vs Predicted NetCashFlow (Test Set) — BranchID = 1\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab9076-5e29-43b4-8c1f-193ecad53c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine true and predicted values into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_best).reset_index(drop=True)  \n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# Filter for BranchID = 2\n",
    "branch_df = test_df[test_df['BRANCHID'] == 2].sort_values('TXNDATE')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Predicted\", color=\"dodgerblue\", linestyle='--')\n",
    "plt.title(\"Actual vs Predicted NetCashFlow (Test Set) — BranchID = 2\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2eb33-a7b8-483d-bdab-d07c11b9fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine true and predicted values into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_best).reset_index(drop=True)  \n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# Filter for BranchID = 56\n",
    "branch_df = test_df[test_df['BRANCHID'] == 56].sort_values('TXNDATE')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Predicted\", color=\"dodgerblue\", linestyle='--')\n",
    "plt.title(\"Actual vs Predicted NetCashFlow (Test Set) — BranchID = 56\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b83a5d-cb10-415f-9263-de5c21d01975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine true and predicted values into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_best).reset_index(drop=True)  \n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# Filter for BranchID = 100\n",
    "branch_df = test_df[test_df['BRANCHID'] == 100].sort_values('TXNDATE')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Predicted\", color=\"dodgerblue\", linestyle='--')\n",
    "plt.title(\"Actual vs Predicted NetCashFlow (Test Set) — BranchID = 100\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a53028-e319-4e04-acd2-f97d7b60b350",
   "metadata": {},
   "source": [
    "# Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e804d-4617-4373-bb1b-cbaceeff093b",
   "metadata": {},
   "source": [
    "### Basic LightGBM (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041f225-084a-4fcf-957a-e51e07207c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Basic LightGBM model with default parameters\n",
    "lgb_model = LGBMRegressor(random_state=42)\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "r2 = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f\"\\n LightGBM Basic Model Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0d6b4aa-e274-4049-aa99-c2d98fcd9188",
   "metadata": {},
   "source": [
    "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042986 seconds.\n",
    "You can set `force_col_wise=true` to remove the overhead.\n",
    "[LightGBM] [Info] Total Bins 11111\n",
    "[LightGBM] [Info] Number of data points in the train set: 150104, number of used features: 61\n",
    "[LightGBM] [Info] Start training from score 324877.337334\n",
    "\n",
    " LightGBM Basic Model Results:\n",
    " RMSE: 1,151,843.52\n",
    " MAE : 128,026.35\n",
    " R²  : 0.9144"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0492d6-d141-4094-b666-eca1e0af71f2",
   "metadata": {},
   "source": [
    "### RandomizedSearch using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eac9c-eac2-42c7-a2ad-56f00dc8cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.005, 0.05),        \n",
    "    'n_estimators': randint(300, 1000),\n",
    "    'subsample': uniform(0.6, 0.4),                \n",
    "    'colsample_bytree': uniform(0.6, 0.4),         \n",
    "    'reg_alpha': uniform(0, 1.0),\n",
    "    'reg_lambda': uniform(0, 1.0),\n",
    "    'min_child_samples': randint(10, 100),\n",
    "    'min_split_gain': uniform(0.0, 0.5)\n",
    "}\n",
    "\n",
    "# Initial model\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_lgb_final = random_search.best_estimator_\n",
    "\n",
    "y_pred_lgb_final = best_lgb_final.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_final))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_final)\n",
    "r2 = r2_score(y_test, y_pred_lgb_final)\n",
    "\n",
    "print(f\"\\n Final Optimized LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters Found:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c28b20f4-8703-46ee-9074-9f6a4c410d85",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    "Final Optimized LightGBM Results:\n",
    " RMSE: 1,092,380.01\n",
    " MAE : 105,834.63\n",
    " R²  : 0.9230\n",
    "\n",
    "Best Parameters Found:\n",
    "{'colsample_bytree': np.float64(0.9343920482048823), 'learning_rate': np.float64(0.039798710304684896), 'max_depth': 7, 'min_child_samples': 10, 'min_split_gain': np.float64(0.08664716003542289), 'n_estimators': 812, 'num_leaves': 53, 'reg_alpha': np.float64(0.23978735915740323), 'reg_lambda': np.float64(0.09387329008129175), 'subsample': np.float64(0.6731463988429229)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe0ed-cdd1-42ee-8e07-7af2a563f5f7",
   "metadata": {},
   "source": [
    "### Tune num_leaves and max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639bc2c-bc50-41bb-93ae-02a6d84c2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [50, 60, 70],\n",
    "    'max_depth': [5, 7, 10, -1]\n",
    "}\n",
    "\n",
    "#  TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_lgb_1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred_lgb_1 = best_lgb_1.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_1))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_1)\n",
    "r2 = r2_score(y_test, y_pred_lgb_1)\n",
    "\n",
    "print(f\"\\n Step 1 Optimized LightGBM:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc94225f-4e44-4222-b1a9-14e209a98133",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "\n",
    " Step 1 Optimized LightGBM:\n",
    " RMSE: 1,232,580.79\n",
    " MAE : 115,642.62\n",
    " R²  : 0.9020\n",
    "\n",
    " Best Params: {'max_depth': -1, 'num_leaves': 70}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "650e6caa-d3f7-4d0d-a753-9ae79044ac7d",
   "metadata": {},
   "source": [
    "num_leaves = 70 — higher complexity, deeper trees\n",
    "\n",
    "max_depth = -1 — no depth restriction (let trees grow as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6011-f62e-452e-b57b-4d85cff88d83",
   "metadata": {},
   "source": [
    "### Tune learning_rate & n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f18d1-913c-428b-804d-efaef57cb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Use best params from the previous step\n",
    "lgb = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    max_depth=-1,\n",
    "    num_leaves=70\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.04, 0.05, 0.06],\n",
    "    'n_estimators': [800, 900, 1000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_lgb_2 = grid_search.best_estimator_\n",
    "\n",
    "y_pred_lgb_2 = best_lgb_2.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_2))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_2)\n",
    "r2 = r2_score(y_test, y_pred_lgb_2)\n",
    "\n",
    "print(f\"\\n Step 2 Optimized LightGBM:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "698331ff-66e0-4828-937e-70c140a23c28",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "\n",
    " Step 2 Optimized LightGBM:\n",
    " RMSE: 1,249,534.74\n",
    " MAE : 118,544.23\n",
    " R²  : 0.8993\n",
    "\n",
    " Best Params: {'learning_rate': 0.04, 'n_estimators': 800}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedefa3-126e-4a72-bc38-b668f28725dc",
   "metadata": {},
   "source": [
    "### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f88e70-f667-4ed0-ba05-cc79b0381b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all previous best parameters\n",
    "lgb = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    max_depth=-1,\n",
    "    num_leaves=70,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=700\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [ 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_lgb_3 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred_lgb_3 = best_lgb_3.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_3))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_3)\n",
    "r2 = r2_score(y_test, y_pred_lgb_3)\n",
    "\n",
    "print(f\"\\n Step 3 Optimized LightGBM:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32c84725-a3ed-4271-b34c-7885f99e2073",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "\n",
    " Step 3 Optimized LightGBM:\n",
    " RMSE: 1,203,785.01\n",
    " MAE : 110,212.76\n",
    " R²  : 0.9065\n",
    "\n",
    " Best Params: {'colsample_bytree': 0.9, 'subsample': 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41571e-36b4-4f66-a875-ea803594e317",
   "metadata": {},
   "source": [
    "### With found variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49184a-942c-402e-b4a2-d8c083849032",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [70],\n",
    "    'learning_rate': [0.04],\n",
    "    'n_estimators': [800],\n",
    "    'max_depth': [-1]\n",
    "}\n",
    "\n",
    "lgb = LGBMRegressor(\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_alpha=0.24,\n",
    "    reg_lambda=0.09,\n",
    "    min_child_samples=10,\n",
    "    min_split_gain=0.087,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "\n",
    "print(f\"\\n Final GridSearch LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f18be8f7-95f8-4998-a1b0-f82bbc71f345",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "\n",
    "Final GridSearch LightGBM Results:\n",
    " RMSE: 1,163,874.15\n",
    " MAE : 109,055.88\n",
    " R²  : 0.9126\n",
    "\n",
    "Best Parameters:\n",
    "{'learning_rate': 0.04, 'max_depth': -1, 'n_estimators': 800, 'num_leaves': 70}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a03506-0fd2-4d34-bcbf-f4ba7a982466",
   "metadata": {},
   "source": [
    "### Optuna Tuning for LightGBM - All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d5f2b-9afa-44d8-897e-d00b3db1270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 700,\n",
    "        'max_depth': -1,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10)\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_train_cv, X_val_cv = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_cv, y_train_cv,\n",
    "            eval_set=[(X_val_cv, y_val_cv)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[early_stopping(50), log_evaluation(0)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val_cv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, preds))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "# Start the study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best parameters\n",
    "print(\" Best Params Found by Optuna:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c6c92-d6f7-4376-8ac2-873d64c800ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd6982-596a-47b5-b1f8-dab57d20f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best params\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 700,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "best_model = lgb.LGBMRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Optuna-Tuned LightGBM:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88fa1104-caca-42f4-b2ba-579d084d695b",
   "metadata": {},
   "source": [
    "Optuna-Tuned LightGBM:\n",
    " RMSE: 1,219,551.12\n",
    " MAE : 109,500.67\n",
    " R²  : 0.9041"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f846664-1260-4fa1-a013-9be03763ad21",
   "metadata": {},
   "source": [
    "### Basic LightGBM (All Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997891a-78a3-4619-9cdc-246dede5c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Basic LightGBM model with default parameters\n",
    "lgb_model = LGBMRegressor(random_state=42)\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "r2 = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f\"\\n LightGBM Basic Model Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fb00e-f510-4019-a5cf-3dad6377c9e4",
   "metadata": {},
   "source": [
    "### Featue Selection in LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83917cee-4bbc-4f7f-8213-8f13c7c1c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train_dataset,\n",
    "    num_boost_round=100\n",
    ")\n",
    "\n",
    "# Get feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgb_model.feature_importance(importance_type='gain')\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "# Select top 20 features\n",
    "TOP_N = 20\n",
    "top_features = importance_df.head(TOP_N)['feature'].tolist()\n",
    "\n",
    "# Create reduced feature sets\n",
    "X_train_select_LGBM = X_train[top_features]\n",
    "X_test_select_LGBM = X_test[top_features]  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df.head(TOP_N)['feature'][::-1], importance_df.head(TOP_N)['importance'][::-1])\n",
    "plt.title('Top LightGBM Feature Importances (by Gain)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Selected features for LightGBM:\", top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267638f3-bd6e-45ba-9a35-ae7724b2239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw gain importances\n",
    "feature_importances = lgb_model.feature_importance(importance_type='gain')\n",
    "feature_names = lgb_model.feature_name()\n",
    "\n",
    "# Normalize importances\n",
    "total_gain = np.sum(feature_importances)\n",
    "importance_percent = 100.0 * feature_importances / total_gain\n",
    "\n",
    "# Create DataFrame\n",
    "normalized_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Normalized Importance (%)': importance_percent\n",
    "})\n",
    "\n",
    "# Sort and round\n",
    "normalized_importance_df = normalized_importance_df.sort_values(by='Normalized Importance (%)', ascending=False)\n",
    "normalized_importance_df['Normalized Importance (%)'] = normalized_importance_df['Normalized Importance (%)'].round(4)\n",
    "\n",
    "print(\"Normalized Feature Importances (by Gain):\")\n",
    "for _, row in normalized_importance_df.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Normalized Importance (%)']:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2365c7-40ac-46e5-b722-6d5b12e2068b",
   "metadata": {},
   "source": [
    "### RandomizedSearch using Selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5866f-cd3c-418b-a1c1-9f41a7d7fb37",
   "metadata": {},
   "source": [
    "### threshold = 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dc03a-e8d5-4f5a-afb7-bbea78fcd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # in percent\n",
    "\n",
    "selected_features = normalized_importance_df[\n",
    "    normalized_importance_df['Normalized Importance (%)'] > threshold\n",
    "]['Feature'].tolist()\n",
    "\n",
    "# Create reduced feature sets\n",
    "X_train_select_LGBM = X_train[selected_features]\n",
    "X_test_select_LGBM = X_test[selected_features]  \n",
    "\n",
    "print(f\"Selected {len(selected_features)} features with > {threshold}% importance:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca278729-9b5f-442a-9b75-d2e00f7bd331",
   "metadata": {},
   "source": [
    "Selected 12 features with > 0.5% importance:\n",
    "['TOTALCREDITAMOUNT', 'Debit_to_Credit_Ratio', 'TOTALDEBITAMOUNT', 'CashOut_Per_Customer', 'CashIn_Per_Customer', 'CODE', 'Debit_ZScore', 'Delta_Debit', 'CUSTOMER', 'Lag2_Customers', 'Rolling7_DebitStd', 'Credit_ZScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16134ace-fccc-43a0-a4d2-c00a5f40aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.005, 0.05),\n",
    "    'n_estimators': randint(300, 1000),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1.0),\n",
    "    'reg_lambda': uniform(0, 1.0),\n",
    "    'min_child_samples': randint(10, 100),\n",
    "    'min_split_gain': uniform(0.0, 0.5)\n",
    "}\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "y_pred_lgb_final = random_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_final))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_final)\n",
    "r2 = r2_score(y_test, y_pred_lgb_final)\n",
    "\n",
    "print(f\"\\n Final Optimized LightGBM (Selected Features):\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89a1387f-1586-4fed-a099-07478b74fbb7",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    "Final Optimized LightGBM (Selected Features):\n",
    " RMSE: 1,207,435.39\n",
    " MAE : 95,598.75\n",
    " R²  : 0.9059\n",
    "\n",
    "Best Parameters:\n",
    "{'colsample_bytree': np.float64(0.9906459823330611), 'learning_rate': np.float64(0.025551850665911566), 'max_depth': 9, 'min_child_samples': 10, 'min_split_gain': np.float64(0.11978094533348621), 'n_estimators': 429, 'num_leaves': 72, 'reg_alpha': np.float64(0.489452760277563), 'reg_lambda': np.float64(0.9856504541106007), 'subsample': np.float64(0.6968221086046001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c87cad-471d-470b-ae38-b0505ce28fe2",
   "metadata": {},
   "source": [
    "### threshold = 0.01%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a55fd5-abc9-4b19-b50d-9a6ef2e953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01  # in percent\n",
    "\n",
    "\n",
    "selected_features = normalized_importance_df[\n",
    "    normalized_importance_df['Normalized Importance (%)'] > threshold\n",
    "]['Feature'].tolist()\n",
    "\n",
    "X_train_select_LGBM = X_train[selected_features]\n",
    "X_test_select_LGBM = X_test[selected_features]  \n",
    "\n",
    "print(f\"Selected {len(selected_features)} features with > {threshold}% importance:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16535586-46a3-4244-a03f-b9097416149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.005, 0.05),\n",
    "    'n_estimators': randint(300, 1500),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1.0),\n",
    "    'reg_lambda': uniform(0, 1.0),\n",
    "    'min_child_samples': randint(10, 100),\n",
    "    'min_split_gain': uniform(0.0, 0.5)\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "y_pred_lgb_final = random_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_final))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_final)\n",
    "r2 = r2_score(y_test, y_pred_lgb_final)\n",
    "\n",
    "print(f\"\\n Final Optimized LightGBM (Selected Features):\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab8fa5d2-0f64-4a19-aa8b-0333541a684c",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    "Final Optimized LightGBM (Selected Features):\n",
    " RMSE: 1,150,343.05\n",
    " MAE : 110,911.26\n",
    " R²  : 0.9146\n",
    "\n",
    "Best Parameters:\n",
    "{'colsample_bytree': np.float64(0.8010716092915446), 'learning_rate': np.float64(0.007573937562499468), 'max_depth': 7, 'min_child_samples': 10, 'min_split_gain': np.float64(0.11978094533348621), 'n_estimators': 1453, 'num_leaves': 72, 'reg_alpha': np.float64(0.489452760277563), 'reg_lambda': np.float64(0.9856504541106007), 'subsample': np.float64(0.6968221086046001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172e0f6-9530-42b1-af64-5c25b707e251",
   "metadata": {},
   "source": [
    "### threshold = 0.05%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3b4e1-8b9c-46ca-b3e6-0c036657e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05  # in percent\n",
    "\n",
    "selected_features = normalized_importance_df[\n",
    "    normalized_importance_df['Normalized Importance (%)'] > threshold\n",
    "]['Feature'].tolist()\n",
    "\n",
    "X_train_select_LGBM = X_train[selected_features]\n",
    "X_test_select_LGBM = X_test[selected_features] \n",
    "\n",
    "print(f\"Selected {len(selected_features)} features with > {threshold}% importance:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb14d28-e6b6-4c4d-b568-814498b9cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid \n",
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.005, 0.05),\n",
    "    'n_estimators': randint(300, 1500),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1.0),\n",
    "    'reg_lambda': uniform(0, 1.0),\n",
    "    'min_child_samples': randint(10, 100),\n",
    "    'min_split_gain': uniform(0.0, 0.5)\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV \n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "y_pred_lgb_final = random_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_final))\n",
    "mae = mean_absolute_error(y_test, y_pred_lgb_final)\n",
    "r2 = r2_score(y_test, y_pred_lgb_final)\n",
    "\n",
    "print(f\"\\n Final Optimized LightGBM (Selected Features):\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73075e4f-f053-4fd1-a5f0-c2efe3f16bc0",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    " Final Optimized LightGBM (Selected Features):\n",
    " RMSE: 1,133,968.75\n",
    " MAE : 109,263.55\n",
    " R²  : 0.9170\n",
    "\n",
    " Best Parameters:\n",
    "{'colsample_bytree': np.float64(0.8010716092915446), 'learning_rate': np.float64(0.007573937562499468), 'max_depth': 7, 'min_child_samples': 10, 'min_split_gain': np.float64(0.11978094533348621), 'n_estimators': 1453, 'num_leaves': 72, 'reg_alpha': np.float64(0.489452760277563), 'reg_lambda': np.float64(0.9856504541106007), 'subsample': np.float64(0.6968221086046001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ae651-0cbb-4c51-bb82-17514c1b2d8d",
   "metadata": {},
   "source": [
    "### Grid search (0.05) - Selected Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448f5f7-329a-4da6-9a0b-0dd39974f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [60, 75, 90],\n",
    "    'learning_rate': [0.0065, 0.0075, 0.0085],\n",
    "    'n_estimators': [1400, 1450, 1500],\n",
    "    'max_depth': [6, 7]\n",
    "}\n",
    "\n",
    "# best-performing values from previous RandomizedSearch\n",
    "lgb = LGBMRegressor(\n",
    "    subsample=0.673,\n",
    "    colsample_bytree=0.934,\n",
    "    reg_alpha=0.24,\n",
    "    reg_lambda=0.09,\n",
    "    min_child_samples=10,\n",
    "    min_split_gain=0.087,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv, \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "print(f\"\\n Final GridSearch LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0377ada7-4f4b-4b4e-a839-3c39d5d84d09",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
    "\n",
    "Final GridSearch LightGBM Results:\n",
    " RMSE: 1,128,234.80\n",
    " MAE : 97,974.20\n",
    " R²  : 0.9179\n",
    "\n",
    "Best Parameters:\n",
    "{'learning_rate': 0.0065, 'max_depth': 7, 'n_estimators': 1400, 'num_leaves': 90}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77c986-34ea-4097-bc1a-84435996d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [85, 110, 130],\n",
    "    'learning_rate': [0.0065, 0.0075],\n",
    "    'n_estimators': [1400, 1500, 1700],\n",
    "    'max_depth': [6, 7]\n",
    "}\n",
    "\n",
    "# best-performing values from previous RandomizedSearch\n",
    "lgb = LGBMRegressor(\n",
    "    subsample=0.673,\n",
    "    colsample_bytree=0.934,\n",
    "    reg_alpha=0.24,\n",
    "    reg_lambda=0.09,\n",
    "    min_child_samples=10,\n",
    "    min_split_gain=0.087,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "print(f\"\\n Final GridSearch LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91a68a0d-b05c-487e-a129-61e3b4d82268",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
    "\n",
    "Final GridSearch LightGBM Results:\n",
    " RMSE: 1,133,910.31\n",
    " MAE : 96,585.39\n",
    " R²  : 0.9171\n",
    "\n",
    " Best Parameters:\n",
    "{'learning_rate': 0.0065, 'max_depth': 7, 'n_estimators': 1400, 'num_leaves': 130}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31813d75-281e-4362-8c35-8c5348a8882d",
   "metadata": {},
   "source": [
    "### Optuna Tuning for LightGBM - Selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82faf1d-8da8-4c1a-8942-a87d1ddbe3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 700,\n",
    "        'max_depth': -1,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10)\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X_train_select_LGBM):\n",
    "        X_train_cv, X_val_cv = X_train_select_LGBM.iloc[train_idx], X_train_select_LGBM.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_cv, y_train_cv,\n",
    "            eval_set=[(X_val_cv, y_val_cv)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[early_stopping(50), log_evaluation(0)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val_cv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, preds))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "# Start the study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best parameters\n",
    "print(\" Best Params Found by Optuna:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0ada3-021d-4da1-9b87-d352fa699140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best score\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "\n",
    "# best parameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa48ae31-2232-4dae-afb1-98417bd4d262",
   "metadata": {},
   "source": [
    "Best RMSE: 189197.14368183882\n",
    "Best hyperparameters:\n",
    "  num_leaves: 67\n",
    "  min_child_samples: 11\n",
    "  min_split_gain: 0.20485301657568256\n",
    "  reg_alpha: 0.8616275988506972\n",
    "  reg_lambda: 0.07778724633113288\n",
    "  feature_fraction: 0.8628855037698339\n",
    "  bagging_fraction: 0.6940917596276721\n",
    "  bagging_freq: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5655c3-2259-48fb-b00e-a89e4d6de89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with best params\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 700,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "best_model = lgb.LGBMRegressor(**best_params)\n",
    "best_model.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test_select_LGBM)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Optuna-Tuned LightGBM:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed22a1c-c2aa-4341-8ff2-6bdadaa8998f",
   "metadata": {},
   "source": [
    "Optuna-Tuned LightGBM:\n",
    " RMSE: 1,171,260.51\n",
    " MAE : 109,309.71\n",
    " R²  : 0.9115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e808022-b727-45fa-8d91-bef464585799",
   "metadata": {},
   "source": [
    "### Optimal model - Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557fae1e-c288-4e9b-af4b-66d76ea50f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [130],\n",
    "    'learning_rate': [0.0065],\n",
    "    'n_estimators': [1400],\n",
    "    'max_depth': [7]\n",
    "}\n",
    "\n",
    "# best-performing values from previous RandomizedSearch\n",
    "lgb = LGBMRegressor(\n",
    "    subsample=0.673,\n",
    "    colsample_bytree=0.934,\n",
    "    reg_alpha=0.24,\n",
    "    reg_lambda=0.09,\n",
    "    min_child_samples=10,\n",
    "    min_split_gain=0.087,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# TimeSeriesSplit \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "\n",
    "print(f\"\\n Final GridSearch LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6a41c-90c5-46db-be4c-4c08824dbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [130],\n",
    "    'learning_rate': [0.0065],\n",
    "    'n_estimators': [1400],\n",
    "    'max_depth': [7]\n",
    "}\n",
    "\n",
    "# best-performing values from previous RandomizedSearch\n",
    "lgb = LGBMRegressor(\n",
    "    subsample=0.673,\n",
    "    colsample_bytree=0.934,\n",
    "    reg_alpha=0.24,\n",
    "    reg_lambda=0.09,\n",
    "    min_child_samples=10,\n",
    "    min_split_gain=0.087,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# TimeSeriesSplit \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "\n",
    "y_pred_grid = grid_search.best_estimator_.predict(X_test_select_LGBM)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "\n",
    "print(f\"\\n Final GridSearch LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")\n",
    "print(\"\\n Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b970cfc1-45d9-4d85-8fb9-9b90888edd3a",
   "metadata": {},
   "source": [
    "RMSE: 1,133,910.31\n",
    " MAE : 96,585.39\n",
    " R²  : 0.9171"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacadd6-0e19-442b-b80d-19bd0dc12c1a",
   "metadata": {},
   "source": [
    "### Saving the optimal LGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82766280-0a13-441d-93a2-b686ef9c1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best LightGBM model\n",
    "with open(\"lgbm_best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grid_search.best_estimator_, f)\n",
    "\n",
    "# Save the selected feature names used for LGBM\n",
    "with open(\"lgbm_selected_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_train_select_LGBM.columns.tolist(), f)\n",
    "\n",
    "print(\"LightGBM model and selected features saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226da709-fb9a-48ab-8a0a-1c6b99df6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved LGBM model\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "# Load selected feature names\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    selected_features_lgbm = pickle.load(f)\n",
    "\n",
    "# Use selected features for prediction\n",
    "X_test_lgbm = X_test[selected_features_lgbm]\n",
    "y_pred_grid = lgbm_model.predict(X_test_lgbm)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_grid))\n",
    "mae = mean_absolute_error(y_test, y_pred_grid)\n",
    "r2 = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "print(f\"\\n Final LightGBM Results:\")\n",
    "print(f\" RMSE: {rmse:,.2f}\")\n",
    "print(f\" MAE : {mae:,.2f}\")\n",
    "print(f\" R²  : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61cc291b-18f9-4624-938e-add199f51ac9",
   "metadata": {},
   "source": [
    "Final LightGBM Results:\n",
    " RMSE: 1,133,910.31\n",
    " MAE : 96,585.39\n",
    " R²  : 0.9171"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2a058-1a0d-4fbe-8264-23c6bc4dd4f5",
   "metadata": {},
   "source": [
    "### Branch wise MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bff80a-0161-4716-8127-96f4b17155fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original BRANCHID values for the test set\n",
    "branch_ids_test = df_cleaned.loc[X_test_select_LGBM.index, \"BRANCHID\"].values\n",
    "\n",
    "# DataFrame with predictions and actuals\n",
    "results_df = X_test_select_LGBM.copy()\n",
    "results_df[\"Actual\"] = y_test.values\n",
    "results_df[\"Predicted\"] = y_pred_grid\n",
    "results_df[\"BRANCHID\"] = branch_ids_test\n",
    "\n",
    "# Compute Branchwise MAE\n",
    "branch_mae = results_df.groupby(\"BRANCHID\").apply(\n",
    "    lambda g: mean_absolute_error(g[\"Actual\"], g[\"Predicted\"])\n",
    ").reset_index(name=\"MAE\")\n",
    "\n",
    "# Top 10 branches by MAE\n",
    "print(\"\\n Branch-wise MAE (Top 10 branches with highest error):\")\n",
    "print(branch_mae.sort_values(\"MAE\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaf4ad-5ee8-4802-89b1-207b1caffa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_branches = branch_mae.sort_values(\"MAE\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(top_branches[\"BRANCHID\"].astype(str), top_branches[\"MAE\"], color=\"tomato\")\n",
    "plt.xlabel(\"Branch ID\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Top 10 Branches by MAE\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813ea89-fbed-4e7c-a964-3c192d06c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"AbsError\"] = abs(results_df[\"Actual\"] - results_df[\"Predicted\"])\n",
    "relative_mae = results_df.groupby(\"BRANCHID\").agg(\n",
    "    MAE=(\"AbsError\", \"mean\"),\n",
    "    AvgCashFlow=(\"Actual\", \"mean\")\n",
    ").assign(MAE_Percent=lambda x: 100 * x[\"MAE\"] / x[\"AvgCashFlow\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a4b6-ef89-404b-afa4-e0a20f249c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original BRANCHID values for the test set\n",
    "branch_ids_test = df_cleaned.loc[X_test_select_LGBM.index, \"BRANCHID\"].values\n",
    "\n",
    "# DataFrame with predictions and actuals\n",
    "results_df = X_test_select_LGBM.copy()\n",
    "results_df[\"Actual\"] = y_test.values\n",
    "results_df[\"Predicted\"] = y_pred_grid\n",
    "results_df[\"BRANCHID\"] = branch_ids_test\n",
    "\n",
    "# Compute absolute error per row\n",
    "results_df[\"AbsError\"] = abs(results_df[\"Actual\"] - results_df[\"Predicted\"])\n",
    "\n",
    "# Compute per-branch metrics: MAE, average actual, and % MAE\n",
    "relative_mae = results_df.groupby(\"BRANCHID\").agg(\n",
    "    MAE=(\"AbsError\", \"mean\"),\n",
    "    AvgCashFlow=(\"Actual\", \"mean\")\n",
    ").assign(\n",
    "    MAE_Percent=lambda x: 100 * x[\"MAE\"] / x[\"AvgCashFlow\"]\n",
    ").reset_index()\n",
    "\n",
    "# Display Top 10 branches with highest % MAE\n",
    "print(\"\\n Branch-wise MAE (% of Avg Daily Cash Flow) – Top 10:\")\n",
    "print(relative_mae.sort_values(\"MAE_Percent\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cbb7b-4e42-4066-9c78-742596de46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TXNDATE is datetime\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine true and predicted values into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_grid).reset_index(drop=True)  \n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# Filter for BranchID = 210\n",
    "branch_df = test_df[test_df['BRANCHID'] == 210].sort_values('TXNDATE')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Predicted\", color=\"dodgerblue\", linestyle='--')\n",
    "plt.title(\"Actual vs Predicted NetCashFlow (Test Set) — BranchID = 210\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fd0c1-6e53-4cf6-a1c5-33b3477ba63f",
   "metadata": {},
   "source": [
    "# Model Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4a49c-0f86-4863-9c97-17effbfa2fb4",
   "metadata": {},
   "source": [
    "### Optimize Blend Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c3e25-75ac-4ba7-9024-e9f8b8a3dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% XGB, 50% LightGBM\n",
    "y_pred_blend_weighted = 0.5 * y_pred_best + 0.5 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (50% XGB, 50% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45c472b3-e7d8-4300-ab6c-49ed6cd765e0",
   "metadata": {},
   "source": [
    "Weighted Blended Model Evaluation \n",
    "(50% XGB, 50% LGBM):\n",
    "   RMSE: 1,030,237.07\n",
    "   MAE : 81,410.20\n",
    "   R²  : 0.9315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b97ee5-270d-44a2-a310-1329a37b655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40% XGB, 60% LightGBM\n",
    "y_pred_blend_weighted = 0.4 * y_pred_best + 0.6 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (40% XGB, 60% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c06880a8-cf8b-48bc-8b10-96c0eefe32b3",
   "metadata": {},
   "source": [
    "Weighted Blended Model Evaluation (40% XGB, 60% LGBM):\n",
    "   RMSE: 1,046,518.78\n",
    "   MAE : 82,592.84\n",
    "   R²  : 0.9293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d8128-4ec3-469e-b549-a02dc6f07a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% XGB, 70% LightGBM\n",
    "y_pred_blend_weighted = 0.3 * y_pred_best + 0.7 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (30% XGB, 70% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f97f41ac-f075-4001-b14f-ed566ec22735",
   "metadata": {},
   "source": [
    " Weighted Blended Model Evaluation (30% XGB, 70% LGBM):\n",
    "   RMSE: 1,065,150.01\n",
    "   MAE : 84,758.04\n",
    "   R²  : 0.9268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0af4bc-3c3f-4487-803d-356b59bc2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% XGB, 40% LightGBM\n",
    "y_pred_blend_weighted = 0.6 * y_pred_best + 0.4 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (60% XGB, 40% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd331cf3-58bd-4769-a191-87d23c1aa1ba",
   "metadata": {},
   "source": [
    "Weighted Blended Model Evaluation \n",
    "(60% XGB, 40% LGBM):\n",
    "   RMSE: 1,016,417.78\n",
    "   MAE : 81,251.71\n",
    "   R²  : 0.9334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df5d67-ec26-4fba-b9e3-a502362bb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% XGB, 30% LightGBM\n",
    "y_pred_blend_weighted = 0.7 * y_pred_best + 0.3 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (70% XGB, 30% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6fd8d79-531a-4aaa-a4c5-4d2063dafffa",
   "metadata": {},
   "source": [
    "Weighted Blended Model Evaluation (70% XGB, 30% LGBM):\n",
    "   RMSE: 1,005,162.52\n",
    "   MAE : 81,914.70\n",
    "   R²  : 0.9348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc456d-6ea1-483c-a208-bfe84860a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% XGB, 20% LightGBM\n",
    "y_pred_blend_weighted = 0.8 * y_pred_best + 0.2 * y_pred_grid\n",
    "\n",
    "rmse_weighted = np.sqrt(mean_squared_error(y_test, y_pred_blend_weighted))\n",
    "mae_weighted = mean_absolute_error(y_test, y_pred_blend_weighted)\n",
    "r2_weighted = r2_score(y_test, y_pred_blend_weighted)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (80% XGB, 20% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_weighted:,.2f}\")\n",
    "print(f\"   MAE : {mae_weighted:,.2f}\")\n",
    "print(f\"   R²  : {r2_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b1e10ae-da8c-433e-a3c4-2f61898ce1f5",
   "metadata": {},
   "source": [
    "Weighted Blended Model Evaluation (80% XGB, 20% LGBM):\n",
    "   RMSE: 996,558.11\n",
    "   MAE : 83,216.37\n",
    "   R²  : 0.9359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57c4f7-3506-4b43-a2a6-aa6caeca73ec",
   "metadata": {},
   "source": [
    "### Optimal Blending model (Using saved XGB & LGBM models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b3f2f-d14a-486b-97c1-6f6a19b2bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Models & Features\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_xgb = X_test[xgb_features]\n",
    "X_test_lgbm = X_test[lgbm_features]\n",
    "\n",
    "\n",
    "# Predict with Each Model\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "y_pred_lgbm = lgbm_model.predict(X_test_lgbm)\n",
    "\n",
    "\n",
    "# Weighted Blending\n",
    "y_pred_blend = 0.6 * y_pred_xgb + 0.4 * y_pred_lgbm\n",
    "\n",
    "\n",
    "# Evaluate Blended Model\n",
    "rmse_blend = np.sqrt(mean_squared_error(y_test, y_pred_blend))\n",
    "mae_blend = mean_absolute_error(y_test, y_pred_blend)\n",
    "r2_blend = r2_score(y_test, y_pred_blend)\n",
    "\n",
    "print(\"\\n Weighted Blended Model Evaluation (60% XGB, 40% LGBM):\")\n",
    "print(f\"   RMSE: {rmse_blend:,.2f}\")\n",
    "print(f\"   MAE : {mae_blend:,.2f}\")\n",
    "print(f\"   R²  : {r2_blend:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c68d8-d730-4c1e-90b6-8a908ef1890c",
   "metadata": {},
   "source": [
    "### Saving the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ea47e-1d4d-4446-b403-74043dc40100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBlender:\n",
    "    def __init__(self, model1, model2, weight1, weight2, features1, features2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "\n",
    "    def predict(self, X):\n",
    "        X1 = X[self.features1]\n",
    "        X2 = X[self.features2]\n",
    "        pred1 = self.model1.predict(X1)\n",
    "        pred2 = self.model2.predict(X2)\n",
    "        return self.weight1 * pred1 + self.weight2 * pred2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b04eaa-0c13-4cdd-a45b-c1b2d80dbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the weighted blender object\n",
    "blender = WeightedBlender(\n",
    "    model1=xgb_model,\n",
    "    model2=lgbm_model,\n",
    "    weight1=0.6,\n",
    "    weight2=0.4,\n",
    "    features1=xgb_features,\n",
    "    features2=lgbm_features\n",
    ")\n",
    "\n",
    "# Save the blended model object\n",
    "with open(\"weighted_blended_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blender, f)\n",
    "\n",
    "print(\"Blended model saved successfully as 'weighted_blended_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983c97b-620f-47af-89ca-d14521b7af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the blended model\n",
    "\n",
    "class WeightedBlender:\n",
    "    def __init__(self, model1, model2, weight1, weight2, features1, features2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "\n",
    "    def predict(self, X):\n",
    "        X1 = X[self.features1]\n",
    "        X2 = X[self.features2]\n",
    "        pred1 = self.model1.predict(X1)\n",
    "        pred2 = self.model2.predict(X2)\n",
    "        return self.weight1 * pred1 + self.weight2 * pred2\n",
    "\n",
    "with open(\"weighted_blended_model.pkl\", \"rb\") as f:\n",
    "    blended_model = pickle.load(f)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_blended = blended_model.predict(X_test)\n",
    "\n",
    "# Evaluate if needed\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_blended))\n",
    "mae = mean_absolute_error(y_test, y_pred_blended)\n",
    "r2 = r2_score(y_test, y_pred_blended)\n",
    "\n",
    "print(\"\\n Loaded Blended Model Evaluation:\")\n",
    "print(f\"   RMSE: {rmse:,.2f}\")\n",
    "print(f\"   MAE : {mae:,.2f}\")\n",
    "print(f\"   R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4925e1ec-27b1-4809-bdd3-1eb39b39f7a4",
   "metadata": {},
   "source": [
    "✅ Loaded Blended Model Evaluation:\n",
    "   RMSE: 1,016,417.78\n",
    "   MAE : 81,251.71\n",
    "   R²  : 0.9334"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5d04c-b79a-495e-bc74-f9eef5e5f51e",
   "metadata": {},
   "source": [
    "### Branchwise Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd948b-dd25-453a-9666-811b82a09b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"BlendedPred\"] = y_pred_blend_weighted\n",
    "results_df[\"AbsError_Blend\"] = abs(results_df[\"Actual\"] - results_df[\"BlendedPred\"])\n",
    "\n",
    "# MAE per branch and as a percentage of cash flow\n",
    "relative_mae_blend = results_df.groupby(\"BRANCHID\").agg(\n",
    "    MAE=(\"AbsError_Blend\", \"mean\"),\n",
    "    AvgCashFlow=(\"Actual\", \"mean\")\n",
    ").assign(\n",
    "    MAE_Percent=lambda x: 100 * x[\"MAE\"] / x[\"AvgCashFlow\"]\n",
    ").reset_index()\n",
    "\n",
    "# Top risky branches\n",
    "print(\"\\n Top 10 Branches by MAE% (Blended Model):\")\n",
    "print(relative_mae_blend.sort_values(\"MAE_Percent\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f335c-7b9e-4691-8fa7-9891a7188d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_weight = 0.5\n",
    "y_pred_blend = blend_weight * y_pred_best + (1 - blend_weight) * y_pred_grid\n",
    "\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine actuals and predictions into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_blend).reset_index(drop=True)\n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# BranchID = 210\n",
    "branch_df = test_df[test_df['BRANCHID'] == 210].sort_values('TXNDATE')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Blended Prediction\", color=\"forestgreen\", linestyle='--')\n",
    "plt.title(\"Actual vs Blended Predicted NetCashFlow — BranchID = 210\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726db07-8e31-49eb-87c5-7b3a7c87c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_weight = 0.5\n",
    "y_pred_blend = blend_weight * y_pred_best + (1 - blend_weight) * y_pred_grid\n",
    "\n",
    "\n",
    "test_df = test_df.copy()\n",
    "test_df['TXNDATE'] = pd.to_datetime(test_df['TXNDATE'])\n",
    "\n",
    "# Combine actuals and predictions into test_df\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "y_pred_series = pd.Series(y_pred_blend).reset_index(drop=True)\n",
    "\n",
    "test_df['Actual'] = y_test_series\n",
    "test_df['Predicted'] = y_pred_series\n",
    "\n",
    "# BranchID = 2\n",
    "branch_df = test_df[test_df['BRANCHID'] == 2].sort_values('TXNDATE')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Predicted\"], label=\"Blended Prediction\", color=\"forestgreen\", linestyle='--')\n",
    "plt.title(\"Actual vs Blended Predicted NetCashFlow — BranchID = 2\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95f7e4-bf9e-4288-aab2-22ce8803ec4d",
   "metadata": {},
   "source": [
    "# Model Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1406e0d-031a-4cc9-9a0a-6dad5b00c454",
   "metadata": {},
   "source": [
    "### Meta-Model (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737f0a3-965f-49f1-9d51-3c868b7e1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1550, max_depth=5, learning_rate=0.1, subsample=1.0,\n",
    "    colsample_bytree=1.0, reg_alpha=0.1, reg_lambda=10, random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = LGBMRegressor(\n",
    "    num_leaves=130, learning_rate=0.0065, n_estimators=1400, max_depth=7,\n",
    "    subsample=0.673, colsample_bytree=0.934, reg_alpha=0.24, reg_lambda=0.09,\n",
    "    min_child_samples=10, min_split_gain=0.087, random_state=42\n",
    ")\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize out-of-fold prediction arrays\n",
    "xgb_oof = np.zeros(len(X_train_selected))\n",
    "lgb_oof = np.zeros(len(X_train_select_LGBM))\n",
    "\n",
    "# Loop to generate OOF predictions\n",
    "for train_idx, val_idx in tscv.split(X_train_selected):\n",
    "    # XGB\n",
    "    xgb_model.fit(X_train_selected.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    xgb_oof[val_idx] = xgb_model.predict(X_train_selected.iloc[val_idx])\n",
    "    \n",
    "    # LGB\n",
    "    lgb_model.fit(X_train_select_LGBM.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    lgb_oof[val_idx] = lgb_model.predict(X_train_select_LGBM.iloc[val_idx])\n",
    "    \n",
    "# Combine OOF predictions\n",
    "stacked_train = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_oof,\n",
    "    \"LGB_Pred\": lgb_oof\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf81fb2-442f-493e-aec6-c74e57f5a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-model (Ridge)\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "# Retrain base models on full training set\n",
    "xgb_model.fit(X_train_selected, y_train)\n",
    "lgb_model.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "xgb_test_pred = xgb_model.predict(X_test_selected)\n",
    "lgb_test_pred = lgb_model.predict(X_test_select_LGBM)\n",
    "\n",
    "# Create stacked features for test set\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_test_pred,\n",
    "    \"LGB_Pred\": lgb_test_pred\n",
    "})\n",
    "\n",
    "# Final prediction using meta-model\n",
    "final_stacked_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, final_stacked_pred))\n",
    "mae_stack = mean_absolute_error(y_test, final_stacked_pred)\n",
    "r2_stack = r2_score(y_test, final_stacked_pred)\n",
    "\n",
    "print(\"\\n Basic Stacked Model (Time Series CV) Evaluation:\")\n",
    "print(f\"   RMSE: {rmse_stack:,.2f}\")\n",
    "print(f\"   MAE : {mae_stack:,.2f}\")\n",
    "print(f\"   R²  : {r2_stack:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10f0c588-5b61-43ce-a209-c854482d0709",
   "metadata": {},
   "source": [
    " Basic Stacked Model Evaluation:\n",
    "   RMSE: 1,032,871.32\n",
    "   MAE : 173,813.32\n",
    "   R²  : 0.9312"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d8115-a0c5-4dea-bb2a-6db8ada6ba63",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor as Meta-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf62d7-c3ba-453f-869a-daf8d43a757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Ridge with Gradient Boosting\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_test_pred,\n",
    "    \"LGB_Pred\": lgb_test_pred\n",
    "})\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (GradientBoosting Meta) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "671231bb-5d82-41e5-871e-e6e020eebc8b",
   "metadata": {},
   "source": [
    "Final Stacked Model (GradientBoosting Meta) Evaluation:\n",
    "   RMSE : 1,306,046.54\n",
    "   MAE  : 107,055.97\n",
    "   R²   : 0.8900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76babff-3e49-4edd-87fc-72f61f172faf",
   "metadata": {},
   "source": [
    "### Randomised Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26292386-87dc-4771-b38e-20cca2260b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 1001, 50),         \n",
    "    'learning_rate': np.linspace(0.005, 0.2, 20),    \n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on stacked_train\n",
    "random_search.fit(stacked_train, y_train)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\n Best Parameters from RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Get the best estimator from RandomizedSearchCV\n",
    "best_meta_model = random_search.best_estimator_\n",
    "\n",
    "# Predict on stacked test data\n",
    "final_stack_pred = best_meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "# Print final performance\n",
    "print(\"\\n Evaluation of Best Meta-Model from RandomizedSearchCV:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8590f576-0c9c-4656-b6d4-5c4de1466259",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "\n",
    " Best Parameters from RandomizedSearchCV:\n",
    "{'subsample': 0.7, 'n_estimators': np.int64(150), 'max_depth': 5, 'learning_rate': np.float64(0.10763157894736843)}\n",
    "\n",
    "Evaluation of Best Meta-Model from RandomizedSearchCV:\n",
    "   RMSE : 1,294,756.94\n",
    "   MAE  : 96,921.65\n",
    "   R²   : 0.8919"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ff043-cc88-45c3-98d4-50c83f270ee4",
   "metadata": {},
   "source": [
    "### Fine-Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f738590-afc8-40e1-9b94-f3c4e116a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned grid based on previous best\n",
    "param_grid = {\n",
    "    'n_estimators': [125, 150, 175],\n",
    "    'learning_rate': [0.09, 0.10, 0.11, 0.12],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'subsample': [0.65, 0.7, 0.75]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit Grid Search on stacked_train\n",
    "grid_search.fit(stacked_train, y_train)\n",
    "\n",
    "\n",
    "print(\"\\n Best Parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict with best fine-tuned meta-model\n",
    "final_meta_model = grid_search.best_estimator_\n",
    "final_stack_pred = final_meta_model.predict(stacked_test)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Tuned GradientBoosting Meta-Model Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ef94c0a-8e0f-414b-9fa9-dcf9cf5d1e56",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
    "\n",
    " Best Parameters from GridSearchCV:\n",
    "{'learning_rate': 0.09, 'max_depth': 6, 'n_estimators': 175, 'subsample': 0.75}\n",
    "\n",
    " Final Tuned GradientBoosting Meta-Model Evaluation:\n",
    "   RMSE : 1,298,169.81\n",
    "   MAE  : 94,359.92\n",
    "   R²   : 0.8913\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eea2f2-a463-4714-b1c6-e2997e3f0477",
   "metadata": {},
   "source": [
    "### Tune min_samples_split & min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca24463-50cd-4207-90f8-8b86320d0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned grid based on previous best\n",
    "param_grid = {\n",
    "    'n_estimators': [175],\n",
    "    'learning_rate': [0.09],\n",
    "    'max_depth': [6],\n",
    "    'subsample': [0.75],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit Grid Search on stacked_train\n",
    "grid_search.fit(stacked_train, y_train)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\n Best Parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict with best fine-tuned meta-model\n",
    "final_meta_model = grid_search.best_estimator_\n",
    "final_stack_pred = final_meta_model.predict(stacked_test)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Tuned GradientBoosting Meta-Model Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6dbe103-8952-4663-8a3e-0d96db587781",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "\n",
    " Best Parameters from GridSearchCV:\n",
    "{'learning_rate': 0.09, 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 175, 'subsample': 0.75}\n",
    "\n",
    " Final Tuned GradientBoosting Meta-Model Evaluation:\n",
    "   RMSE : 1,298,169.81\n",
    "   MAE  : 94,359.92\n",
    "   R²   : 0.8913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5265488-ce5f-46f9-ba79-05694b97347e",
   "metadata": {},
   "source": [
    "### With tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb81a87-6b4f-44c2-816f-aa79de3a1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuned grid based on previous best\n",
    "param_grid = {\n",
    "    'n_estimators': [175],\n",
    "    'learning_rate': [0.09],\n",
    "    'max_depth': [6],\n",
    "    'subsample': [0.75],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Time-aware CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit Grid Search on stacked_train\n",
    "grid_search.fit(stacked_train, y_train)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\n Best Parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict with best fine-tuned meta-model\n",
    "final_meta_model = grid_search.best_estimator_\n",
    "final_stack_pred = final_meta_model.predict(stacked_test)\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Tuned GradientBoosting Meta-Model Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc5952c7-1e89-4ffa-ab0e-5fd8683a4433",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "\n",
    " Best Parameters from GridSearchCV:\n",
    "{'learning_rate': 0.09, 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 175, 'subsample': 0.75}\n",
    "\n",
    " Final Tuned GradientBoosting Meta-Model Evaluation:\n",
    "   RMSE : 1,298,169.81\n",
    "   MAE  : 94,359.92\n",
    "   R²   : 0.8913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8a9c0-4200-4d2b-abf9-ffcbeb7f98d1",
   "metadata": {},
   "source": [
    "### Extended Meta Feature Set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48ab2580-56da-46ec-8c72-fc56d3f4f8a3",
   "metadata": {},
   "source": [
    "01\n",
    "\n",
    "Base predictions:\n",
    "\n",
    "    XGB_Pred\n",
    "    \n",
    "    LGB_Pred\n",
    "\n",
    "Extended meta-features:\n",
    "\n",
    "    Avg_Pred\n",
    "    \n",
    "    Diff_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7896cf-f712-48b7-9f46-50558210f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays\n",
    "xgb_oof = np.zeros(len(X_train_selected))\n",
    "lgb_oof = np.zeros(len(X_train_select_LGBM))\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Loop over folds\n",
    "for train_idx, val_idx in tscv.split(X_train_selected):\n",
    "    # XGB\n",
    "    xgb_model.fit(X_train_selected.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    xgb_oof[val_idx] = xgb_model.predict(X_train_selected.iloc[val_idx])\n",
    "    \n",
    "    # LGBM\n",
    "    lgb_model.fit(X_train_select_LGBM.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    lgb_oof[val_idx] = lgb_model.predict(X_train_select_LGBM.iloc[val_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e52879-aa11-4e8a-a099-7d4dfa1cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train meta features\n",
    "stacked_train = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_oof,\n",
    "    \"LGB_Pred\": lgb_oof\n",
    "})\n",
    "stacked_train[\"Avg_Pred\"] = (stacked_train[\"XGB_Pred\"] + stacked_train[\"LGB_Pred\"]) / 2\n",
    "stacked_train[\"Diff_Pred\"] = stacked_train[\"XGB_Pred\"] - stacked_train[\"LGB_Pred\"]\n",
    "\n",
    "\n",
    "# Retrain base models on full training data\n",
    "xgb_model.fit(X_train_selected, y_train)\n",
    "lgb_model.fit(X_train_select_LGBM, y_train)\n",
    "\n",
    "# Test meta features\n",
    "xgb_test_pred = xgb_model.predict(X_test_selected)\n",
    "lgb_test_pred = lgb_model.predict(X_test_select_LGBM)\n",
    "\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_test_pred,\n",
    "    \"LGB_Pred\": lgb_test_pred\n",
    "})\n",
    "stacked_test[\"Avg_Pred\"] = (stacked_test[\"XGB_Pred\"] + stacked_test[\"LGB_Pred\"]) / 2\n",
    "stacked_test[\"Diff_Pred\"] = stacked_test[\"XGB_Pred\"] - stacked_test[\"LGB_Pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fa7db-87df-40d7-b6bc-45d58ae270cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=175,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aadd2143-dc28-4d3a-b1d5-7e5446d325b9",
   "metadata": {},
   "source": [
    " Final Stacked Model (Extended Features) Evaluation:\n",
    "   RMSE : 1,295,038.31\n",
    "   MAE  : 92,768.80\n",
    "   R²   : 0.8918"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50030e5d-d755-471d-b62f-f4e4cd5c6552",
   "metadata": {},
   "source": [
    "02\n",
    "\n",
    " Base Predictions\n",
    "    These are simply the raw outputs of each base model:\n",
    "    \n",
    "    XGB_Pred: prediction from your XGBoost model.\n",
    "    \n",
    "    LGB_Pred: prediction from your LightGBM model.\n",
    "\n",
    " Extended Meta-Features\n",
    "    These are engineered combinations of your base predictions to help the meta-model learn relationships:\n",
    "    \n",
    "    Feature Name\tDescription\n",
    "    Avg_Pred\tAverage of XGB and LGB predictions\n",
    "    Diff_Pred\tDifference: XGB - LGB\n",
    "    Ratio_Pred\tRatio: XGB / (LGB + ε)\n",
    "    Min_Pred\tMinimum of XGB and LGB predictions\n",
    "    Max_Pred\tMaximum of XGB and LGB predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3cb2b-6134-4087-b6f5-320efb521b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build extended meta-features for train set\n",
    "stacked_train = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_oof,\n",
    "    \"LGB_Pred\": lgb_oof\n",
    "})\n",
    "stacked_train[\"Avg_Pred\"] = (stacked_train[\"XGB_Pred\"] + stacked_train[\"LGB_Pred\"]) / 2\n",
    "stacked_train[\"Diff_Pred\"] = stacked_train[\"XGB_Pred\"] - stacked_train[\"LGB_Pred\"]\n",
    "stacked_train[\"Ratio_Pred\"] = stacked_train[\"XGB_Pred\"] / (stacked_train[\"LGB_Pred\"] + 1e-6)\n",
    "stacked_train[\"Min_Pred\"] = stacked_train[[\"XGB_Pred\", \"LGB_Pred\"]].min(axis=1)\n",
    "stacked_train[\"Max_Pred\"] = stacked_train[[\"XGB_Pred\", \"LGB_Pred\"]].max(axis=1)\n",
    "\n",
    "# Build extended meta-features for test set\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_test_pred,\n",
    "    \"LGB_Pred\": lgb_test_pred\n",
    "})\n",
    "stacked_test[\"Avg_Pred\"] = (stacked_test[\"XGB_Pred\"] + stacked_test[\"LGB_Pred\"]) / 2\n",
    "stacked_test[\"Diff_Pred\"] = stacked_test[\"XGB_Pred\"] - stacked_test[\"LGB_Pred\"]\n",
    "stacked_test[\"Ratio_Pred\"] = stacked_test[\"XGB_Pred\"] / (stacked_test[\"LGB_Pred\"] + 1e-6)\n",
    "stacked_test[\"Min_Pred\"] = stacked_test[[\"XGB_Pred\", \"LGB_Pred\"]].min(axis=1)\n",
    "stacked_test[\"Max_Pred\"] = stacked_test[[\"XGB_Pred\", \"LGB_Pred\"]].max(axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n Extended Meta Features (train):\")\n",
    "print(stacked_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6e68a-73b9-4043-9234-c862b90897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned hyperparameters\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=175,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85c36b89-dae5-497e-a479-8d1e8be42c06",
   "metadata": {},
   "source": [
    "Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,283,326.92\n",
    "   MAE  : 91,602.07\n",
    "   R²   : 0.8938"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7bf16-e956-49c3-ae00-5333bc2daf43",
   "metadata": {},
   "source": [
    "### RandomisedSearch to improve the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8dbeb-6459-410f-9f16-be078b01f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": np.arange(100, 1001, 100),\n",
    "    \"learning_rate\": np.linspace(0.01, 0.2, 20),\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Time Series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Initialize the model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,                  \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on extended stacked_train\n",
    "random_search.fit(stacked_train, y_train)\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\n Best Parameters from RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Evaluate the best estimator\n",
    "best_meta_model = random_search.best_estimator_\n",
    "final_stack_pred = best_meta_model.predict(stacked_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Tuned GradientBoosting Meta-Model (Extended Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "545e495d-91ba-4508-9ee6-e2a142bf61e0",
   "metadata": {},
   "source": [
    "itting 3 folds for each of 30 candidates, totalling 90 fits\n",
    "\n",
    " Best Parameters from RandomizedSearchCV:\n",
    "{'subsample': 1.0, 'n_estimators': np.int64(700), 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 3, 'learning_rate': np.float64(0.02)}\n",
    "\n",
    " Tuned GradientBoosting Meta-Model (Extended Features) Evaluation:\n",
    "   RMSE : 1,306,787.48\n",
    "   MAE  : 93,866.56\n",
    "   R²   : 0.8898"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fbf40-f4fe-40f1-99fb-d01ea117a37b",
   "metadata": {},
   "source": [
    "### Manually adjusting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f6c98-07f8-4c31-b603-2c88af3a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned hyperparameters\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=190,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2d42671-6fdd-4503-ace3-be142069c772",
   "metadata": {},
   "source": [
    "Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,283,199.91\n",
    "   MAE  : 91,571.81\n",
    "   R²   : 0.8938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978729b7-8ac3-4d7f-bf16-39688e2b2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned hyperparameters\n",
    "\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=250,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6b1316-4446-4af8-aeff-6a5763c0b329",
   "metadata": {},
   "source": [
    " Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,283,696.40\n",
    "   MAE  : 91,554.89\n",
    "   R²   : 0.8937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd7585-994a-4519-ad0f-4e00b252332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned hyperparameters\n",
    "\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=7,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=250,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "407b3013-309d-4da3-810c-7f6bb24eca87",
   "metadata": {},
   "source": [
    " Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,302,588.75\n",
    "   MAE  : 91,319.55\n",
    "   R²   : 0.8905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf97358-44ca-4e6e-9f9d-bf871e4b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned hyperparameters\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=7,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=500,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ece2e7-19a7-4d52-b734-4a123bb91881",
   "metadata": {},
   "source": [
    "Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,303,328.88\n",
    "   MAE  : 91,510.54\n",
    "   R²   : 0.8904\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c480db1-2080-40f8-9f66-b0d3c8d5425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=250,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0865ea0f-a622-406c-9da0-0751b4f3de8c",
   "metadata": {},
   "source": [
    "Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,284,612.68\n",
    "   MAE  : 91,985.90\n",
    "   R²   : 0.8935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6dbdc-d496-4c20-8e02-56e96450e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=500,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3527e796-e70e-492f-9412-441370c93879",
   "metadata": {},
   "source": [
    "Final Stacked Model (Extended Meta-Features) Evaluation:\n",
    "   RMSE : 1,284,645.66\n",
    "   MAE  : 92,138.33\n",
    "   R²   : 0.8935"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c265db-7cda-4f20-847a-b2de1b37a268",
   "metadata": {},
   "source": [
    "### Optimised Stack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb531c-c817-426b-b7b8-7f5bedf88a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the optimised model ###\n",
    "\n",
    "# Load saved base models and features\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "\n",
    "\n",
    "# Prepare training and test sets\n",
    "X_train_xgb = X_train[xgb_features]\n",
    "X_test_xgb = X_test[xgb_features]\n",
    "X_train_lgb = X_train[lgbm_features]\n",
    "X_test_lgb = X_test[lgbm_features]\n",
    "\n",
    "\n",
    "# Generate Out-of-Fold Predictions\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_oof = np.zeros(len(X_train_xgb))\n",
    "lgb_oof = np.zeros(len(X_train_lgb))\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train_xgb):\n",
    "    # Train and predict with XGB\n",
    "    xgb_model.fit(X_train_xgb.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    xgb_oof[val_idx] = xgb_model.predict(X_train_xgb.iloc[val_idx])\n",
    "    \n",
    "    # Train and predict with LGB\n",
    "    lgbm_model.fit(X_train_lgb.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    lgb_oof[val_idx] = lgbm_model.predict(X_train_lgb.iloc[val_idx])\n",
    "\n",
    "\n",
    "# Build Extended Meta-Features for Train\n",
    "stacked_train = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_oof,\n",
    "    \"LGB_Pred\": lgb_oof\n",
    "})\n",
    "stacked_train[\"Avg_Pred\"] = (stacked_train[\"XGB_Pred\"] + stacked_train[\"LGB_Pred\"]) / 2\n",
    "stacked_train[\"Diff_Pred\"] = stacked_train[\"XGB_Pred\"] - stacked_train[\"LGB_Pred\"]\n",
    "stacked_train[\"Ratio_Pred\"] = stacked_train[\"XGB_Pred\"] / (stacked_train[\"LGB_Pred\"] + 1e-6)\n",
    "stacked_train[\"Min_Pred\"] = stacked_train[[\"XGB_Pred\", \"LGB_Pred\"]].min(axis=1)\n",
    "stacked_train[\"Max_Pred\"] = stacked_train[[\"XGB_Pred\", \"LGB_Pred\"]].max(axis=1)\n",
    "\n",
    "\n",
    "# Train base models on full training set\n",
    "xgb_model.fit(X_train_xgb, y_train)\n",
    "lgbm_model.fit(X_train_lgb, y_train)\n",
    "\n",
    "\n",
    "# Predict base models on test set\n",
    "xgb_test_pred = xgb_model.predict(X_test_xgb)\n",
    "lgb_test_pred = lgbm_model.predict(X_test_lgb)\n",
    "\n",
    "\n",
    "# Build Extended Meta-Features for Test\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_test_pred,\n",
    "    \"LGB_Pred\": lgb_test_pred\n",
    "})\n",
    "stacked_test[\"Avg_Pred\"] = (stacked_test[\"XGB_Pred\"] + stacked_test[\"LGB_Pred\"]) / 2\n",
    "stacked_test[\"Diff_Pred\"] = stacked_test[\"XGB_Pred\"] - stacked_test[\"LGB_Pred\"]\n",
    "stacked_test[\"Ratio_Pred\"] = stacked_test[\"XGB_Pred\"] / (stacked_test[\"LGB_Pred\"] + 1e-6)\n",
    "stacked_test[\"Min_Pred\"] = stacked_test[[\"XGB_Pred\", \"LGB_Pred\"]].min(axis=1)\n",
    "stacked_test[\"Max_Pred\"] = stacked_test[[\"XGB_Pred\", \"LGB_Pred\"]].max(axis=1)\n",
    "\n",
    "\n",
    "# Train Meta-Model on Extended Features\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    learning_rate=0.09,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=190,\n",
    "    subsample=0.75,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "\n",
    "# Predict & Evaluate Final Stacked Model\n",
    "final_stack_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_stack_pred))\n",
    "mae = mean_absolute_error(y_test, final_stack_pred)\n",
    "r2 = r2_score(y_test, final_stack_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")\n",
    "\n",
    "\n",
    "# Save Final Meta Model\n",
    "with open(\"stacked_meta_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_model, f)\n",
    "\n",
    "print(\"Final stacked meta-model saved as 'stacked_meta_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a3bf2-40e5-4f0a-b1a6-c3f25a4473fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "\n",
    "with open(\"stacked_meta_model.pkl\", \"rb\") as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "# Prepare features\n",
    "X_test_xgb = X_test[xgb_features]\n",
    "X_test_lgb = X_test[lgbm_features]\n",
    "\n",
    "# Base predictions\n",
    "xgb_pred = xgb_model.predict(X_test_xgb)\n",
    "lgb_pred = lgbm_model.predict(X_test_lgb)\n",
    "\n",
    "# Meta features\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_pred,\n",
    "    \"LGB_Pred\": lgb_pred,\n",
    "    \"Avg_Pred\": (xgb_pred + lgb_pred) / 2,\n",
    "    \"Diff_Pred\": xgb_pred - lgb_pred,\n",
    "    \"Ratio_Pred\": xgb_pred / (lgb_pred + 1e-6),\n",
    "    \"Min_Pred\": np.minimum(xgb_pred, lgb_pred),\n",
    "    \"Max_Pred\": np.maximum(xgb_pred, lgb_pred)\n",
    "})\n",
    "\n",
    "# Final stacked prediction\n",
    "final_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "mae = mean_absolute_error(y_test, final_pred)\n",
    "r2 = r2_score(y_test, final_pred)\n",
    "\n",
    "print(\"\\n Final Stacked Model (Extended Meta-Features) Evaluation:\")\n",
    "print(f\"   RMSE : {rmse:,.2f}\")\n",
    "print(f\"   MAE  : {mae:,.2f}\")\n",
    "print(f\"   R²   : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "779866a2-7237-471f-8e28-86e99abfdead",
   "metadata": {},
   "source": [
    "Final Stacked Model Evaluation:\n",
    "   RMSE : 1,283,199.91\n",
    "   MAE  : 91,571.81\n",
    "   R²   : 0.8938"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010880e-555a-4ede-b81e-5a1845b13ca7",
   "metadata": {},
   "source": [
    "# Comparison of findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921ba7d-c8e0-4ef7-a493-46471714107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load models and selected features ---\n",
    "\n",
    "# XGBoost\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "X_test_xgb = X_test[xgb_features]\n",
    "xgb_pred = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# LightGBM\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "X_test_lgb = X_test[lgbm_features]\n",
    "lgb_pred = lgbm_model.predict(X_test_lgb)\n",
    "\n",
    "# Weighted Blend\n",
    "with open(\"weighted_blended_model.pkl\", \"rb\") as f:\n",
    "    blended_model = pickle.load(f)\n",
    "y_pred_blended = blended_model.predict(X_test)\n",
    "\n",
    "# Stacked Model\n",
    "with open(\"stacked_meta_model.pkl\", \"rb\") as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": xgb_pred,\n",
    "    \"LGB_Pred\": lgb_pred,\n",
    "    \"Avg_Pred\": (xgb_pred + lgb_pred) / 2,\n",
    "    \"Diff_Pred\": xgb_pred - lgb_pred,\n",
    "    \"Ratio_Pred\": xgb_pred / (lgb_pred + 1e-6),\n",
    "    \"Min_Pred\": np.minimum(xgb_pred, lgb_pred),\n",
    "    \"Max_Pred\": np.maximum(xgb_pred, lgb_pred)\n",
    "})\n",
    "final_pred = meta_model.predict(stacked_test)\n",
    "\n",
    "# --- Evaluate all models ---\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"XGBoost\", \"LightGBM\", \"Weighted Blend\", \"Stacked Model\"],\n",
    "    \"RMSE\": [],\n",
    "    \"MAE\": [],\n",
    "    \"R²\": []\n",
    "}\n",
    "\n",
    "# Calculate metrics\n",
    "for pred in [xgb_pred, lgb_pred, y_pred_blended, final_pred]:\n",
    "    rmse, mae, r2 = evaluate(y_test, pred)\n",
    "    results[\"RMSE\"].append(rmse)\n",
    "    results[\"MAE\"].append(mae)\n",
    "    results[\"R²\"].append(r2)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Format numbers for readability\n",
    "results_df[\"RMSE\"] = results_df[\"RMSE\"].map(\"{:,.2f}\".format)\n",
    "results_df[\"MAE\"] = results_df[\"MAE\"].map(\"{:,.2f}\".format)\n",
    "results_df[\"R²\"] = results_df[\"R²\"].map(\"{:.4f}\".format)\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639fe175-3ab0-4d2f-9759-4fefac48e32d",
   "metadata": {},
   "source": [
    "Model Performance Summary:\n",
    "            Model          RMSE        MAE      R²\n",
    "0         XGBoost    987,557.80  88,086.85  0.9371\n",
    "1        LightGBM  1,133,910.31  96,585.39  0.9171\n",
    "2  Weighted Blend  1,016,417.78  81,251.71  0.9334\n",
    "3   Stacked Model  1,283,199.91  91,571.81  0.8938"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0948a4-eef9-4131-a8b9-5e3333884c3d",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d950c1-86e5-458c-9371-827fd779976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models and features\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "\n",
    "with open(\"stacked_meta_model.pkl\", \"rb\") as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Ensure TXNDATE is datetime and reset index\n",
    "test_df = test_df.copy()\n",
    "test_df[\"TXNDATE\"] = pd.to_datetime(test_df[\"TXNDATE\"])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Prepare test sets\n",
    "X_test_xgb = test_df[xgb_features]\n",
    "X_test_lgb = test_df[lgbm_features]\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Get predictions from base models\n",
    "y_pred_xgb = pd.Series(xgb_model.predict(X_test_xgb)).reset_index(drop=True)\n",
    "y_pred_lgb = pd.Series(lgbm_model.predict(X_test_lgb)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Blended prediction (60% XGB + 40% LGBM)\n",
    "y_pred_blend = 0.6 * y_pred_xgb + 0.4 * y_pred_lgb\n",
    "\n",
    "\n",
    "# Build meta features for stacked prediction\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": y_pred_xgb,\n",
    "    \"LGB_Pred\": y_pred_lgb\n",
    "})\n",
    "stacked_test[\"Avg_Pred\"] = (y_pred_xgb + y_pred_lgb) / 2\n",
    "stacked_test[\"Diff_Pred\"] = y_pred_xgb - y_pred_lgb\n",
    "stacked_test[\"Ratio_Pred\"] = y_pred_xgb / (y_pred_lgb + 1e-6)\n",
    "stacked_test[\"Min_Pred\"] = np.minimum(y_pred_xgb, y_pred_lgb)\n",
    "stacked_test[\"Max_Pred\"] = np.maximum(y_pred_xgb, y_pred_lgb)\n",
    "\n",
    "\n",
    "# Final stacked prediction\n",
    "y_pred_stack = pd.Series(meta_model.predict(stacked_test)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Merge predictions into test_df\n",
    "test_df[\"Actual\"] = y_test_series\n",
    "test_df[\"XGB_Pred\"] = y_pred_xgb\n",
    "test_df[\"LGB_Pred\"] = y_pred_lgb\n",
    "test_df[\"Blended_Pred\"] = y_pred_blend\n",
    "test_df[\"Stacked_Pred\"] = y_pred_stack\n",
    "\n",
    "\n",
    "# Filter for specific BranchID and plot\n",
    "branch_id = 210 \n",
    "branch_df = test_df[test_df[\"BRANCHID\"] == branch_id].sort_values(\"TXNDATE\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\", linewidth=2)\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"XGB_Pred\"], label=\"XGB\", color=\"dodgerblue\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"LGB_Pred\"], label=\"LightGBM\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Blended_Pred\"], label=\"Blended (60% XGB + 40% LGBM)\", color=\"purple\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Stacked_Pred\"], label=\"Stacked\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.title(f\"Actual vs Predicted NetCashFlow — BranchID = {branch_id}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651f973-1a23-4931-8521-b367007d1699",
   "metadata": {},
   "source": [
    "### ML Prediction for a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfdb8b-603d-4407-8ff6-481fe513187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models and features\n",
    "with open(\"xgb_best_model.pkl\", \"rb\") as f:\n",
    "    xgb_model = pickle.load(f)\n",
    "\n",
    "with open(\"xgb_selected_features.pkl\", \"rb\") as f:\n",
    "    xgb_features = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_best_model.pkl\", \"rb\") as f:\n",
    "    lgbm_model = pickle.load(f)\n",
    "\n",
    "with open(\"lgbm_selected_features.pkl\", \"rb\") as f:\n",
    "    lgbm_features = pickle.load(f)\n",
    "\n",
    "with open(\"stacked_meta_model.pkl\", \"rb\") as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Ensure TXNDATE is datetime and reset index\n",
    "test_df = test_df.copy()\n",
    "test_df[\"TXNDATE\"] = pd.to_datetime(test_df[\"TXNDATE\"])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Prepare test sets\n",
    "X_test_xgb = test_df[xgb_features]\n",
    "X_test_lgb = test_df[lgbm_features]\n",
    "y_test_series = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Get predictions from base models\n",
    "y_pred_xgb = pd.Series(xgb_model.predict(X_test_xgb)).reset_index(drop=True)\n",
    "y_pred_lgb = pd.Series(lgbm_model.predict(X_test_lgb)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Blended prediction (60% XGB + 40% LGBM)\n",
    "y_pred_blend = 0.6 * y_pred_xgb + 0.4 * y_pred_lgb\n",
    "\n",
    "\n",
    "# Build meta features for stacked prediction\n",
    "stacked_test = pd.DataFrame({\n",
    "    \"XGB_Pred\": y_pred_xgb,\n",
    "    \"LGB_Pred\": y_pred_lgb\n",
    "})\n",
    "stacked_test[\"Avg_Pred\"] = (y_pred_xgb + y_pred_lgb) / 2\n",
    "stacked_test[\"Diff_Pred\"] = y_pred_xgb - y_pred_lgb\n",
    "stacked_test[\"Ratio_Pred\"] = y_pred_xgb / (y_pred_lgb + 1e-6)\n",
    "stacked_test[\"Min_Pred\"] = np.minimum(y_pred_xgb, y_pred_lgb)\n",
    "stacked_test[\"Max_Pred\"] = np.maximum(y_pred_xgb, y_pred_lgb)\n",
    "\n",
    "\n",
    "# Final stacked prediction\n",
    "y_pred_stack = pd.Series(meta_model.predict(stacked_test)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Merge predictions into test_df\n",
    "test_df[\"Actual\"] = y_test_series\n",
    "test_df[\"XGB_Pred\"] = y_pred_xgb\n",
    "test_df[\"LGB_Pred\"] = y_pred_lgb\n",
    "test_df[\"Blended_Pred\"] = y_pred_blend\n",
    "test_df[\"Stacked_Pred\"] = y_pred_stack\n",
    "\n",
    "\n",
    "# Filter for specific BranchID and Date Range, then plot\n",
    "branch_id = 21   \n",
    "start_date = \"2025-03-04\"   \n",
    "end_date   = \"2025-03-10\"   \n",
    "\n",
    "branch_df = test_df[test_df[\"BRANCHID\"] == branch_id].copy()\n",
    "branch_df = branch_df.sort_values(\"TXNDATE\")\n",
    "\n",
    "# Apply date filter\n",
    "mask = (branch_df[\"TXNDATE\"] >= pd.to_datetime(start_date)) & (branch_df[\"TXNDATE\"] <= pd.to_datetime(end_date))\n",
    "branch_df = branch_df.loc[mask]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\", linewidth=2)\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"XGB_Pred\"], label=\"XGB\", color=\"dodgerblue\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"LGB_Pred\"], label=\"LightGBM\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Blended_Pred\"], label=\"Blended (60% XGB + 40% LGBM)\", color=\"purple\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Stacked_Pred\"], label=\"Stacked\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.title(f\"Actual vs Predicted NetCashFlow — BranchID = {branch_id}\\n({start_date} to {end_date})\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f3c1d-df62-4352-b79d-c682189ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the date and branch you want to inspect\n",
    "query_date = pd.to_datetime(\"2025-03-20\")  \n",
    "branch_id = 210 \n",
    "\n",
    "# Filter test_df for branch and date\n",
    "subset = test_df[\n",
    "    (test_df[\"BRANCHID\"] == branch_id) & \n",
    "    (test_df[\"TXNDATE\"] == query_date)\n",
    "]\n",
    "\n",
    "if subset.empty:\n",
    "    print(f\"No data found for BranchID={branch_id} on {query_date.date()}\")\n",
    "else:\n",
    "    # Extract actual and predicted values\n",
    "    actual = subset[\"Actual\"].values[0]\n",
    "    xgb_pred = subset[\"XGB_Pred\"].values[0]\n",
    "    lgb_pred = subset[\"LGB_Pred\"].values[0]\n",
    "    blended_pred = subset[\"Blended_Pred\"].values[0]\n",
    "    stacked_pred = subset[\"Stacked_Pred\"].values[0]\n",
    "\n",
    "    # Prepare plot data\n",
    "    labels = [\"Actual\", \"XGB\", \"LightGBM\", \"Blended\", \"Stacked\"]\n",
    "    values = [actual, xgb_pred, lgb_pred, blended_pred, stacked_pred]\n",
    "    colors = [\"black\", \"dodgerblue\", \"green\", \"purple\", \"red\"]\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    # Plot each point individually to assign legend labels\n",
    "    for label, val, color in zip(labels, values, colors):\n",
    "        plt.scatter(label, val, color=color, s=150, label=label)\n",
    "        plt.text(label, val, f\"{val:,.0f}\", ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.title(f\"NetCashFlow on {query_date.date()} for BranchID {branch_id}\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322453b-7813-4564-8d4d-ea18719bbbf7",
   "metadata": {},
   "source": [
    "### Predicted values in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1853e-97e5-4f77-9584-2bb031d60dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify date range and branch\n",
    "\n",
    "start_date = pd.to_datetime(\"2025-03-10\")  \n",
    "end_date = pd.to_datetime(\"2025-03-15\")    \n",
    "branch_id = 210                            \n",
    "\n",
    "\n",
    "# Filter DataFrame for given branch and date range\n",
    "subset = test_df[\n",
    "    (test_df[\"BRANCHID\"] == branch_id) & \n",
    "    (test_df[\"TXNDATE\"] >= start_date) & \n",
    "    (test_df[\"TXNDATE\"] <= end_date)\n",
    "].copy()\n",
    "\n",
    "\n",
    "# Check if data exists and display table\n",
    "if subset.empty:\n",
    "    print(f\"No data found for BranchID={branch_id} between {start_date.date()} and {end_date.date()}\")\n",
    "else:\n",
    "    # Select and rename relevant columns\n",
    "    display_df = subset[[\n",
    "        \"TXNDATE\", \"Actual\", \"XGB_Pred\", \"LGB_Pred\", \"Blended_Pred\", \"Stacked_Pred\"\n",
    "    ]].rename(columns={\n",
    "        \"TXNDATE\": \"Date\",\n",
    "        \"Actual\": \"Actual\",\n",
    "        \"XGB_Pred\": \"XGBoost\",\n",
    "        \"LGB_Pred\": \"LightGBM\",\n",
    "        \"Blended_Pred\": \"Blended\",\n",
    "        \"Stacked_Pred\": \"Stacked\"\n",
    "    })\n",
    "\n",
    "    # Format large numbers for display \n",
    "    display_df = display_df.sort_values(\"Date\")\n",
    "    display_df[\"Date\"] = display_df[\"Date\"].dt.date \n",
    "\n",
    "    # Format numeric columns\n",
    "    formatted_df = display_df.copy()\n",
    "    for col in [\"Actual\", \"XGBoost\", \"LightGBM\", \"Blended\", \"Stacked\"]:\n",
    "        formatted_df[col] = formatted_df[col].apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "    # Show table\n",
    "    import IPython\n",
    "    from IPython.display import display\n",
    "    display(formatted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760cdc6-12c7-438e-9d54-a6eebb972ea8",
   "metadata": {},
   "source": [
    "# Part 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1facf0-e019-4792-bfb9-c73638970245",
   "metadata": {},
   "source": [
    "## Part 02 - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066bc6d-751a-4454-87b6-e38831d6b0a9",
   "metadata": {},
   "source": [
    "### Check Data Types and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422051a5-3c8b-4804-ab8b-082e9d3b341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f120d3-8299-40ec-8049-14306a1baa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step1 = df.copy()\n",
    "\n",
    "# Convert TXNDATE to datetime\n",
    "df_step1['TXNDATE'] = pd.to_datetime(df_step1['TXNDATE'], dayfirst=True)\n",
    "\n",
    "# Convert categorical columns to 'category' dtype\n",
    "categorical_cols = ['BRANCH', 'DISTRICT', 'PROVINCE', 'CODE']\n",
    "for col in categorical_cols:\n",
    "    df_step1[col] = df_step1[col].astype('category')\n",
    "\n",
    "print(df_step1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559642b9-d9c8-42b5-85eb-86eabf61c30f",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd88513-5b61-4ff8-9f73-a7ac75170c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step1 = df_step1.dropna(subset=rolling_lag_cols).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12dd3-fe2c-43aa-bea3-0e43d72a2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9436d-8ee1-410c-880a-824677f6dcd9",
   "metadata": {},
   "source": [
    "### Drop 'BRANCH' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b1dd8-b18e-4cfe-aa8b-6830fda23dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary once before dropping the column\n",
    "branch_mapping = df_step1[['BRANCHID', 'BRANCH']].drop_duplicates().set_index('BRANCHID')['BRANCH'].to_dict()\n",
    "\n",
    "# Drop the BRANCH column before modeling\n",
    "df_step2 = df_step1.drop(columns=['BRANCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b0b94-da44-43cc-a376-ae0520433561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a74c23-12b5-4795-a3e1-77f587da2be7",
   "metadata": {},
   "source": [
    "### Categorical Encoding and Binary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bc7f9-48f1-41ba-bc90-e0c49dacb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to one-hot encode\n",
    "cat_cols = ['DISTRICT', 'PROVINCE']\n",
    "\n",
    "# For binary columns, convert to int \n",
    "binary_cols = ['IsWeekend', 'IsHoliday', 'IsNonWorkingDay', 'IsMonthStart', 'IsMonthEnd', 'IsFirst5Days', 'IsLast5Days']\n",
    "\n",
    "# Convert binary columns to integer (0/1) type\n",
    "for col in binary_cols:\n",
    "    df_step2[col] = df_step2[col].astype(int)\n",
    "\n",
    "# One-hot encode categorical columns with drop_first=True to avoid multicollinearity\n",
    "df_encoded = pd.get_dummies(df_step2, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "538c1e17-22f8-4040-a1b5-81e84e3dfefc",
   "metadata": {},
   "source": [
    "One-Hot Encoding is better in LSTM because if we use label encoding here; model will think like 0<1<2 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6106df4-bd2e-493e-aafd-dd4d593d2b66",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c6174-ee72-4990-b087-f65f3d8f7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure TXNDATE is datetime\n",
    "df_encoded[\"TXNDATE\"] = pd.to_datetime(df_encoded[\"TXNDATE\"], dayfirst=True)\n",
    "\n",
    "# Sort by TXNDATE and BRANCHID\n",
    "df_encoded = df_encoded.sort_values([\"TXNDATE\", \"BRANCHID\"]).reset_index(drop=True)\n",
    "\n",
    "# Split date thresholds\n",
    "train_end = pd.Timestamp(\"2024-12-31\")\n",
    "test_start = pd.Timestamp(\"2025-01-01\")\n",
    "test_end = pd.Timestamp(\"2025-03-31\")\n",
    "\n",
    "# Training data\n",
    "train_df = df_encoded[df_encoded[\"TXNDATE\"] <= train_end].copy()\n",
    "\n",
    "# Test data\n",
    "test_df = df_encoded[\n",
    "    (df_encoded[\"TXNDATE\"] >= test_start) &\n",
    "    (df_encoded[\"TXNDATE\"] <= test_end)\n",
    "].copy()\n",
    "\n",
    "print(\"Train dates:\", train_df[\"TXNDATE\"].min(), \"to\", train_df[\"TXNDATE\"].max())\n",
    "print(\"Test dates:\", test_df[\"TXNDATE\"].min(), \"to\", test_df[\"TXNDATE\"].max())\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Columns to drop from inputs\n",
    "drop_cols = [\n",
    "    \"NetCashFlow_Per_Customer\",\n",
    "    \"TOTALTXNAMOUNT\"\n",
    "]\n",
    "\n",
    "# Drop from both train and test\n",
    "train_df = train_df.drop(columns=drop_cols)\n",
    "test_df = test_df.drop(columns=drop_cols)\n",
    "\n",
    "# Define target column\n",
    "target_col = \"NetCashFlow\"\n",
    "\n",
    "# Prepare feature and target sets\n",
    "X_train_full = train_df.drop(columns=[target_col])\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test_full = test_df.drop(columns=[target_col])\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"X_train columns:\", X_train_full.columns.tolist())\n",
    "print(\"X_train shape:\", X_train_full.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23cb1e0-e841-4c03-9601-496fa249ef26",
   "metadata": {},
   "source": [
    "### Correlation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be74a4c-709c-43b9-b17d-8ae92410c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "X_train_full_reset = X_train_full.reset_index(drop=True)\n",
    "y_train_reset = y_train.reset_index(drop=True)\n",
    "\n",
    "# Combine to a single DataFrame\n",
    "train_merged = pd.concat([X_train_full_reset, y_train_reset], axis=1)\n",
    "\n",
    "# Confirm columns\n",
    "print(train_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c46f9-369e-47f7-9ff8-e00c8cff0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce41205-22ee-40db-966b-aa66ec39c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric columns to feed LSTM\n",
    "cols_to_exclude = [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]\n",
    "numeric_cols = [c for c in train_merged.columns if c not in cols_to_exclude]\n",
    "print(\"Numeric features for LSTM:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f717c-3fe6-4965-86a0-78a740564ffc",
   "metadata": {},
   "source": [
    "### sequence length = 7 - Without log & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f65f81-cd41-451f-8e60-348793326c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seed for Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# Sequence Generator Function\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "    \n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(group.loc[i:i+seq_len-1, numeric_cols].values)\n",
    "            X_branch_seq.append(branch_id)\n",
    "            y_seq.append(group.loc[i+seq_len, target_col])\n",
    "    \n",
    "    return (\n",
    "        np.array(X_num_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).reshape(-1, 1).astype(np.int32),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "    \n",
    "# Define Sequence Length and Columns\n",
    "SEQ_LEN = 7\n",
    "numeric_cols = [c for c in X_train_full.columns if c not in [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]]\n",
    "\n",
    "\n",
    "# Prepare Train Data\n",
    "train_merged = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = generate_sequences(train_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_aligned = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "test_merged = pd.concat([\n",
    "    X_test_aligned.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = generate_sequences(test_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM Path\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46437fc6-d0d2-47d3-b515-120d11fbf9ce",
   "metadata": {},
   "source": [
    "Test MAE: 1,651,900.12\n",
    "Test RMSE: 3,959,007.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f86c6-ff62-46a9-b218-74e908efc152",
   "metadata": {},
   "source": [
    "### Rescale The Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ce48e-14e7-4511-8d5f-4d45c5d16cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize y (Target Variable)\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "y_train_scaled = y_scaler.fit_transform(y_seq.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = y_scaler.transform(y_seq_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "# Train Model on Scaled Target\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_train_scaled,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Predict & Inverse Transform\n",
    "y_pred_scaled = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "\n",
    "# Evaluate Performance\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\" Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9957e64-d759-4e14-b1f5-e88b9bbf5239",
   "metadata": {},
   "source": [
    "7-day sequence model result\n",
    "    Test MAE: 1,703,946.75\n",
    "    Test RMSE: 3,940,193.39\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ec85f-c108-411e-916a-e6ba40532a27",
   "metadata": {},
   "source": [
    "### sequence length = 30 - Without log & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bebaf2-e0a4-459a-a240-15b641f40566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set Random Seed for Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# Sequence Generator Function\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "    \n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(group.loc[i:i+seq_len-1, numeric_cols].values)\n",
    "            X_branch_seq.append(branch_id)\n",
    "            y_seq.append(group.loc[i+seq_len, target_col])\n",
    "    \n",
    "    return (\n",
    "        np.array(X_num_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).reshape(-1, 1).astype(np.int32),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "# Define Sequence Length and Columns\n",
    "SEQ_LEN = 30\n",
    "numeric_cols = [c for c in X_train_full.columns if c not in [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]]\n",
    "\n",
    "\n",
    "# Prepare Train Data\n",
    "train_merged = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = generate_sequences(train_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_aligned = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "test_merged = pd.concat([\n",
    "    X_test_aligned.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = generate_sequences(test_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM Path\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27690289-124e-4382-b3a9-451364ac87c7",
   "metadata": {},
   "source": [
    " Test MAE: 1,718,945.50\n",
    " Test RMSE: 4,144,292.46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ff624-5d17-4681-9dee-003046a1ea8d",
   "metadata": {},
   "source": [
    "### Sequence length = 30 - Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9772d-41aa-422f-9d4d-3f057602c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform\n",
    "min_target = y_train.min()\n",
    "shift_value = abs(min_target) + 1\n",
    "\n",
    "y_train_log = np.log1p(y_train + shift_value)\n",
    "\n",
    "# Sequence Generator with Efficient NumPy Access\n",
    "def create_lstm_sequences(X_array, y_array, branch_array, window):\n",
    "    X_numeric_seq = []\n",
    "    X_branch_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for i in range(window, len(X_array)):\n",
    "        X_numeric_seq.append(X_array[i - window:i])\n",
    "        X_branch_seq.append(branch_array[i])\n",
    "        y_seq.append(y_array[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "#Prepare Train Data\n",
    "X_train_full_sorted = X_train_full.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "X_array = X_train_full_sorted[numeric_cols].to_numpy(dtype=np.float32)\n",
    "y_array = y_train_log.loc[X_train_full_sorted.index].to_numpy(dtype=np.float32)\n",
    "branch_array = X_train_full_sorted[\"BRANCHID\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "SEQ_LEN = 30\n",
    "X_num_seq, X_branch_seq, y_seq_log = create_lstm_sequences(X_array, y_array, branch_array, SEQ_LEN)\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_full_reset = X_test_full.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "test_merged = pd.concat([X_test_full_reset, y_test_reset], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_test_num_seq = []\n",
    "X_test_branch_seq = []\n",
    "y_test_seq = []\n",
    "\n",
    "for branch_id, group in test_merged.groupby(\"BRANCHID\"):\n",
    "    group = group.reset_index(drop=True)\n",
    "    numeric_array = group[numeric_cols].to_numpy(dtype=np.float32)\n",
    "    target_array = group[\"NetCashFlow\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(group) - SEQ_LEN):\n",
    "        X_test_num_seq.append(numeric_array[i:i+SEQ_LEN])\n",
    "        X_test_branch_seq.append(branch_id)\n",
    "        y_test_seq.append(target_array[i + SEQ_LEN])\n",
    "\n",
    "X_num_seq_test = np.array(X_test_num_seq, dtype=np.float32)\n",
    "X_branch_seq_test = np.array(X_test_branch_seq, dtype=np.int32).reshape(-1, 1)\n",
    "y_seq_test = np.array(y_test_seq, dtype=np.float32)\n",
    "\n",
    "# Model Definition\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq_log,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Transform\n",
    "y_pred_log = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "y_pred = np.expm1(y_pred_log.flatten()) - shift_value\n",
    "\n",
    "# Evaluate Performance\n",
    "mae = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\" Test MAE: {mae:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7792d420-8314-4c88-9917-affbdb0fdf42",
   "metadata": {},
   "source": [
    " Test MAE: 154,599,365.21\n",
    " Test RMSE: 154,656,721.43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddc90d-7108-4dd0-bd3f-8040f685b0d7",
   "metadata": {},
   "source": [
    "### Sequence length = 60 - Without log & Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03b9ec93-3062-4df6-a9bd-292aa98957c8",
   "metadata": {},
   "source": [
    "Branch Embedding:\n",
    "    Learns an 8-dimensional trainable vector for each unique BRANCHID, allowing the model to capture branch-specific behavior patterns.\n",
    "\n",
    "LSTM Layer:\n",
    "    A single LSTM layer with 64 units processes the 60-day time series sequence of numeric features. It outputs only the final hidden state (not the full sequence).\n",
    "\n",
    "Dense Layer after Embedding:\n",
    "    A Dense layer with 8 units and ReLU activation transforms the flattened branch embedding before merging with LSTM output.\n",
    "\n",
    "Concatenation Layer:\n",
    "    Merges the output of the LSTM path and the transformed branch embedding into a single vector for further processing.\n",
    "\n",
    "Dense Layer (1st):\n",
    "    A fully connected layer with 64 units and ReLU activation that learns complex interactions between temporal and branch-level features.\n",
    "\n",
    "Dropout Layer:\n",
    "    Dropout with a rate of 0.3 is applied to reduce overfitting by randomly deactivating 30% of neurons during training.\n",
    "\n",
    "Dense Layer (2nd):\n",
    "    Another Dense layer with 32 units and ReLU activation for further feature extraction and non-linear transformation.\n",
    "\n",
    "Output Layer:\n",
    "    A final Dense layer with 1 unit and no activation, used for predicting the continuous NetCashFlow value (regression).\n",
    "\n",
    "Loss Function:\n",
    "    Mean Squared Error (mse) is used as the loss function, suitable for regression tasks.\n",
    "\n",
    "Metric:\n",
    "    Mean Absolute Error (mae) is used as an evaluation metric to measure average prediction error in cash flow units.\n",
    "\n",
    "Optimizer:\n",
    "    The Adam optimizer is used for efficient gradient descent with adaptive learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a6cc1-2852-489c-88b8-ccda05c0ee60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set Random Seed for Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Sequence Generator Function\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "    \n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(group.loc[i:i+seq_len-1, numeric_cols].values)\n",
    "            X_branch_seq.append(branch_id)\n",
    "            y_seq.append(group.loc[i+seq_len, target_col])\n",
    "    \n",
    "    return (\n",
    "        np.array(X_num_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).reshape(-1, 1).astype(np.int32),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "# Define Sequence Length and Columns\n",
    "SEQ_LEN = 60\n",
    "numeric_cols = [c for c in X_train_full.columns if c not in [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]]\n",
    "\n",
    "# Prepare Train Data\n",
    "train_merged = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = generate_sequences(train_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_aligned = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "test_merged = pd.concat([\n",
    "    X_test_aligned.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = generate_sequences(test_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "# Model Architecture\n",
    "\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM Path\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3def28f1-e146-4f9b-b1c4-95f610e35eaa",
   "metadata": {},
   "source": [
    "Test MAE: 1,676,476.00\n",
    "Test RMSE: 3,773,754.97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c6bea-49ec-4fb5-ac0e-76c1b820660e",
   "metadata": {},
   "source": [
    "### Sequence length = 60 - With Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a087aad-600d-4adf-9523-d0a980cc5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Random Seed for Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Sequence Generator Function\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "    \n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        \n",
    "        # Convert once per branch to NumPy arrays \n",
    "        numeric_array = group[numeric_cols].to_numpy(dtype=np.float32)\n",
    "        target_array = group[target_col].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(numeric_array[i:i+seq_len])\n",
    "            y_seq.append(target_array[i + seq_len])\n",
    "            X_branch_seq.append(branch_id)\n",
    "    \n",
    "    return (\n",
    "        np.array(X_num_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "# Define Sequence Length and Columns\n",
    "SEQ_LEN = 60\n",
    "numeric_cols = [c for c in X_train_full.columns if c not in [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]]\n",
    "\n",
    "# Prepare and Scale Train Data\n",
    "train_merged = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "# Scale numeric columns in training data\n",
    "scaler = StandardScaler()\n",
    "train_merged[numeric_cols] = scaler.fit_transform(train_merged[numeric_cols])\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = generate_sequences(train_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "# Prepare and Scale Test Data\n",
    "X_test_aligned = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "test_merged = pd.concat([\n",
    "    X_test_aligned.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "# transform test numeric data\n",
    "test_merged[numeric_cols] = scaler.transform(test_merged[numeric_cols])\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = generate_sequences(test_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM Path\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce320f10-650b-413c-9d11-956cf3d8ba2b",
   "metadata": {},
   "source": [
    " Test MAE: 1,759,810.75\n",
    " Test RMSE: 3,697,077.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c822c-9d86-4de9-9457-8c7919d7fb48",
   "metadata": {},
   "source": [
    "### Sequence length = 60 - Log Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6b31fd0-6eaa-4221-9375-cd254c4b2703",
   "metadata": {},
   "source": [
    "Can't perform log transformation directly as i have negative cash flows\n",
    "\n",
    "Solution:\n",
    "\n",
    "Shift all targets before log\n",
    "If you have negative cash flows, you can shift:\n",
    "\n",
    "𝑦(shifted) = 𝑦 + 𝐶\n",
    "where 𝐶 is a constant making all values >0 (e.g., if min=-5000, use C=6000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1eb5b9-b6d1-4705-b6bf-6dce37aafde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform\n",
    "min_target = y_train.min()\n",
    "shift_value = abs(min_target) + 1\n",
    "\n",
    "y_train_log = np.log1p(y_train + shift_value)\n",
    "\n",
    "# Sequence Generator with Efficient NumPy Access\n",
    "def create_lstm_sequences(X_array, y_array, branch_array, window):\n",
    "    X_numeric_seq = []\n",
    "    X_branch_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for i in range(window, len(X_array)):\n",
    "        X_numeric_seq.append(X_array[i - window:i])\n",
    "        X_branch_seq.append(branch_array[i])\n",
    "        y_seq.append(y_array[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "#Prepare Train Data\n",
    "X_train_full_sorted = X_train_full.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "X_array = X_train_full_sorted[numeric_cols].to_numpy(dtype=np.float32)\n",
    "y_array = y_train_log.loc[X_train_full_sorted.index].to_numpy(dtype=np.float32)\n",
    "branch_array = X_train_full_sorted[\"BRANCHID\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "SEQ_LEN = 60\n",
    "X_num_seq, X_branch_seq, y_seq_log = create_lstm_sequences(X_array, y_array, branch_array, SEQ_LEN)\n",
    "\n",
    "# Prepare Test Data \n",
    "X_test_full_reset = X_test_full.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "test_merged = pd.concat([X_test_full_reset, y_test_reset], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_test_num_seq = []\n",
    "X_test_branch_seq = []\n",
    "y_test_seq = []\n",
    "\n",
    "for branch_id, group in test_merged.groupby(\"BRANCHID\"):\n",
    "    group = group.reset_index(drop=True)\n",
    "    numeric_array = group[numeric_cols].to_numpy(dtype=np.float32)\n",
    "    target_array = group[\"NetCashFlow\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(group) - SEQ_LEN):\n",
    "        X_test_num_seq.append(numeric_array[i:i+SEQ_LEN])\n",
    "        X_test_branch_seq.append(branch_id)\n",
    "        y_test_seq.append(target_array[i + SEQ_LEN])\n",
    "\n",
    "X_num_seq_test = np.array(X_test_num_seq, dtype=np.float32)\n",
    "X_branch_seq_test = np.array(X_test_branch_seq, dtype=np.int32).reshape(-1, 1)\n",
    "y_seq_test = np.array(y_test_seq, dtype=np.float32)\n",
    "\n",
    "# Model Definition\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq_log,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Transform\n",
    "y_pred_log = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "y_pred = np.expm1(y_pred_log.flatten()) - shift_value\n",
    "\n",
    "# Evaluate Performance\n",
    "mae = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\" Test MAE: {mae:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c2c6660-acbb-455c-9d2b-616cc1dd0fcc",
   "metadata": {},
   "source": [
    " Test MAE: 145,067,499.70\n",
    " Test RMSE: 145,118,574.76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6d017-52e9-43dc-9de8-73484bc218da",
   "metadata": {},
   "source": [
    "## Improving the basic LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02cd77-1340-44e8-9301-a211f5219c6e",
   "metadata": {},
   "source": [
    "### Common sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a5724-f1b8-42d7-bbca-4489da4361a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seed for Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Optimized Sequence Generator\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        num_array = group[numeric_cols].to_numpy(dtype=np.float32)\n",
    "        target_array = group[target_col].to_numpy(dtype=np.float32)\n",
    "\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(num_array[i:i+seq_len])\n",
    "            X_branch_seq.append(branch_id)\n",
    "            y_seq.append(target_array[i + seq_len])\n",
    "\n",
    "    return (\n",
    "        np.array(X_num_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "# Define Sequence Length and Feature Columns\n",
    "SEQ_LEN = 60\n",
    "numeric_cols = [c for c in X_train_full.columns if c not in [\"TXNDATE\", \"BRANCHID\", \"NetCashFlow\"]]\n",
    "\n",
    "# Prepare Train Data\n",
    "train_merged = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged[\"TXNDATE\"] = pd.to_datetime(train_merged[\"TXNDATE\"])\n",
    "train_merged = train_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = generate_sequences(train_merged, SEQ_LEN, numeric_cols)\n",
    "\n",
    "# Prepare Test Data\n",
    "X_test_aligned = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "test_merged = pd.concat([\n",
    "    X_test_aligned.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "test_merged[\"TXNDATE\"] = pd.to_datetime(test_merged[\"TXNDATE\"])\n",
    "test_merged = test_merged.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = generate_sequences(test_merged, SEQ_LEN, numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275549cb-615a-4cb2-be32-98f803b6753a",
   "metadata": {},
   "source": [
    "### Tuned Architecture (Deep LSTM with Dropout & Dense Layers - 60 day sequence)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "564a489c-6c36-42fc-9084-89b3ef89aca5",
   "metadata": {},
   "source": [
    "Branch Embedding Layer\n",
    "\n",
    "    Learns an 8-dimensional embedding vector for each branch (BRANCHID).\n",
    "    \n",
    "    Input dimension = number of branches; output dimension = 8.\n",
    "    \n",
    "    Flattened using Reshape to combine with LSTM output later.\n",
    "\n",
    "Stacked LSTM Layers\n",
    "\n",
    "    LSTM Layer 1: 64 units, returns the full sequence (return_sequences=True) to feed into the next LSTM layer.\n",
    "    \n",
    "    Dropout 1: 0.3 rate applied after first LSTM to reduce overfitting.\n",
    "    \n",
    "    LSTM Layer 2: 32 units, returns the final hidden state (return_sequences=False).\n",
    "    \n",
    "    Dropout 2: 0.3 rate applied again for regularization.\n",
    "\n",
    "Concatenation Layer\n",
    "\n",
    "    Merges the output of the second LSTM (dropout_2) and the flattened branch embedding vector.\n",
    "\n",
    "Fully Connected Dense Layers\n",
    "\n",
    "    Dense Layer 1: 64 units with ReLU activation.\n",
    "    \n",
    "    Dropout 3: 0.2 rate applied after the first dense layer.\n",
    "    \n",
    "    Dense Layer 2: 32 units with ReLU activation for further feature transformation.\n",
    "\n",
    "Output Layer\n",
    "\n",
    "    Final Dense layer with 1 unit and no activation (linear) to predict a single continuous value (NetCashFlow).\n",
    "\n",
    "Loss Function\n",
    "\n",
    "    Mean Squared Error (mse) — standard for regression tasks.\n",
    "\n",
    "Metric\n",
    "\n",
    "    Mean Absolute Error (mae) — interpretable error metric in cash flow units.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "    Adam optimizer with a learning rate of 0.001 — efficient and adaptive optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daffa40-5e23-42fa-92d7-a1952d49c14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Reshape((8,), name=\"reshape_embedding\")(branch_embedding)\n",
    "\n",
    "# Stacked LSTM layers\n",
    "lstm_1 = LSTM(64, return_sequences=True, name=\"lstm_layer_1\")(seq_input)\n",
    "dropout_1 = Dropout(0.3, name=\"dropout_1\")(lstm_1)\n",
    "\n",
    "lstm_2 = LSTM(32, return_sequences=False, name=\"lstm_layer_2\")(dropout_1)\n",
    "dropout_2 = Dropout(0.3, name=\"dropout_2\")(lstm_2)\n",
    "\n",
    "# Merge LSTM and branch embedding\n",
    "merged = Concatenate(name=\"concat_lstm_branch\")([dropout_2, branch_embedding_flat])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "dropout_3 = Dropout(0.2, name=\"dropout_3\")(dense_1)\n",
    "dense_2 = Dense(32, activation=\"relu\", name=\"dense_2\")(dropout_3)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, name=\"output\")(dense_2)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4414748a-b8b3-4f65-a5d9-c103d4ffe711",
   "metadata": {},
   "source": [
    " Test MAE: 1,688,696.75\n",
    " Test RMSE: 3,772,795.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88caa0-df91-4ebf-ad63-9623234d10e0",
   "metadata": {},
   "source": [
    "### Basic LSTM with Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7e689-b922-4b57-bfde-16d8dd232236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- LSTM Model with Residual Connection ---\n",
    "\n",
    "# Model Architecture\n",
    "\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# Residual LSTM Block\n",
    "lstm_out = LSTM(64, return_sequences=False, name=\"main_lstm\")(seq_input)\n",
    "\n",
    "# Project input to same shape for residual connection\n",
    "residual_proj = Dense(64, name=\"residual_proj\")(seq_input[:, -1, :])  # take last timestep input\n",
    "residual_out = Add(name=\"residual_add\")([lstm_out, residual_proj])\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate(name=\"merge_lstm_branch\")([residual_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "x = Dropout(0.3, name=\"dropout_1\")(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"dense_2\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e8a3894-7940-43e6-a4a3-5dbbc71c9b78",
   "metadata": {},
   "source": [
    " Test MAE: 1,692,995.12\n",
    " Test RMSE: 3,598,599.42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866d82c-9cf2-4483-b764-0be03b6f2fc7",
   "metadata": {},
   "source": [
    "### Basic LSTM with Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b1265-28e5-4720-ba82-5385751fc566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- LSTM Model with Attention ---\n",
    "\n",
    "# Model Architecture with Attention\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Custom Attention Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "# Number of branches\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM + Attention\n",
    "lstm_out = LSTM(64, return_sequences=True)(seq_input)\n",
    "attention_out = Attention()(lstm_out)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([attention_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "923f17e0-ef54-446d-8ab9-caabe1ed6eb4",
   "metadata": {},
   "source": [
    "Test MAE: 1,684,129.12\n",
    " Test RMSE: 3,769,520.92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ee655-1b31-43ef-9f8f-b2bd2ae871ed",
   "metadata": {},
   "source": [
    "### Basic LSTM Model with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4135544-c867-41c0-b38d-d14edf908b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model with Multi-Head Attention\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Concatenate, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Number of branches\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM to generate temporal representations\n",
    "lstm_out = LSTM(64, return_sequences=True, name=\"lstm_output\")(seq_input)\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=16, name=\"multihead_attention\")(lstm_out, lstm_out)\n",
    "attention_out = LayerNormalization(epsilon=1e-6)(attention_out + lstm_out)  # Residual connection\n",
    "\n",
    "# Global Pooling after attention\n",
    "context_vector = GlobalAveragePooling1D(name=\"global_avg_pool\")(attention_out)\n",
    "\n",
    "# Merge with branch embedding\n",
    "merged = Concatenate(name=\"merged_features\")([context_vector, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(merged)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4ec7743-1f05-49a6-8209-a4683c853eae",
   "metadata": {},
   "source": [
    "Test MAE: 1,686,683.00\n",
    "Test RMSE: 3,805,151.76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a9c22-96a4-4b1e-afeb-c3f0e3055ccf",
   "metadata": {},
   "source": [
    "### Basic LSTM Model with Residuals + Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160430b0-f54c-4371-9696-f696ba00eca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Model Architecture ---\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\", name=\"branch_dense\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# LSTM Layer (with return_sequences for attention)\n",
    "lstm_out = LSTM(64, return_sequences=True, name=\"main_lstm\")(seq_input)\n",
    "\n",
    "# Multi-Head Attention\n",
    "attn_out = MultiHeadAttention(num_heads=4, key_dim=16, name=\"multihead_attention\")(lstm_out, lstm_out)\n",
    "\n",
    "# Residual Connection and Normalization\n",
    "attn_res = Add(name=\"attn_residual_add\")([lstm_out, attn_out])\n",
    "attn_norm = LayerNormalization(name=\"attn_norm\")(attn_res)\n",
    "\n",
    "# Global Average Pooling (to convert 3D to 2D before merge)\n",
    "pooled_output = GlobalAveragePooling1D(name=\"avg_pool\")(attn_norm)\n",
    "\n",
    "# Merge attention output with branch embedding\n",
    "merged = Concatenate(name=\"merge_lstm_branch\")([pooled_output, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "x = Dropout(0.3, name=\"dropout_1\")(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"dense_2\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Build & Compile Model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# --- Training ---\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Prediction & Evaluation ---\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d0271a8-2727-4b38-989b-0d6642938a93",
   "metadata": {},
   "source": [
    "Test MAE: 1,691,222.50\n",
    " Test RMSE: 3,728,978.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45088f90-e59c-47f6-afc0-0a11a47122f0",
   "metadata": {},
   "source": [
    "### Tuned LSTM with Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f86fc-a646-41a9-b683-09da4910d022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Reshape((8,), name=\"reshape_embedding\")(branch_embedding)\n",
    "\n",
    "# Project input for residual connection\n",
    "proj_input = TimeDistributed(Dense(64), name=\"proj_input\")(seq_input)\n",
    "\n",
    "# First LSTM layer + residual\n",
    "lstm_1 = LSTM(64, return_sequences=True, name=\"lstm_layer_1\")(seq_input)\n",
    "dropout_1 = Dropout(0.3, name=\"dropout_1\")(lstm_1)\n",
    "residual_1 = Add(name=\"residual_1\")([proj_input, dropout_1])\n",
    "\n",
    "# Second LSTM layer\n",
    "lstm_2 = LSTM(32, return_sequences=False, name=\"lstm_layer_2\")(residual_1)\n",
    "dropout_2 = Dropout(0.3, name=\"dropout_2\")(lstm_2)\n",
    "\n",
    "# Merge LSTM and branch embedding\n",
    "merged = Concatenate(name=\"concat_lstm_branch\")([dropout_2, branch_embedding_flat])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "dropout_3 = Dropout(0.2, name=\"dropout_3\")(dense_1)\n",
    "dense_2 = Dense(32, activation=\"relu\", name=\"dense_2\")(dropout_3)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, name=\"output\")(dense_2)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f6485d9-4cc6-434a-aca5-3277cb362af2",
   "metadata": {},
   "source": [
    "Test MAE: 1,674,884.25\n",
    " Test RMSE: 3,783,179.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d432d-c812-42b6-8180-3cec9f176a60",
   "metadata": {},
   "source": [
    "### Tuned LSTM with Residual + Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd34d20-c95b-4403-a01e-7dcb99488ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Reshape((8,), name=\"reshape_embedding\")(branch_embedding)\n",
    "\n",
    "# Project input for residual\n",
    "proj_input = TimeDistributed(Dense(64), name=\"proj_input\")(seq_input)\n",
    "\n",
    "# First LSTM layer + Dropout + Residual\n",
    "lstm_1 = LSTM(64, return_sequences=True, name=\"lstm_1\")(seq_input)\n",
    "dropout_1 = Dropout(0.3, name=\"dropout_1\")(lstm_1)\n",
    "residual_1 = Add(name=\"residual_1\")([proj_input, dropout_1])\n",
    "norm_1 = LayerNormalization(name=\"norm_1\")(residual_1)\n",
    "\n",
    "# Multi-Head Attention\n",
    "attention_output = MultiHeadAttention(num_heads=4, key_dim=16, name=\"multihead_attention\")(norm_1, norm_1)\n",
    "residual_2 = Add(name=\"residual_2\")([norm_1, attention_output])\n",
    "norm_2 = LayerNormalization(name=\"norm_2\")(residual_2)\n",
    "\n",
    "# Second LSTM layer (no return_sequences)\n",
    "lstm_2 = LSTM(32, return_sequences=False, name=\"lstm_2\")(norm_2)\n",
    "dropout_2 = Dropout(0.3, name=\"dropout_2\")(lstm_2)\n",
    "\n",
    "# Merge with branch embedding\n",
    "merged = Concatenate(name=\"merge\")([dropout_2, branch_embedding_flat])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "dropout_3 = Dropout(0.2, name=\"dropout_3\")(dense_1)\n",
    "dense_2 = Dense(32, activation=\"relu\", name=\"dense_2\")(dropout_3)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, name=\"output\")(dense_2)\n",
    "\n",
    "# Define and compile model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b7399ac-0a01-4350-919d-f012e2cab6a2",
   "metadata": {},
   "source": [
    "Test MAE: 1,696,084.62\n",
    " Test RMSE: 3,740,553.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c35263-c68d-4c1f-b54c-f7f7e624b51e",
   "metadata": {},
   "source": [
    "### Tuned LSTM with Residual Connections + Early Stopping + Batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d29ce-5058-4c13-a75d-5f736da698e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Reshape((8,), name=\"reshape_embedding\")(branch_embedding)\n",
    "\n",
    "# Project input for residual connection\n",
    "proj_input = TimeDistributed(Dense(64), name=\"proj_input\")(seq_input)\n",
    "\n",
    "# First LSTM layer + dropout + batch norm + residual\n",
    "lstm_1 = LSTM(64, return_sequences=True, name=\"lstm_layer_1\")(seq_input)\n",
    "lstm_1 = BatchNormalization(name=\"bn_lstm1\")(lstm_1)\n",
    "dropout_1 = Dropout(0.3, name=\"dropout_1\")(lstm_1)\n",
    "residual_1 = Add(name=\"residual_1\")([proj_input, dropout_1])\n",
    "\n",
    "# Second LSTM layer + dropout + batch norm\n",
    "lstm_2 = LSTM(32, return_sequences=False, name=\"lstm_layer_2\")(residual_1)\n",
    "lstm_2 = BatchNormalization(name=\"bn_lstm2\")(lstm_2)\n",
    "dropout_2 = Dropout(0.3, name=\"dropout_2\")(lstm_2)\n",
    "\n",
    "# Merge LSTM and branch embedding\n",
    "merged = Concatenate(name=\"concat_lstm_branch\")([dropout_2, branch_embedding_flat])\n",
    "\n",
    "# Dense layers with BatchNorm + Dropout\n",
    "dense_1 = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "dense_1 = BatchNormalization(name=\"bn_dense1\")(dense_1)\n",
    "dropout_3 = Dropout(0.2, name=\"dropout_3\")(dense_1)\n",
    "\n",
    "dense_2 = Dense(32, activation=\"relu\", name=\"dense_2\")(dropout_3)\n",
    "dense_2 = BatchNormalization(name=\"bn_dense2\")(dense_2)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, name=\"output\")(dense_2)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# EarlyStopping Callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cab50c1a-89c5-449c-bc25-b6c2d3a78ffb",
   "metadata": {},
   "source": [
    "Test MAE: 1,744,325.12\n",
    "Test RMSE: 3,913,073.68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d855833-dd1a-4ed6-a39e-ab9a9a4aa4e7",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bdd6d-ce36-4e3e-aaff-16abd7fa88e7",
   "metadata": {},
   "source": [
    "### Basic Bidirectional LSTM + Dropout + Dense Layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac31b909-08d7-4396-8d18-04779e570319",
   "metadata": {},
   "source": [
    "Branch Embedding Layer\n",
    "\n",
    "    Learns a unique 8-dimensional embedding vector for each BRANCHID.\n",
    "\n",
    "    Output is reshaped into a flat vector using Reshape for merging with LSTM output.\n",
    "\n",
    "Bidirectional LSTM Layer\n",
    "\n",
    "    A single Bidirectional LSTM with 64 units in each direction (forward + backward).\n",
    "\n",
    "    Captures both past and future context from the 60-day sequence of numeric features.\n",
    "\n",
    "    return_sequences=False — outputs the final state from both directions (concatenated into a 128-dim vector).\n",
    "\n",
    "Dropout Layer\n",
    "\n",
    "    Dropout with a rate of 0.3 applied after BiLSTM to prevent overfitting by randomly dropping 30% of neurons during training.\n",
    "\n",
    "Concatenation Layer\n",
    "\n",
    "    Merges the BiLSTM output (128-dim) with the flattened branch embedding (8-dim), resulting in a 136-dimensional vector.\n",
    "\n",
    "Dense Layer\n",
    "\n",
    "    Fully connected layer with 32 units and ReLU activation to learn non-linear combinations of merged features.\n",
    "\n",
    "Output Layer\n",
    "\n",
    "    A single-unit Dense layer with no activation (linear) to predict the continuous target value — NetCashFlow.\n",
    "\n",
    "Loss Function\n",
    "\n",
    "    Mean Squared Error (mse) — standard regression loss function.\n",
    "\n",
    "Metric\n",
    "\n",
    "    Mean Absolute Error (mae) — interpretable error metric in the same units as the target.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "    Adam optimizer — adaptive and widely used optimizer for training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cde043-d628-4587-a8eb-a8d8377c6f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Model Architecture\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Define inputs\n",
    "input_numeric = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"numeric_input\")\n",
    "input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Embedding layer for branch ID\n",
    "embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(input_branch)\n",
    "embedding_flat = Reshape((8,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "# Bidirectional LSTM + Dropout\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=False), name=\"bilstm\")(input_numeric)\n",
    "dropout = Dropout(0.3, name=\"dropout\")(lstm_out)\n",
    "\n",
    "# Concatenate LSTM and branch embedding\n",
    "concat = Concatenate(name=\"concat\")([dropout, embedding_flat])\n",
    "\n",
    "# Additional Dense layer\n",
    "dense = Dense(32, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a908a119-3da1-4169-abd4-bf787ee6ee5f",
   "metadata": {},
   "source": [
    "Test MAE: 1,686,443.00\n",
    " Test RMSE: 3,836,891.38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5b3bd-0522-4cc0-bb1a-be8c659d89ab",
   "metadata": {},
   "source": [
    "## Tuned Architecture (Stacked BiLSTM + Dropout + Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2edd9bf-8aa8-4ad1-ae60-327ad8b1ba80",
   "metadata": {},
   "source": [
    "### 01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de69275f-a963-4964-b767-74302f4ccabf",
   "metadata": {},
   "source": [
    "Branch Embedding Layer\n",
    "\n",
    "    Learns an 8-dimensional embedding for each BRANCHID using an Embedding layer.\n",
    "\n",
    "    Followed by a Dense layer with 8 units and ReLU activation to transform the embedding vector.\n",
    "\n",
    "Bidirectional LSTM Layer\n",
    "\n",
    "    A single Bidirectional LSTM with 64 units (per direction), resulting in a 128-dimensional output.\n",
    "\n",
    "    return_sequences=False ensures only the final combined hidden state is used (not the full sequence).\n",
    "\n",
    "    Captures both past and future dependencies in the input sequence.\n",
    "\n",
    "Concatenation Layer\n",
    "\n",
    "    Concatenates the BiLSTM output (128-dim) with the transformed branch embedding (8-dim), producing a 136-dimensional vector.\n",
    "\n",
    "Dense Layer 1\n",
    "\n",
    "    Fully connected layer with 64 units and ReLU activation to learn complex feature interactions.\n",
    "\n",
    "Dropout Layer\n",
    "\n",
    "    Dropout with a rate of 0.3 is applied after the first dense layer to reduce overfitting.\n",
    "\n",
    "Dense Layer 2\n",
    "\n",
    "    Another fully connected layer with 32 units and ReLU activation for further transformation.\n",
    "\n",
    "Output Layer\n",
    "\n",
    "    Final Dense layer with 1 unit (no activation) to output the predicted continuous value (NetCashFlow).\n",
    "\n",
    "Loss Function\n",
    "\n",
    "    Mean Squared Error (mse) is used for training, suitable for regression tasks.\n",
    "\n",
    "Metric\n",
    "\n",
    "    Mean Absolute Error (mae) is tracked for evaluation, helpful in understanding average prediction error.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "    Adam optimizer is used for efficient gradient descent with adaptive learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0a0f4-b85c-4cd9-8e36-49ef90b797cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bidirectional LSTM Model\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# Bidirectional LSTM Path\n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=False))(seq_input)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([bilstm_out, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bf7cf22-7d0f-45e2-b893-35a95f0adaf6",
   "metadata": {},
   "source": [
    " Test MAE: 1,675,974.75\n",
    " Test RMSE: 3,772,828.61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5ee2d-0c23-41e2-94b4-d6e0b9f4d1d8",
   "metadata": {},
   "source": [
    "### 02"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d48e915-31fb-43a2-973b-817d43288fc0",
   "metadata": {},
   "source": [
    "Branch Embedding Layer\n",
    "\n",
    "    Uses an Embedding layer to learn an 8-dimensional vector for each unique BRANCHID.\n",
    "\n",
    "    Output is reshaped using Reshape((8,)) to flatten the embedding for merging.\n",
    "\n",
    "First Bidirectional LSTM Layer\n",
    "\n",
    "    Bidirectional(LSTM(64, return_sequences=True))\n",
    "\n",
    "    Processes the input sequence in both forward and backward directions.\n",
    "\n",
    "    Outputs the full sequence (not just the last time step), enabling stacking.\n",
    "\n",
    "Dropout Layer 1\n",
    "\n",
    "    Dropout with a rate of 0.3 to prevent overfitting after the first BiLSTM layer.\n",
    "\n",
    "Second Bidirectional LSTM Layer\n",
    "\n",
    "    Bidirectional(LSTM(32, return_sequences=False))\n",
    "\n",
    "    Processes the sequence from the first BiLSTM.\n",
    "\n",
    "    Outputs only the final hidden state (flattened), capturing summarized sequence features.\n",
    "\n",
    "Dropout Layer 2\n",
    "\n",
    "    Dropout with a rate of 0.3 applied after the second BiLSTM layer for regularization.\n",
    "\n",
    "Concatenation Layer\n",
    "\n",
    "    Merges the output of the second BiLSTM (64-dim) with the flattened branch embedding (8-dim), resulting in a 72-dimensional feature vector.\n",
    "\n",
    "Dense Layer 1\n",
    "\n",
    "    Fully connected layer with 64 units and ReLU activation to learn non-linear interactions.\n",
    "\n",
    "Dropout Layer 3\n",
    "\n",
    "    Dropout with a rate of 0.3 applied after the first dense layer.\n",
    "\n",
    "Dense Layer 2\n",
    "\n",
    "    Fully connected layer with 32 units and ReLU activation for further transformation.\n",
    "\n",
    "Output Layer\n",
    "\n",
    "    Final Dense(1) layer to produce a single continuous value (NetCashFlow prediction).\n",
    "\n",
    "Loss Function\n",
    "\n",
    "    Mean Squared Error (mse) — suitable for regression.\n",
    "\n",
    "Metric\n",
    "\n",
    "    Mean Absolute Error (mae) — interpretable and useful for monitoring performance.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "    Adam optimizer with a learning rate of 0.001 — adaptive and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5dfa3-f369-4925-baf1-a2433a92c2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bidirectional LSTM Model\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Bidirectional LSTM Stack with Dropout and Dense\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Embedding + Reshape\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8)(branch_input)\n",
    "embedding_flat = Reshape((8,))(branch_embedding)\n",
    "\n",
    "# BiLSTM Stack\n",
    "bilstm_1 = Bidirectional(LSTM(64, return_sequences=True))(seq_input)\n",
    "dropout_1 = Dropout(0.3)(bilstm_1)\n",
    "bilstm_2 = Bidirectional(LSTM(32, return_sequences=False))(dropout_1)\n",
    "dropout_2 = Dropout(0.3)(bilstm_2)\n",
    "\n",
    "# Merge and Dense\n",
    "concat = Concatenate()([dropout_2, embedding_flat])\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33cd492c-5443-44d2-a9c2-3cd65121642a",
   "metadata": {},
   "source": [
    " Test MAE: 1,688,378.12\n",
    " Test RMSE: 3,763,182.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f01c1-6604-4556-91a8-c821d940906e",
   "metadata": {},
   "source": [
    "### Attention Mechanism - BILSTM Tuned 01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98d1ff22-5a53-4903-9319-2f8068f66e46",
   "metadata": {},
   "source": [
    "The Attention layer calculates weights for each time step.\n",
    "\n",
    "It allows the model to learn which parts of the 60-day history are more important for predicting the NetCashFlow.\n",
    "\n",
    "Unlike regular LSTM that gives a single summary, this attends to different time steps differently—enhancing interpretability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a2bc4-e662-411b-ac6a-6e5be806f233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bidirectional LSTM + Attention Model\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8)(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# BiLSTM\n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=True))(seq_input)\n",
    "\n",
    "# Attention\n",
    "score = Dense(1)(bilstm_out)  \n",
    "attention_weights = Lambda(lambda x: K.softmax(x, axis=1))(score)\n",
    "context_vector = Multiply()([bilstm_out, attention_weights])\n",
    "context_vector = Lambda(lambda x: K.sum(x, axis=1))(context_vector)\n",
    "\n",
    "# Merge\n",
    "concat = Concatenate()([context_vector, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56f0d822-b7aa-4449-9c22-2161ba7f2c27",
   "metadata": {},
   "source": [
    "Test MAE: 1,700,613.75\n",
    " Test RMSE: 3,771,406.18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcde70-edb3-4c3d-87c5-1dc023b156bf",
   "metadata": {},
   "source": [
    "### Multi-Head Attention + BiLSTM Tuned 01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c23c8807-be19-41a8-ab5f-ac4f8613dfec",
   "metadata": {},
   "source": [
    "return_sequences=True in BiLSTM to output full sequences.\n",
    "\n",
    "MultiHeadAttention(num_heads=4, key_dim=32) layer applied to BiLSTM output.\n",
    "\n",
    "LayerNormalization and residual connection (+ bilstm_out) for stability.\n",
    "\n",
    "GlobalAveragePooling1D to reduce attention output to vector before merging with branch embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fbc0a-1299-48ea-9b39-e40d9f380f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BiLSTM + Multi-Head Attention Model\n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# BiLSTM Layer (return_sequences=True for attention)\n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=True))(seq_input)\n",
    "\n",
    "# Multi-Head Attention\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=32)(bilstm_out, bilstm_out)\n",
    "attention_out = LayerNormalization(epsilon=1e-6)(attention_out + bilstm_out)\n",
    "\n",
    "# Global Average Pooling (or Flatten last timestep)\n",
    "attn_flat = tf.keras.layers.GlobalAveragePooling1D()(attention_out)\n",
    "\n",
    "# Merge Attention Output + Branch Embedding\n",
    "concat = Concatenate()([attn_flat, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Model Compile\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE : {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c557e543-49f7-4c04-9b0f-275a9ab39017",
   "metadata": {},
   "source": [
    " Test MAE : 1,688,341.75\n",
    " Test RMSE: 3,725,260.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd6ee0-b75c-4aff-acfe-1a34265698f4",
   "metadata": {},
   "source": [
    "### Multi-Head Attention + Residual Connections + EarlyStopping + BiLSTM Tuned 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff3188-d699-4c4c-b90d-91c672dc3d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inputs and Embeddings \n",
    "n_branches = int(max(X_branch_seq.max(), X_branch_seq_test.max())) + 1\n",
    "\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "# BiLSTM Block \n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=True))(seq_input)\n",
    "\n",
    "# Multi-Head Attention + Residual\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=32)(bilstm_out, bilstm_out)\n",
    "attention_out = Add()([attention_out, bilstm_out])  # Residual connection\n",
    "attention_out = LayerNormalization(epsilon=1e-6)(attention_out)\n",
    "\n",
    "# Global Pooling\n",
    "attn_flat = GlobalAveragePooling1D()(attention_out)\n",
    "\n",
    "# Concatenate with Branch Embedding\n",
    "concat = Concatenate()([attn_flat, branch_embedding_flat])\n",
    "\n",
    "# Dense Layers with Residual Connection\n",
    "dense_input = concat\n",
    "x = Dense(64, activation=\"relu\")(dense_input)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# Residual projection (if input dim ≠ x dim)\n",
    "residual_proj = Dense(32)(dense_input)\n",
    "x = Add()([x, residual_proj])  # Residual connection\n",
    "x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"\\n Test MAE : {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5bf9155-ee3e-45e7-a4b9-0358a1ebe131",
   "metadata": {},
   "source": [
    "Test MAE : 1,746,729.75\n",
    " Test RMSE: 3,921,757.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b50f4e-252b-4aee-8fdc-463223b73362",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca886e0-b4c7-47b6-8d0e-dd93fa1f49d8",
   "metadata": {},
   "source": [
    "### Common Sequnce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e628e-505c-4be8-a009-dbd0d11dc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility \n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Feature Selection\n",
    "TOP_N_FEATURES = 50\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "all_numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "X_temp_selected = X_train_full[all_numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp_selected = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "xgb_model.fit(X_temp_selected, y_temp_selected)\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_temp_selected.columns).sort_values(ascending=False)\n",
    "top_features_selected = feat_imp.head(TOP_N_FEATURES).index.tolist()\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features_selected)\n",
    "\n",
    "# Use for modeling\n",
    "numeric_cols_selected = top_features_selected\n",
    "\n",
    "# Sequence Generator (Per Branch)\n",
    "def generate_sequences(data, seq_len, numeric_cols, target_col=\"NetCashFlow\"):\n",
    "    X_num_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for branch_id, group in data.groupby(\"BRANCHID\"):\n",
    "        group = group.sort_values(\"TXNDATE\").reset_index(drop=True)\n",
    "        num_array = group[numeric_cols].to_numpy(dtype=np.float32)\n",
    "        target_array = group[target_col].to_numpy(dtype=np.float32)\n",
    "\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_num_seq.append(num_array[i:i+seq_len])\n",
    "            X_branch_seq.append(branch_id)\n",
    "            y_seq.append(target_array[i + seq_len])\n",
    "\n",
    "    return (\n",
    "        np.array(X_num_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "# Prepare Train Sequences\n",
    "SEQ_LEN = 60\n",
    "\n",
    "train_merged_selected = pd.concat([X_train_full.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "train_merged_selected[\"TXNDATE\"] = pd.to_datetime(train_merged_selected[\"TXNDATE\"])\n",
    "train_merged_selected = train_merged_selected.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_selected, X_branch_seq_selected, y_seq_selected = generate_sequences(\n",
    "    train_merged_selected, SEQ_LEN, numeric_cols_selected\n",
    ")\n",
    "\n",
    "# Prepare Test Sequences\n",
    "X_test_aligned_selected = (\n",
    "    X_test_full.drop(columns=[\"TXNDATE\"])\n",
    "    .reindex(columns=[c for c in X_train_full.columns if c != \"TXNDATE\"], fill_value=0)\n",
    ")\n",
    "\n",
    "test_merged_selected = pd.concat([\n",
    "    X_test_aligned_selected.reset_index(drop=True),\n",
    "    X_test_full[\"TXNDATE\"].reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "test_merged_selected[\"TXNDATE\"] = pd.to_datetime(test_merged_selected[\"TXNDATE\"])\n",
    "test_merged_selected = test_merged_selected.sort_values([\"BRANCHID\", \"TXNDATE\"]).reset_index(drop=True)\n",
    "\n",
    "X_num_seq_test_selected, X_branch_seq_test_selected, y_seq_test_selected = generate_sequences(\n",
    "    test_merged_selected, SEQ_LEN, numeric_cols_selected\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cbc03-db3d-4503-80f1-b7a55313ef77",
   "metadata": {},
   "source": [
    "### Feature selection - basic LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d2c8b-4988-43e0-bbe2-9d8fc4f3605b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Model Architecture\n",
    "n_branches = int(max(X_branch_seq_selected.max(), X_branch_seq_test_selected.max())) + 1\n",
    "\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols_selected)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Dense(8, activation=\"relu\")(branch_embedding[:, 0, :])\n",
    "\n",
    "lstm_out = LSTM(64, return_sequences=False)(seq_input)\n",
    "\n",
    "concat = Concatenate()([lstm_out, branch_embedding_flat])\n",
    "x = Dense(64, activation=\"relu\")(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq_selected, X_branch_seq_selected],\n",
    "    y_seq_selected,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred_selected = model.predict([X_num_seq_test_selected, X_branch_seq_test_selected])\n",
    "\n",
    "mae_test_selected = mean_absolute_error(y_seq_test_selected, y_pred_selected)\n",
    "rmse_test_selected = np.sqrt(mean_squared_error(y_seq_test_selected, y_pred_selected))\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test_selected:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse_test_selected:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2062b27a-06f5-4c15-9342-c3674fd3a586",
   "metadata": {},
   "source": [
    "Test MAE: 1,679,814.25\n",
    "Test RMSE: 3,771,309.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373215fb-0842-4235-89b3-085f386dedc0",
   "metadata": {},
   "source": [
    "### Feature selection - Tuned LSTM with Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d64d35-b7d2-4953-9423-c12b09c16db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "n_branches = int(max(X_branch_seq_selected.max(), X_branch_seq_test_selected.max())) + 1\n",
    "\n",
    "# Inputs\n",
    "seq_input = Input(shape=(SEQ_LEN, len(numeric_cols_selected)), name=\"seq_input\")\n",
    "branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "branch_embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(branch_input)\n",
    "branch_embedding_flat = Reshape((8,), name=\"reshape_embedding\")(branch_embedding)\n",
    "\n",
    "# Project input for residual connection\n",
    "proj_input = TimeDistributed(Dense(64), name=\"proj_input\")(seq_input)\n",
    "\n",
    "# First LSTM layer + residual\n",
    "lstm_1 = LSTM(64, return_sequences=True, name=\"lstm_layer_1\")(seq_input)\n",
    "dropout_1 = Dropout(0.3, name=\"dropout_1\")(lstm_1)\n",
    "residual_1 = Add(name=\"residual_1\")([proj_input, dropout_1])\n",
    "\n",
    "# Second LSTM layer\n",
    "lstm_2 = LSTM(32, return_sequences=False, name=\"lstm_layer_2\")(residual_1)\n",
    "dropout_2 = Dropout(0.3, name=\"dropout_2\")(lstm_2)\n",
    "\n",
    "# Merge LSTM and branch embedding\n",
    "merged = Concatenate(name=\"concat_lstm_branch\")([dropout_2, branch_embedding_flat])\n",
    "\n",
    "# Dense layers\n",
    "dense_1 = Dense(64, activation=\"relu\", name=\"dense_1\")(merged)\n",
    "dropout_3 = Dropout(0.2, name=\"dropout_3\")(dense_1)\n",
    "dense_2 = Dense(32, activation=\"relu\", name=\"dense_2\")(dropout_3)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, name=\"output\")(dense_2)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[seq_input, branch_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# --- Train Model ---\n",
    "history = model.fit(\n",
    "    [X_num_seq_selected, X_branch_seq_selected],\n",
    "    y_seq_selected,\n",
    "    epochs=20,#20\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred_selected = model.predict([X_num_seq_test_selected, X_branch_seq_test_selected])\n",
    "\n",
    "mae_test_selected = mean_absolute_error(y_seq_test_selected, y_pred_selected)\n",
    "rmse_test_selected = np.sqrt(mean_squared_error(y_seq_test_selected, y_pred_selected))\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test_selected:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse_test_selected:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6f74272-3891-4af5-8610-f32e65426fc1",
   "metadata": {},
   "source": [
    "Test MAE: 1,674,853.50\n",
    "Test RMSE: 3,782,385.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692216c0-d751-4f3c-abf1-f3b1ec9a237c",
   "metadata": {},
   "source": [
    "### Feature Selection - tuned 01 BiLSTM (BiLSTM + Multi-Head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4d6cc-db6c-4a12-bac3-1dcf0b57cd34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Inputs using selected features\n",
    "n_branches = int(max(X_branch_seq_selected.max(), X_branch_seq_test_selected.max())) + 1\n",
    "\n",
    "input_numeric = Input(shape=(SEQ_LEN, len(numeric_cols_selected)), name=\"numeric_input\")\n",
    "input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Branch Embedding\n",
    "embedding = Embedding(input_dim=n_branches, output_dim=8, name=\"branch_embedding\")(input_branch)\n",
    "embedding_flat = Reshape((8,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "# BiLSTM + Multi-Head Self-Attention \n",
    "bilstm_out = Bidirectional(LSTM(64, return_sequences=True), name=\"bilstm\")(input_numeric)\n",
    "\n",
    "attention_out = MultiHeadAttention(num_heads=4, key_dim=32, name=\"multihead_attention\")(\n",
    "    bilstm_out, bilstm_out\n",
    ")\n",
    "attention_out = LayerNormalization(epsilon=1e-6, name=\"layer_norm\")(\n",
    "    attention_out + bilstm_out\n",
    ")\n",
    "\n",
    "# Flatten & Merge\n",
    "attn_flat = GlobalAveragePooling1D(name=\"global_avg_pool\")(attention_out)\n",
    "concat = Concatenate(name=\"concat\")([attn_flat, embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "x = Dense(64, activation=\"relu\", name=\"dense_64\")(concat)\n",
    "x = Dropout(0.3, name=\"dropout\")(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"dense_32\")(x)\n",
    "output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model using selected feature sequences\n",
    "history = model.fit(\n",
    "    [X_num_seq_selected, X_branch_seq_selected],\n",
    "    y_seq_selected,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set with selected features\n",
    "y_pred_selected = model.predict([X_num_seq_test_selected, X_branch_seq_test_selected])\n",
    "mae_test_selected = mean_absolute_error(y_seq_test_selected, y_pred_selected)\n",
    "rmse_test_selected = np.sqrt(mean_squared_error(y_seq_test_selected, y_pred_selected))\n",
    "\n",
    "print(f\"\\nTest MAE (Selected Features): {mae_test_selected:,.2f}\")\n",
    "print(f\"Test RMSE (Selected Features): {rmse_test_selected:,.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd08e3a8-fb36-4aa4-80ce-e7ac4b24862a",
   "metadata": {},
   "source": [
    "Test MAE (Selected Features): 1,701,298.88\n",
    "Test RMSE (Selected Features): 3,772,262.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c248b2-c7bf-49c3-96eb-fa7fe7d0ff52",
   "metadata": {},
   "source": [
    "### Improved model - Feature Selection + Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b768e2d-316b-43f2-8da3-f64dcc1a55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "\n",
    "# Sequence Configuration\n",
    "SEQ_LEN = 60\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_temp.columns)\n",
    "top_features = feature_importances.nlargest(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "\n",
    "# Sequence Generator\n",
    "def create_lstm_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for i in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[i - window:i].values)\n",
    "        X_branch_seq.append(branch_series.iloc[i])\n",
    "        y_seq.append(y_series.iloc[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "# Create Sequences\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq_train, X_branch_seq_train, y_seq_train = create_lstm_sequences(\n",
    "    X_train_scaled_df.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_lstm_sequences(\n",
    "    X_test_scaled_df.reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True),\n",
    "    #pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50d562-4153-4c42-9033-742c95deabd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "input_numeric = Input(shape=(SEQ_LEN, len(top_features)), name=\"numeric_input\")\n",
    "input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "embedding = Embedding(input_dim=n_branches, output_dim=16, name=\"branch_embedding\")(input_branch)\n",
    "embedding_flat = Reshape((16,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=False), name=\"bilstm\")(input_numeric)\n",
    "dropout = Dropout(0.3, name=\"dropout\")(lstm_out)\n",
    "\n",
    "concat = Concatenate(name=\"concat\")([dropout, embedding_flat])\n",
    "dense = Dense(32, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    [X_num_seq_train, X_branch_seq_train],\n",
    "    y_seq_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "mse_test = mean_squared_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test:,.2f}\")\n",
    "print(f\"Test MSE: {mse_test:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2f0ce91-bed9-47ae-b538-970516d0f142",
   "metadata": {},
   "source": [
    "Test MAE: 1,617,951.62\n",
    "Test MSE: 14,842,533,511,168.00\n",
    "Test RMSE: 3,852,600.88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9bd30-4ff1-4379-b083-52d018c1509d",
   "metadata": {},
   "source": [
    "### + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2689bd5-2e14-4dfd-aa2c-d70b2fdc14bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Custom Attention Layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.score_dense = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.score_dense(inputs))             \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)         \n",
    "        context_vector = attention_weights * inputs              \n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)   \n",
    "        return context_vector\n",
    "\n",
    "# Define BiLSTM + Attention Model\n",
    "input_numeric = Input(shape=(SEQ_LEN, len(top_features)), name=\"numeric_input\")\n",
    "input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Embedding for branch\n",
    "embedding = Embedding(input_dim=n_branches, output_dim=16, name=\"branch_embedding\")(input_branch)\n",
    "embedding_flat = Reshape((16,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "# BiLSTM + Attention\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True), name=\"bilstm\")(input_numeric)\n",
    "attention_out = Attention(name=\"attention\")(lstm_out)\n",
    "dropout = Dropout(0.3, name=\"dropout\")(attention_out)\n",
    "\n",
    "# Concatenate with branch embedding\n",
    "concat = Concatenate(name=\"concat\")([dropout, embedding_flat])\n",
    "dense = Dense(32, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq_train, X_branch_seq_train],\n",
    "    y_seq_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "mse_test = mean_squared_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f\" Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test MSE: {mse_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4d7368-ac15-4f95-a6a3-6ef54f847ddc",
   "metadata": {},
   "source": [
    "Test MAE: 1,623,550.25\n",
    "Test RMSE: 3,867,412.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e460d-4cd8-4766-9dd7-a4c284ec2bdc",
   "metadata": {},
   "source": [
    "### Improved model - Feature Selection + Scaling + Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0687746-e616-4e43-b546-0ad980f934c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Architecture with Residual\n",
    "input_numeric = Input(shape=(SEQ_LEN, len(top_features)), name=\"numeric_input\")\n",
    "input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "# Project input for residual connection\n",
    "x_proj = GlobalAveragePooling1D()(input_numeric)  \n",
    "x_proj = Dense(128, activation=None, name=\"residual_projection\")(x_proj) \n",
    "\n",
    "# Embedding\n",
    "embedding = Embedding(input_dim=n_branches, output_dim=16, name=\"branch_embedding\")(input_branch)\n",
    "embedding_flat = Reshape((16,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "# BiLSTM Block\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=False), name=\"bilstm\")(input_numeric)\n",
    "\n",
    "# Residual Connection\n",
    "residual_out = Add(name=\"residual_add\")([lstm_out, x_proj])\n",
    "\n",
    "# Continue with Dropout and Dense\n",
    "dropout = Dropout(0.3, name=\"dropout\")(residual_out)\n",
    "concat = Concatenate(name=\"concat\")([dropout, embedding_flat])\n",
    "dense = Dense(32, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "# Compile\n",
    "model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    [X_num_seq_train, X_branch_seq_train],\n",
    "    y_seq_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred = model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "mse_test = mean_squared_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f\" Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3dbd87b-8782-49a5-abe9-6a2da0afe56e",
   "metadata": {},
   "source": [
    "Test MAE: 1,626,561.38\n",
    "Test RMSE: 3,864,962.08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b96fc-c4cc-48ea-aec2-c68c595b06d9",
   "metadata": {},
   "source": [
    "### Randomised search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffddd2-ecff-4712-a1bc-406c0ea35172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "\n",
    "# Constants\n",
    "SEQ_LEN = 60\n",
    "TOP_N_FEATURES = 50\n",
    "\n",
    "\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_temp.columns)\n",
    "top_features = feature_importances.nlargest(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "\n",
    "# Sequence Generator\n",
    "def create_lstm_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for i in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[i - window:i].values)\n",
    "        X_branch_seq.append(branch_series.iloc[i])\n",
    "        y_seq.append(y_series.iloc[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq_train, X_branch_seq_train, y_seq_train = create_lstm_sequences(\n",
    "    X_train_scaled_df.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_lstm_sequences(\n",
    "    X_test_scaled_df.reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf68989-8364-420a-8e7e-08f0bb04e717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keras Tuner Model\n",
    "def build_model(hp):\n",
    "    input_numeric = Input(shape=(SEQ_LEN, len(top_features)), name=\"numeric_input\")\n",
    "    input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "    # Embedding dimension\n",
    "    embed_dim = hp.Int(\"embed_dim\", min_value=4, max_value=32, step=4)\n",
    "    embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(input_branch)\n",
    "    embedding_flat = Reshape((embed_dim,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "    # BiLSTM\n",
    "    lstm_units = hp.Int(\"lstm_units\", min_value=32, max_value=128, step=16)\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=False), name=\"bilstm\")(input_numeric)\n",
    "\n",
    "    # Dropout\n",
    "    dropout_rate = hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    dropout = Dropout(dropout_rate)(lstm_out)\n",
    "\n",
    "    # Dense Layers\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=16, max_value=128, step=16)\n",
    "    concat = Concatenate()([dropout, embedding_flat])\n",
    "    dense = Dense(dense_units, activation=\"relu\")(concat)\n",
    "    output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "    model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "    \n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Tuner Setup\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tuner_results\",\n",
    "    project_name=\"bilstm_cashflow\"\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Search\n",
    "tuner.search(\n",
    "    [X_num_seq_train, X_branch_seq_train],\n",
    "    y_seq_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=25,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Get Best Model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_hp.values)\n",
    "print(f\"\\nTest MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa87b30-142b-41d8-b48a-79e9c0fbd176",
   "metadata": {},
   "source": [
    "Test MAE: 1,605,684.88\n",
    "Test RMSE: 3,881,449.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d4c5a-b2f0-4c4c-a023-86fdb34e6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters as dictionary\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "hp_dict = best_hps.values\n",
    "\n",
    "import json\n",
    "with open(\"p2.json\", \"w\") as f:\n",
    "    json.dump(hp_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd52e2-9753-460e-9547-7c6956ec39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    saved_hps = json.load(f)\n",
    "\n",
    "# Print the hyperparameters nicely\n",
    "print(\"Saved Hyperparameters:\")\n",
    "for key, value in saved_hps.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933300c-686d-4272-83e5-08bfb6009c5e",
   "metadata": {},
   "source": [
    "### Common Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2133b9-87d9-4ce9-b0ae-960f67b05c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Sequence Configuration\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_temp.columns)\n",
    "top_features = feature_importances.nlargest(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "# Sequence Generator\n",
    "def create_lstm_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for i in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[i - window:i].values)\n",
    "        X_branch_seq.append(branch_series.iloc[i])\n",
    "        y_seq.append(y_series.iloc[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "# Create Sequences\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq_train, X_branch_seq_train, y_seq_train = create_lstm_sequences(\n",
    "    X_train_scaled_df.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_lstm_sequences(\n",
    "    X_test_scaled_df.reset_index(drop=True),\n",
    "    #y_test.reset_index(drop=True),\n",
    "    pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff174881-d026-4672-bf60-d15bd477c083",
   "metadata": {},
   "source": [
    "### With valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9da99-39a1-4345-9979-6938b4d168b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Rebuild Model Using Saved Hyperparameters\n",
    "def build_final_lstm_model(input_shape, n_branches, best_params):\n",
    "    input_numeric = Input(shape=input_shape, name=\"numeric_input\")\n",
    "    input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "    # Embedding Layer\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(input_branch)\n",
    "    embedding_flat = Reshape((embed_dim,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "    # LSTM Block\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=False), name=\"bilstm\")(input_numeric)\n",
    "    dropout = Dropout(dropout_rate, name=\"dropout\")(lstm_out)\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    concat = Concatenate(name=\"concat\")([dropout, embedding_flat])\n",
    "    dense = Dense(dense_units, activation=\"relu\", name=\"dense_1\")(concat)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, name=\"output\")(dense)\n",
    "\n",
    "    # Build and Compile\n",
    "    model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the Model\n",
    "input_shape = (SEQ_LEN, len(top_features))  # assuming SEQ_LEN and top_features are defined\n",
    "model_1 = build_final_lstm_model(input_shape, n_branches, best_params)\n",
    "model_1.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Internal Train-Test Split from Training Sequences\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "# Train Final Model with Manual Validation Set\n",
    "history = model_1.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_1 = model_1.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_1)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_1))\n",
    "\n",
    "print(f\"\\nTest MAE: {mae_test:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse_test:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e4ee585-64e6-4b33-81f7-7fae74368572",
   "metadata": {},
   "source": [
    "Test MAE: 1,641,514.88\n",
    "Test RMSE: 3,846,820.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386e671-7e55-4feb-af35-af30852f0efa",
   "metadata": {},
   "source": [
    "### Improved structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46c91e-f07c-47be-ac87-cd13664accae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "# Rebuild Enhanced LSTM Model Using Saved Hyperparameters\n",
    "def build_final_lstm_model_with_attention(input_shape, n_branches, best_params):\n",
    "    # Inputs\n",
    "    numeric_input = Input(shape=input_shape, name=\"numeric_input\")       \n",
    "    branch_input = Input(shape=(1,), name=\"branch_input\")                \n",
    "\n",
    "    # Branch Embedding + Repeat\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    branch_embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]                                  \n",
    "    branch_embedding_repeated = RepeatVector(input_shape[0])(branch_embedding)     \n",
    "\n",
    "    # Concatenate Embedding with Numeric Input\n",
    "    x = Concatenate(name=\"concat_embedding_numeric\")([numeric_input, branch_embedding_repeated]) \n",
    "\n",
    "    # BiLSTM Block with Return Sequences\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    x = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm\")(x)\n",
    "\n",
    "    # Attention Layer\n",
    "    x = CustomAttention(name=\"attention\")(x)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    x = Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    x = Dropout(dropout_rate, name=\"dropout\")(x)\n",
    "\n",
    "    # Output Layer\n",
    "    output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "    # Build and Compile\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the Model\n",
    "input_shape = (SEQ_LEN, len(top_features))  # assuming SEQ_LEN and top_features are defined\n",
    "model_2 = build_final_lstm_model_with_attention(input_shape, n_branches, best_params)\n",
    "model_2.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Internal Train-Test Split from Training Sequences\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "# Train Final Model\n",
    "history = model_2.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_2 = model_2.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_2)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_2))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bb6a9e1-aa73-43e6-8f8f-40036ddc2434",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,646,666.12\n",
    "Final Test RMSE: 3,857,141.86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6ce2a-907d-4887-98ff-c3a94c3f91dd",
   "metadata": {},
   "source": [
    "### Residual BiLSTM + Attention for Cash Flow Forecasting (Tuned Para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e8e51-3874-496e-bc0a-1e647962d942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Build Residual BiLSTM + Attention Model\n",
    "def build_lstm_attention_residual_model(input_shape, n_branches, best_params):\n",
    "    numeric_input = Input(shape=input_shape, name=\"numeric_input\")     \n",
    "    branch_input = Input(shape=(1,), name=\"branch_input\")               \n",
    "\n",
    "    # Embedding + Repeat\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    branch_embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding_repeated = RepeatVector(input_shape[0])(branch_embedding) \n",
    "\n",
    "    # Merge numeric + embedding\n",
    "    x = Concatenate(name=\"concat_embedding_numeric\")([numeric_input, branch_embedding_repeated])  \n",
    "\n",
    "    # BiLSTM Layer\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    lstm_out = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm\")(x)\n",
    "\n",
    "    # Residual Connection (project input if needed)\n",
    "    if x.shape[-1] != lstm_out.shape[-1]:\n",
    "        x_proj = Dense(lstm_out.shape[-1], name=\"residual_projection\")(x)\n",
    "        x = Add(name=\"residual_lstm\")([lstm_out, x_proj])\n",
    "    else:\n",
    "        x = Add(name=\"residual_lstm\")([lstm_out, x])\n",
    "\n",
    "    # Attention\n",
    "    x = CustomAttention(name=\"attention\")(x)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    dense_out = Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    dense_out = Dropout(dropout_rate, name=\"dropout\")(dense_out)\n",
    "\n",
    "    # Optional Residual on Dense\n",
    "    if x.shape[-1] == dense_out.shape[-1]:\n",
    "        x = Add(name=\"residual_dense\")([x, dense_out])\n",
    "    else:\n",
    "        x = dense_out\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Set Input Shape and Build Model\n",
    "input_shape = (SEQ_LEN, len(top_features))  \n",
    "model_3 = build_lstm_attention_residual_model(input_shape, n_branches, best_params)\n",
    "model_3.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Internal Train-Test Split\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "# Train Final Model\n",
    "history = model_3.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_3 = model_3.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_3)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_3))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29061c8c-a6ed-44b9-91b7-4da155add010",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,634,241.12\n",
    "Final Test RMSE: 3,847,103.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffda53-cc4c-41e4-81ce-0d7c5a11a317",
   "metadata": {},
   "source": [
    "### Deep Residual BiLSTM + Attention Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e1e5b88-f322-4608-82e9-4c10b703af2d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "\n",
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Build Deep Residual BiLSTM + Attention Model\n",
    "def build_deep_residual_bilstm_attention(input_shape, n_branches, best_params):\n",
    "    numeric_input = Input(shape=input_shape, name=\"numeric_input\")  \n",
    "    branch_input = Input(shape=(1,), name=\"branch_input\")           \n",
    "\n",
    "    # Embedding + Repeat\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    branch_embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding_repeated = RepeatVector(input_shape[0])(branch_embedding)  \n",
    "\n",
    "    # Merge numeric + embedding\n",
    "    x = Concatenate(name=\"concat_embedding_numeric\")([numeric_input, branch_embedding_repeated])  \n",
    "\n",
    "    # BiLSTM Layer 1\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    x1 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_1\")(x)\n",
    "\n",
    "    # Residual connection 1\n",
    "    if x.shape[-1] != x1.shape[-1]:\n",
    "        x_proj1 = Dense(x1.shape[-1], name=\"residual_proj_1\")(x)\n",
    "        x1 = Add(name=\"residual_1\")([x1, x_proj1])\n",
    "    else:\n",
    "        x1 = Add(name=\"residual_1\")([x1, x])\n",
    "\n",
    "    # BiLSTM Layer 2\n",
    "    x2 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_2\")(x1)\n",
    "\n",
    "    # Residual connection 2\n",
    "    if x1.shape[-1] != x2.shape[-1]:\n",
    "        x_proj2 = Dense(x2.shape[-1], name=\"residual_proj_2\")(x1)\n",
    "        x2 = Add(name=\"residual_2\")([x2, x_proj2])\n",
    "    else:\n",
    "        x2 = Add(name=\"residual_2\")([x2, x1])\n",
    "\n",
    "    # Attention\n",
    "    x = CustomAttention(name=\"attention\")(x2)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    dense_out = Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    dense_out = Dropout(dropout_rate, name=\"dropout\")(dense_out)\n",
    "\n",
    "    # Optional residual on dense\n",
    "    if x.shape[-1] == dense_out.shape[-1]:\n",
    "        x = Add(name=\"residual_dense\")([x, dense_out])\n",
    "    else:\n",
    "        x = dense_out\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Set Input Shape and Build Model\n",
    "input_shape = (SEQ_LEN, len(top_features))  \n",
    "model_4 = build_deep_residual_bilstm_attention(input_shape, n_branches, best_params)\n",
    "model_4.summary()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Train-Validation Split\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "history = model_4.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_4 = model_4.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_4)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_4))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1dadf63-fa4b-4a30-82d3-08c68175f417",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,629,365.00\n",
    "Final Test RMSE: 3,856,631.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254c697-2ac0-49a0-826c-28b2d4d21714",
   "metadata": {},
   "source": [
    "### Deep Residual BiLSTM + Attention + LayerNormalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81770c9c-15a0-4bc5-8be0-de39805c644a",
   "metadata": {},
   "source": [
    "Branch Embedding + Repeat\n",
    "    Embedding: Learns an embed_dim-dimensional vector for each BRANCHID.\n",
    "\n",
    "    RepeatVector: Repeats the embedding across the time axis (60 timesteps) so it can be concatenated with the numeric sequence input.\n",
    "        Allows branch-specific conditioning at every timestep.\n",
    "\n",
    "Input Merging\n",
    "    Concatenate: Merges the repeated branch embedding and the numeric sequence input along the feature axis.\n",
    "        Each timestep now has both numeric features and branch context.\n",
    "\n",
    "Bidirectional LSTM Block 1 with Residual + LayerNormalization\n",
    "    Bidirectional(LSTM): Processes the sequence forward and backward with lstm_units (default: 64).\n",
    "\n",
    "    LayerNormalization: Stabilizes and accelerates training by normalizing across features.\n",
    "\n",
    "    Residual Connection: Adds the original input (or a projected version if shapes mismatch) back to the BiLSTM output.\n",
    "        Improves gradient flow and avoids vanishing gradients.\n",
    "\n",
    "Bidirectional LSTM Block 2 with Residual + LayerNormalization\n",
    "    Another BiLSTM layer with same configuration.\n",
    "\n",
    "    Residual connection is applied again between this block and the previous one, with shape adjustment if needed.\n",
    "\n",
    "Custom Attention Layer\n",
    "    CustomAttention: Computes a weighted sum of sequence features using learned attention scores over timesteps.\n",
    "        Lets the model focus on the most relevant timesteps for predicting NetCashFlow.\n",
    "\n",
    "Dense Layer + Dropout + Residual (optional)\n",
    "    Dense: A fully connected layer with dense_units (default: 32) and ReLU activation.\n",
    "\n",
    "    Dropout: Applied with dropout rate (default: 0.3) to prevent overfitting.\n",
    "\n",
    "    Optional Residual: Adds the attention output to the dense layer output if shapes match.\n",
    "\n",
    "Output Layer\n",
    "    Dense(1): Final output neuron for continuous NetCashFlow prediction (regression).\n",
    "\n",
    "Compilation\n",
    "    Loss: Mean Squared Error (mse) — suitable for regression.\n",
    "\n",
    "    Metric: Mean Absolute Error (mae) — interpretable performance metric.\n",
    "\n",
    "    Optimizer: Adam — adaptive and efficient for sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57a6fc-7a3c-4cc9-8dbf-6d79e872d07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "\n",
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Build Residual BiLSTM + Attention + LayerNormalization Model\n",
    "def build_bilstm_attention_with_normalization(input_shape, n_branches, best_params):\n",
    "    numeric_input = Input(shape=input_shape, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "    # Embedding + Repeat\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    branch_embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  # shape: (None, embed_dim)\n",
    "    branch_embedding_repeated = RepeatVector(input_shape[0])(branch_embedding)  # shape: (None, SEQ_LEN, embed_dim)\n",
    "\n",
    "    # Merge inputs\n",
    "    x = Concatenate(name=\"concat_embedding_numeric\")([numeric_input, branch_embedding_repeated])\n",
    "\n",
    "    # BiLSTM Layer 1 + LayerNorm + Residual\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    x1 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_1\")(x)\n",
    "    x1 = LayerNormalization(name=\"layernorm_1\")(x1)\n",
    "    if x.shape[-1] != x1.shape[-1]:\n",
    "        x_proj1 = Dense(x1.shape[-1], name=\"residual_proj_1\")(x)\n",
    "        x1 = Add(name=\"residual_1\")([x1, x_proj1])\n",
    "    else:\n",
    "        x1 = Add(name=\"residual_1\")([x1, x])\n",
    "\n",
    "    # BiLSTM Layer 2 + LayerNorm + Residual\n",
    "    x2 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_2\")(x1)\n",
    "    x2 = LayerNormalization(name=\"layernorm_2\")(x2)\n",
    "    if x1.shape[-1] != x2.shape[-1]:\n",
    "        x_proj2 = Dense(x2.shape[-1], name=\"residual_proj_2\")(x1)\n",
    "        x2 = Add(name=\"residual_2\")([x2, x_proj2])\n",
    "    else:\n",
    "        x2 = Add(name=\"residual_2\")([x2, x1])\n",
    "\n",
    "    # Attention Layer\n",
    "    x = CustomAttention(name=\"attention\")(x2)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    dense_out = Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    dense_out = Dropout(dropout_rate, name=\"dropout\")(dense_out)\n",
    "\n",
    "    # Optional residual on dense\n",
    "    if x.shape[-1] == dense_out.shape[-1]:\n",
    "        x = Add(name=\"residual_dense\")([x, dense_out])\n",
    "    else:\n",
    "        x = dense_out\n",
    "\n",
    "    # Output Layer\n",
    "    output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Set Input Shape and Build Model\n",
    "input_shape = (SEQ_LEN, len(top_features))\n",
    "model_5 = build_bilstm_attention_with_normalization(input_shape, n_branches, best_params)\n",
    "model_5.summary()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Train-Validation Split\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "history = model_5.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_5 = model_5.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_5)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_5))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48c2b502-76cf-4c87-8d3c-eb3e3b0bbbec",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,623,386.62\n",
    "Final Test RMSE: 3,828,668.49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5ead7-a944-4479-a916-1179fd3ce376",
   "metadata": {},
   "source": [
    "### Optimal Model - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc22ad4-d9e4-4756-a6fe-2636ac1e9e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating Sequence\n",
    "\n",
    "\n",
    "# Reproducibility Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "\n",
    "# Sequence Configuration\n",
    "SEQ_LEN = 60\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_temp.columns)\n",
    "top_features = feature_importances.nlargest(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "\n",
    "# Sequence Generator\n",
    "def create_lstm_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "\n",
    "    for i in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[i - window:i].values)\n",
    "        X_branch_seq.append(branch_series.iloc[i])\n",
    "        y_seq.append(y_series.iloc[i])\n",
    "\n",
    "    return (\n",
    "        np.array(X_numeric_seq, dtype=np.float32),\n",
    "        np.array(X_branch_seq, dtype=np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "\n",
    "# Create Sequences\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq_train, X_branch_seq_train, y_seq_train = create_lstm_sequences(\n",
    "    X_train_scaled_df.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_lstm_sequences(\n",
    "    X_test_scaled_df.reset_index(drop=True),\n",
    "    pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696842d-d79e-4476-9055-db104108fcd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "\n",
    "# Load Saved Hyperparameters\n",
    "with open(\"p2.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Build Residual BiLSTM + Attention + LayerNormalization Model\n",
    "def build_bilstm_attention_with_normalization(input_shape, n_branches, best_params):\n",
    "    numeric_input = Input(shape=input_shape, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "    # Embedding + Repeat\n",
    "    embed_dim = best_params.get(\"embed_dim\", 16)\n",
    "    branch_embedding = Embedding(input_dim=n_branches, output_dim=embed_dim, name=\"branch_embedding\")(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding_repeated = RepeatVector(input_shape[0])(branch_embedding)  \n",
    "\n",
    "    # Merge inputs\n",
    "    x = Concatenate(name=\"concat_embedding_numeric\")([numeric_input, branch_embedding_repeated])\n",
    "\n",
    "    # BiLSTM Layer 1 + LayerNorm + Residual\n",
    "    lstm_units = best_params.get(\"lstm_units\", 64)\n",
    "    x1 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_1\")(x)\n",
    "    x1 = LayerNormalization(name=\"layernorm_1\")(x1)\n",
    "    if x.shape[-1] != x1.shape[-1]:\n",
    "        x_proj1 = Dense(x1.shape[-1], name=\"residual_proj_1\")(x)\n",
    "        x1 = Add(name=\"residual_1\")([x1, x_proj1])\n",
    "    else:\n",
    "        x1 = Add(name=\"residual_1\")([x1, x])\n",
    "\n",
    "    # BiLSTM Layer 2 + LayerNorm + Residual\n",
    "    x2 = Bidirectional(LSTM(units=lstm_units, return_sequences=True), name=\"bilstm_2\")(x1)\n",
    "    x2 = LayerNormalization(name=\"layernorm_2\")(x2)\n",
    "    if x1.shape[-1] != x2.shape[-1]:\n",
    "        x_proj2 = Dense(x2.shape[-1], name=\"residual_proj_2\")(x1)\n",
    "        x2 = Add(name=\"residual_2\")([x2, x_proj2])\n",
    "    else:\n",
    "        x2 = Add(name=\"residual_2\")([x2, x1])\n",
    "\n",
    "    # Attention Layer\n",
    "    x = CustomAttention(name=\"attention\")(x2)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params.get(\"dense_units\", 32)\n",
    "    dropout_rate = best_params.get(\"dropout\", 0.3)\n",
    "    dense_out = Dense(dense_units, activation=\"relu\", name=\"dense\")(x)\n",
    "    dense_out = Dropout(dropout_rate, name=\"dropout\")(dense_out)\n",
    "\n",
    "    # Optional residual on dense\n",
    "    if x.shape[-1] == dense_out.shape[-1]:\n",
    "        x = Add(name=\"residual_dense\")([x, dense_out])\n",
    "    else:\n",
    "        x = dense_out\n",
    "\n",
    "    # Output Layer\n",
    "    output = Dense(1, name=\"output\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Set Input Shape and Build Model\n",
    "input_shape = (SEQ_LEN, len(top_features))\n",
    "lstm_model = build_bilstm_attention_with_normalization(input_shape, n_branches, best_params)\n",
    "lstm_model.summary()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "\n",
    "# Train-Validation Split\n",
    "X_num_train, X_num_val, X_branch_train, X_branch_val, y_train_new, y_val = train_test_split(\n",
    "    X_num_seq_train, X_branch_seq_train, y_seq_train, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "history = lstm_model.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_val, X_branch_val], y_val),\n",
    "    epochs=30,\n",
    "    batch_size=best_params.get(\"batch_size\", 64),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_lstm)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_lstm))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8472723-3e17-473d-835e-4f9a2e076a87",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,623,216.12\n",
    " Final Test RMSE: 3,831,612.61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb4098-e1fa-4e9b-8fae-b9fc995bc80b",
   "metadata": {},
   "source": [
    "### Save the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcdc78-a73d-4e7b-b8fe-22249182bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model with custom_objects included\n",
    "lstm_model.save(\"best_lstm_model.keras\")\n",
    "print(\" Model saved as 'best_lstm_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9b7f1-d27e-4e28-8462-dc207a49d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the model later\n",
    "\n",
    "# Re-register the custom layer so Keras can find it\n",
    "@register_keras_serializable()\n",
    "class CustomAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.W = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(self.W(inputs))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        return tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(CustomAttention, self).get_config()\n",
    "\n",
    "# Load the model for later use\n",
    "lstm_model = load_model(\"best_lstm_model.keras\", custom_objects={\"CustomAttention\": CustomAttention})\n",
    "\n",
    "# Example: Predict on test set\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred_lstm)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred_lstm))\n",
    "\n",
    "print(f\"\\n Final Test MAE: {mae_test:,.2f}\")\n",
    "print(f\" Final Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92b190f4-2896-43cf-9ca8-80ba889caa81",
   "metadata": {},
   "source": [
    "Final Test MAE: 1,615,675.50\n",
    "Final Test RMSE: 3,822,476.79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39077c16-1ecd-4a55-ac5b-953ec5610815",
   "metadata": {},
   "source": [
    "# Temporal Convolutional Network (TCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc01c9-705b-41e1-bfc6-55ed16347e30",
   "metadata": {},
   "source": [
    "### TCN Hyperparameter Tuning with KerasTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f105be6-dfe8-4505-8b7d-8eff177bd655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup \n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Feature Selection \n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_temp.columns).sort_values(ascending=False)\n",
    "top_features = feat_imp.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "    for idx in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[idx - window:idx].values)\n",
    "        X_branch_seq.append(branch_series.iloc[idx])\n",
    "        y_seq.append(y_series.iloc[idx])\n",
    "    return (\n",
    "        np.array(X_numeric_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).astype(np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = create_sequences(\n",
    "    X_train_scaled.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_sequences(\n",
    "    X_test_scaled.reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "# TCN HyperModel\n",
    "class TCNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, n_branches):\n",
    "        self.input_shape = input_shape\n",
    "        self.n_branches = n_branches\n",
    "\n",
    "    def build(self, hp):\n",
    "        input_numeric = Input(shape=self.input_shape, name=\"numeric_input\")\n",
    "        input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "        embed_dim = hp.Choice(\"embed_dim\", [8, 16, 32])\n",
    "        embedding = Embedding(input_dim=self.n_branches, output_dim=embed_dim)(input_branch)\n",
    "        embedding_flat = Reshape((embed_dim,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "        nb_filters = hp.Choice(\"nb_filters\", [32, 64, 128])\n",
    "        kernel_size = hp.Choice(\"kernel_size\", [2, 3, 5])\n",
    "        dilations = [1, 2, 4, 8]\n",
    "\n",
    "        tcn_out = TCN(\n",
    "            nb_filters=nb_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilations=dilations,\n",
    "            return_sequences=False,\n",
    "            dropout_rate=hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.1)\n",
    "        )(input_numeric)\n",
    "\n",
    "        concat = Concatenate()([tcn_out, embedding_flat])\n",
    "        dense_units = hp.Choice(\"dense_units\", [32, 64, 128])\n",
    "        dense = Dense(dense_units, activation=\"relu\")(concat)\n",
    "\n",
    "        output = Dense(1)(dense)\n",
    "\n",
    "        model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "# Tuning \n",
    "hypermodel = TCNHyperModel(\n",
    "    input_shape=(SEQ_LEN, len(top_features)),\n",
    "    n_branches=n_branches\n",
    ")\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"tcn_tuner_results\",\n",
    "    project_name=\"tcn_model_tuning\",\n",
    "    overwrite=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "tuner.search(\n",
    "    [X_num_seq, X_branch_seq],\n",
    "    y_seq,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred = best_model.predict([X_num_seq_test, X_branch_seq_test])\n",
    "mae_test = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"Best TCN Test MAE: {mae_test:,.2f}\")\n",
    "print(f\"Best TCN Test RMSE: {rmse_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "819f0854-598d-4abb-b727-05b06bbb24f4",
   "metadata": {},
   "source": [
    "Best TCN Test MAE: 1,643,242.62\n",
    "Best TCN Test RMSE: 3,858,901.58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2e7d2-2bdf-4bb4-9cd7-d43486d4b201",
   "metadata": {},
   "source": [
    "### Save Best Hyperparameters to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cc0fa-3e10-46e3-9fdf-96cf510dd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "best_params_dict = best_hps.values\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"best_tcn_hyperparams.json\", \"w\") as f:\n",
    "    json.dump(best_params_dict, f, indent=4)\n",
    "\n",
    "print(\"Best hyperparameters saved to 'best_tcn_hyperparams.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c09afe-c96b-4ff1-be43-71af225419b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the JSON file\n",
    "json_file_path = \"best_tcn_hyperparams.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Print in a nicely formatted way\n",
    "print(\"Best TCN Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e467dc68-9b4e-4f42-a8af-76b9d60663b0",
   "metadata": {},
   "source": [
    "Best TCN Hyperparameters:\n",
    "embed_dim: 16\n",
    "nb_filters: 64\n",
    "kernel_size: 2\n",
    "dropout_rate: 0.30000000000000004\n",
    "dense_units: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb8549-7031-4b27-9ea7-794c85893d52",
   "metadata": {},
   "source": [
    "### Common sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4bf96-be84-4fbc-8f0a-9a6eb81aebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_temp.columns).sort_values(ascending=False)\n",
    "top_features = feat_imp.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "    for idx in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[idx - window:idx].values)\n",
    "        X_branch_seq.append(branch_series.iloc[idx])\n",
    "        y_seq.append(y_series.iloc[idx])\n",
    "    return (\n",
    "        np.array(X_numeric_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).astype(np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = create_sequences(\n",
    "    X_train_scaled.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_sequences(\n",
    "    X_test_scaled.reset_index(drop=True),\n",
    "    #y_test.reset_index(drop=True),\n",
    "    pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_train, X_num_test, X_branch_train, X_branch_test, y_train_new, y_test = train_test_split(\n",
    "    X_num_seq, X_branch_seq, y_seq, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "input_shape_num = X_num_seq.shape[1:]\n",
    "input_shape_branch = X_branch_seq.shape[1:]\n",
    "vocab_size = int(np.max(X_branch_seq)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69d445-fc72-4730-a1b9-92c26b976ecc",
   "metadata": {},
   "source": [
    "### Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e500039-bb72-4b61-bd5e-3dbaef23d8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best parameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Rebuild the model\n",
    "def build_best_tcn_model(input_shape, n_branches, best_params):\n",
    "    input_numeric = Input(shape=input_shape, name=\"numeric_input\")\n",
    "    input_branch = Input(shape=(1,), name=\"branch_input\")\n",
    "\n",
    "    # Embedding\n",
    "    embed_dim = best_params[\"embed_dim\"]\n",
    "    embedding = Embedding(input_dim=n_branches, output_dim=embed_dim)(input_branch)\n",
    "    embedding_flat = Reshape((embed_dim,), name=\"reshape_embedding\")(embedding)\n",
    "\n",
    "    # TCN layer\n",
    "    tcn_out = TCN(\n",
    "        nb_filters=best_params[\"nb_filters\"],\n",
    "        kernel_size=best_params[\"kernel_size\"],\n",
    "        dilations=[1, 2, 4, 8],\n",
    "        dropout_rate=best_params[\"dropout_rate\"],\n",
    "        return_sequences=False\n",
    "    )(input_numeric)\n",
    "\n",
    "    concat = Concatenate()([tcn_out, embedding_flat])\n",
    "    dense = Dense(best_params[\"dense_units\"], activation=\"relu\")(concat)\n",
    "    output = Dense(1)(dense)\n",
    "\n",
    "    model = Model(inputs=[input_numeric, input_branch], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Build final model\n",
    "final_tcn_model = build_best_tcn_model(\n",
    "    input_shape=(SEQ_LEN, len(top_features)),\n",
    "    n_branches=n_branches,\n",
    "    best_params=best_params\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train or evaluate the final model\n",
    "final_tcn_model.fit(\n",
    "    [X_num_train, X_branch_train],\n",
    "    y_train_new,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred = final_tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "mae = mean_absolute_error(y_seq_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred))\n",
    "\n",
    "print(f\"Test MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed594bd-2769-430b-b8db-7080e6a1acb7",
   "metadata": {},
   "source": [
    "TCN basic model:\n",
    "    Test MAE: 1,689,392.50\n",
    "    Test RMSE: 3,879,233.81\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f1a9f-5704-4f1e-a6b5-7e547bd86fb1",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network (TCN) based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e13eb-0c8d-44b7-a504-1a5740f65f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Best Hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Loaded best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Build TCN Model\n",
    "def build_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")   \n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")  \n",
    "\n",
    "    # Embedding + Repeat\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)\n",
    "\n",
    "    # Concatenate\n",
    "    x = Concatenate()([numeric_input, branch_embedding])  \n",
    "\n",
    "    # TCN block\n",
    "    tcn_filters = best_params['nb_filters']  \n",
    "    kernel_size = best_params['kernel_size']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    x = TCN(nb_filters=tcn_filters, kernel_size=kernel_size, dropout_rate=dropout_rate, return_sequences=False)(x)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params['dense_units']\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    lr = 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build & Train Model\n",
    "model_tcn_1 = build_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "\n",
    "history = model_tcn_1.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save and Evaluate\n",
    "model_tcn_1.save(\"final_tcn_model.keras\")\n",
    "print(\" TCN model saved as 'final_tcn_model.keras'\")\n",
    "\n",
    "# Load and Predict\n",
    "model_tcn_1 = load_model(\"final_tcn_model.keras\", custom_objects={\"TCN\": TCN})\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_1 = model_tcn_1.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_1)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_1))\n",
    "\n",
    "print(f\" Final TCN Test MAE: {mae:,.2f}\")\n",
    "print(f\" Final TCN Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b285bd0-d24a-4ca1-a7ab-09548112d2f4",
   "metadata": {},
   "source": [
    "Final TCN Test MAE: 1,625,206.00\n",
    "Final TCN Test RMSE: 3,891,587.89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b7bb1-2600-4dd2-8de2-ef184db34d42",
   "metadata": {},
   "source": [
    "### Deeper TCN (stacking multiple TCN layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26bc23-f580-464d-9eee-59b38af33374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Best Hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Loaded best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# TCN Block\n",
    "def tcn_block(x, filters, kernel_size=3, dilation_rate=1):\n",
    "    x = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# Build Deeper TCN Model\n",
    "def build_deeper_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")       \n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")  \n",
    "\n",
    "    # Embedding\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]\n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)     \n",
    "\n",
    "    # Concatenate numeric + embedding\n",
    "    x = Concatenate()([numeric_input, branch_embedding])  \n",
    "\n",
    "    # TCN Stack (Deeper)\n",
    "    filters = best_params['nb_filters']  \n",
    "    for i in range(4):  \n",
    "        x = tcn_block(x, filters=filters, kernel_size=3, dilation_rate=2**i)\n",
    "\n",
    "    # Global Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params['dense_units']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build and Train the Deeper TCN Model\n",
    "model_tcn_4 = build_deeper_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "\n",
    "history = model_tcn_4.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model_tcn_4.save(\"deeper_tcn_model.keras\")\n",
    "print(\" Model saved as 'deeper_tcn_model.keras'\")\n",
    "\n",
    "# Reload model\n",
    "model_tcn_4 = tf.keras.models.load_model(\"deeper_tcn_model.keras\")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_4 = model_tcn_4.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_4)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_4))\n",
    "\n",
    "print(f\" Deeper TCN Test MAE: {mae:,.2f}\")\n",
    "print(f\" Deeper TCN Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41377748-8b05-43e8-b213-65e0eca49005",
   "metadata": {},
   "source": [
    "Deeper TCN Test MAE: 1,707,977.88\n",
    "Deeper TCN Test RMSE: 3,997,022.76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc678cee-adb6-4e8c-aa97-ec50605d248a",
   "metadata": {},
   "source": [
    "### Residual TCN Blocks (skip connections inside TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3a05b-4cd5-4248-b063-c51e07551442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Best Hyperparameters\n",
    "\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\" Loaded best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Residual TCN Block\n",
    "\n",
    "def residual_tcn_block(x, filters, kernel_size=3, dilation_rate=1):\n",
    "    shortcut = x  \n",
    "\n",
    "    # Main path\n",
    "    x = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Match dimensions if needed\n",
    "    if shortcut.shape[-1] != x.shape[-1]:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    # Add skip connection\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# Build Residual TCN Model\n",
    "\n",
    "def build_residual_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")         \n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")  \n",
    "\n",
    "    # Embedding\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]\n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)\n",
    "\n",
    "    # Concatenate\n",
    "    x = Concatenate()([numeric_input, branch_embedding])\n",
    "\n",
    "    # Residual TCN Blocks\n",
    "    filters = best_params['nb_filters']  \n",
    "    for i in range(4):\n",
    "        x = residual_tcn_block(x, filters=filters, kernel_size=best_params['kernel_size'], dilation_rate=2**i)\n",
    "\n",
    "    # Global Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dense + Dropout\n",
    "    dense_units = best_params['dense_units']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train Model\n",
    "\n",
    "model_tcn_5 = build_residual_tcn_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "\n",
    "history = model_tcn_5.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model_tcn_5.save(\"residual_tcn_model.keras\")\n",
    "print(\" Model saved as 'residual_tcn_model.keras'\")\n",
    "\n",
    "# Reload model\n",
    "model_tcn_5 = tf.keras.models.load_model(\"residual_tcn_model.keras\")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_5 = model_tcn_5.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_5)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_5))\n",
    "\n",
    "print(f\" Residual TCN Test MAE: {mae:,.2f}\")\n",
    "print(f\" Residual TCN Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83576559-9106-4d37-a0ce-334e0b120d39",
   "metadata": {},
   "source": [
    "Residual TCN Test MAE: 1,708,072.62\n",
    "Residual TCN Test RMSE: 3,997,068.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3bc9c-3127-4104-ab7d-544be3148c34",
   "metadata": {},
   "source": [
    "### Residual TCN + MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084dde85-c5eb-486f-b239-0e1463c639ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Loaded best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "# Residual TCN Block\n",
    "def residual_tcn_block(x, filters, kernel_size=3, dilation_rate=1):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Match dimensions for residual connection\n",
    "    if shortcut.shape[-1] != x.shape[-1]:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build TCN + Attention Model\n",
    "def build_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")\n",
    "\n",
    "    # Embedding for branch\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)  \n",
    "\n",
    "    # Combine branch embedding and numeric input\n",
    "    x = Concatenate()([numeric_input, branch_embedding])  \n",
    "\n",
    "    # Residual TCN layers\n",
    "    filters = best_params['nb_filters']  \n",
    "    for i in range(3):\n",
    "        x = residual_tcn_block(x, filters=filters, kernel_size=3, dilation_rate=2**i)\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=filters)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Final dense layers\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(best_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build & Train Model\n",
    "model_tcn_6 = build_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "\n",
    "history = model_tcn_6.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Save & Evaluate\n",
    "model_tcn_6.save(\"tcn_attention_model.keras\")\n",
    "print(\"Model saved as 'tcn_attention_model.keras'\")\n",
    "\n",
    "# Reload model\n",
    "model_tcn_6 = tf.keras.models.load_model(\"tcn_attention_model.keras\")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_6 = model_tcn_6.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_6)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_6))\n",
    "\n",
    "print(f\" Test MAE: {mae:,.2f}\")\n",
    "print(f\" Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21aa7833-be85-4ea7-a5b8-ed9a84d87968",
   "metadata": {},
   "source": [
    " Test MAE: 1,663,462.50\n",
    " Test RMSE: 3,871,747.79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4901b-8385-4c26-9a3a-09c9abf12474",
   "metadata": {},
   "source": [
    "### TCN-Attention Model Using Dilated Convolutions - TCN hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988fcc7-8daf-498e-8a69-466050de7a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Residual Dilated TCN Block\n",
    "def residual_dilated_tcn_block(x, filters, kernel_size=3, dilation_rate=1, dropout_rate=0.2):\n",
    "    shortcut = x\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Match shortcut dimensions\n",
    "    if shortcut.shape[-1] != x.shape[-1]:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    return Add()([shortcut, x])\n",
    "\n",
    "\n",
    "# Model Builder\n",
    "def build_dilated_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")\n",
    "\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)\n",
    "\n",
    "    # Combine numeric features and branch embedding\n",
    "    x = Concatenate()([numeric_input, branch_embedding])\n",
    "\n",
    "    # Stacked Residual Dilated TCN blocks\n",
    "    filters = best_params[\"nb_filters\"]\n",
    "    dilation_rates = [1, 2, 4]\n",
    "    for rate in dilation_rates:\n",
    "        x = residual_dilated_tcn_block(x, filters=filters, kernel_size=3, dilation_rate=rate, dropout_rate=best_params['dropout_rate'])\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=filters)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Final regression layers\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(best_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build and Train model\n",
    "model_tcn_2 = build_dilated_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "history = model_tcn_2.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model_tcn_2.save(\"dilated_tcn_attention_model.keras\")\n",
    "\n",
    "\n",
    "# Reload model\n",
    "model_tcn_2 = tf.keras.models.load_model(\"dilated_tcn_attention_model.keras\")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_2 = model_tcn_2.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_2)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_2))\n",
    "\n",
    "\n",
    "print(f\"Test MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cbc592a-c22c-4618-87af-d39f8f16978d",
   "metadata": {},
   "source": [
    "Test MAE: 1,636,442.62\n",
    "Test RMSE: 3,872,936.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcc1a3-e979-442c-aaf0-1e0e2ddf9568",
   "metadata": {},
   "source": [
    "### TCN-Attention Model with Positional Encoding and Residual Dilated Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f271185-b9df-4b58-8a3e-1be8592da1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Positional Encoding Layer\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        d_model = tf.shape(inputs)[2]\n",
    "        position = tf.cast(tf.range(seq_len)[:, tf.newaxis], tf.float32)\n",
    "        i = tf.cast(tf.range(d_model)[tf.newaxis, :], tf.float32)\n",
    "\n",
    "        angle_rates = 1 / tf.pow(10000., (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = position * angle_rates\n",
    "\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return inputs + pos_encoding[:, :seq_len, :]\n",
    "\n",
    "\n",
    "# Residual Dilated TCN Block\n",
    "def residual_dilated_tcn_block(x, filters, kernel_size=3, dilation_rate=1, dropout_rate=0.2):\n",
    "    shortcut = x\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    if shortcut.shape[-1] != x.shape[-1]:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    return Add()([shortcut, x])\n",
    "\n",
    "\n",
    "# Model Builder\n",
    "def build_improved_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")\n",
    "\n",
    "    # Embedding\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]\n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)\n",
    "\n",
    "    # Combine inputs\n",
    "    x = Concatenate()([numeric_input, branch_embedding])\n",
    "\n",
    "    # Positional Encoding\n",
    "    x = PositionalEncoding()(x)\n",
    "\n",
    "    # Residual Dilated TCN blocks\n",
    "    filters = best_params['nb_filters']\n",
    "    dilation_rates = [1, 2, 4]\n",
    "    for rate in dilation_rates:\n",
    "        x = residual_dilated_tcn_block(x, filters, kernel_size=3, dilation_rate=rate, dropout_rate=best_params['dropout_rate'])\n",
    "\n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=best_params.get('num_heads', 2), key_dim=filters)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "\n",
    "    # Global pooling and dense layers\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(best_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "\n",
    "    # Optional second dense layer\n",
    "    if 'dense_units_2' in best_params:\n",
    "        x = Dense(best_params['dense_units_2'], activation='relu')(x)\n",
    "        x = Dropout(best_params['dropout_rate'])(x)\n",
    "\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load Best Hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "best_params['num_heads'] = 4              \n",
    "best_params['dense_units_2'] = 64         \n",
    "\n",
    "\n",
    "# Train Model\n",
    "model_tcn_3 = build_improved_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "\n",
    "history = model_tcn_3.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn_3 = model_tcn_3.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn_3)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn_3))\n",
    "\n",
    "print(f\"Test MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ca3c672-7d9e-445c-b91f-7f4617154f3f",
   "metadata": {},
   "source": [
    "Test MAE: 1,666,049.00\n",
    "Test RMSE: 3,858,341.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba746b0b-8ab5-477e-b107-7aa9dce0aca5",
   "metadata": {},
   "source": [
    "### Optimised TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa4c0e-d2ad-49f5-b5b6-254befb1c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Sequence\n",
    "\n",
    "# Initial Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_temp.columns).sort_values(ascending=False)\n",
    "top_features = feat_imp.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "    for idx in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[idx - window:idx].values)\n",
    "        X_branch_seq.append(branch_series.iloc[idx])\n",
    "        y_seq.append(y_series.iloc[idx])\n",
    "    return (\n",
    "        np.array(X_numeric_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).astype(np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = create_sequences(\n",
    "    X_train_scaled.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_sequences(\n",
    "    X_test_scaled.reset_index(drop=True),\n",
    "    #y_test.reset_index(drop=True),\n",
    "    pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_train, X_num_test, X_branch_train, X_branch_test, y_train_new, y_test = train_test_split(\n",
    "    X_num_seq, X_branch_seq, y_seq, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "input_shape_num = X_num_seq.shape[1:]\n",
    "input_shape_branch = X_branch_seq.shape[1:]\n",
    "vocab_size = int(np.max(X_branch_seq)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddf920-9099-4f8b-941e-714cc22f9abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best hyperparameters\n",
    "with open(\"best_tcn_hyperparams.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Residual Dilated TCN Block\n",
    "def residual_dilated_tcn_block(x, filters, kernel_size=3, dilation_rate=1, dropout_rate=0.2):\n",
    "    shortcut = x\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Match shortcut dimensions\n",
    "    if shortcut.shape[-1] != x.shape[-1]:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    return Add()([shortcut, x])\n",
    "\n",
    "\n",
    "# Model Builder\n",
    "def build_dilated_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size):\n",
    "    numeric_input = Input(shape=input_shape_num, name=\"numeric_input\")\n",
    "    branch_input = Input(shape=(input_shape_branch[0],), name=\"branch_input\")\n",
    "\n",
    "    embed_dim = best_params['embed_dim']\n",
    "    branch_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(branch_input)\n",
    "    branch_embedding = branch_embedding[:, 0, :]  \n",
    "    branch_embedding = RepeatVector(input_shape_num[0])(branch_embedding)\n",
    "\n",
    "    # Combine numeric features and branch embedding\n",
    "    x = Concatenate()([numeric_input, branch_embedding])\n",
    "\n",
    "    # Stacked Residual Dilated TCN blocks\n",
    "    filters = best_params[\"nb_filters\"]\n",
    "    dilation_rates = [1, 2, 4]\n",
    "    for rate in dilation_rates:\n",
    "        x = residual_dilated_tcn_block(x, filters=filters, kernel_size=3, dilation_rate=rate, dropout_rate=best_params['dropout_rate'])\n",
    "\n",
    "    # Multi-head attention layer\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=filters)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Final regression layers\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(best_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[numeric_input, branch_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build and Train model\n",
    "tcn_model = build_dilated_tcn_attention_model(best_params, input_shape_num, input_shape_branch, vocab_size)\n",
    "history = tcn_model.fit(\n",
    "    [X_num_train, X_branch_train], y_train_new,\n",
    "    validation_data=([X_num_test, X_branch_test], y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn))\n",
    "\n",
    "\n",
    "print(f\"Test MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4203c83c-3fdb-43fa-bfab-94c769ddea18",
   "metadata": {},
   "source": [
    "Test MAE: 1,640,904.00\n",
    "Test RMSE: 3,857,641.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be34ab-884f-41d2-851e-762ff858015c",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656808e4-5d4c-4473-9d16-099b4be5c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the .keras format\n",
    "tcn_model.save(\"tcn_best_model.keras\")\n",
    "print(\" Model saved as 'tcn_best_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addb181-94d8-4409-84a8-1e355ba00437",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved model\n",
    "\n",
    "# Load the model\n",
    "tcn_model = tf.keras.models.load_model(\"tcn_best_model.keras\")\n",
    "\n",
    "print(\" Model loaded successfully\")\n",
    "\n",
    "# Example: Predict on test data\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_pred_tcn)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_pred_tcn))\n",
    "\n",
    "\n",
    "print(f\"Test MAE: {mae:,.2f}\")\n",
    "print(f\"Test RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1489773-3705-410f-87ac-449e057da97a",
   "metadata": {},
   "source": [
    "## Model Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd209818-a847-482d-acc5-3c120e179406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both are using same test inputs\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e346760-689c-4334-bd61-0e4eccc3fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blend Predictions - Simple average\n",
    "\n",
    "# Simple average\n",
    "y_pred_blend = (y_pred_lstm + y_pred_tcn) / 2\n",
    "\n",
    "mae_blend = mean_absolute_error(y_seq_test, y_pred_blend)\n",
    "rmse_blend = np.sqrt(mean_squared_error(y_seq_test, y_pred_blend))\n",
    "\n",
    "print(f\" Blended Test MAE: {mae_blend:,.2f}\")\n",
    "print(f\" Blended Test RMSE: {rmse_blend:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08cd87f1-7c54-4c1d-bd03-35e768af03cb",
   "metadata": {},
   "source": [
    "\n",
    "Basic Blending Model\n",
    "    Blended Test MAE: 1,630,924.50\n",
    "    Blended Test RMSE: 3,838,535.54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf955cde-5893-44e4-ba83-06a98b1c66fe",
   "metadata": {},
   "source": [
    "### Search Best Blending Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dd6c5-5fe2-4639-a62c-e85b496c576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mae = float(\"inf\")\n",
    "best_weight = 0.0\n",
    "\n",
    "for w in np.arange(0, 1.05, 0.05):\n",
    "    y_blend = w * y_pred_lstm + (1 - w) * y_pred_tcn\n",
    "    mae = mean_absolute_error(y_seq_test, y_blend)\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_weight = w\n",
    "\n",
    "print(f\" Best Blending Weight (LSTM): {best_weight:.2f}\")\n",
    "print(f\" Best Blended MAE: {best_mae:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "041a489c-0118-4c54-a9b1-87d7f0f35171",
   "metadata": {},
   "source": [
    " Best Blending Weight (LSTM): 0.90\n",
    " Best Blended MAE: 1,623,555.47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45825c-7c9d-4bd9-acde-ebd9709e72e5",
   "metadata": {},
   "source": [
    "### Manual Blending Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff1fc2-fc66-4cca-a8bb-92c9b53cdec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions if not already\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# List of weights to try (LSTM weight, TCN weight is 1 - w)\n",
    "manual_weights = [0.3, 0.4, 0.5, 0.55, 0.6, 0.7]\n",
    "\n",
    "print(\"Manual Blending Results:\\n\")\n",
    "for w in manual_weights:\n",
    "    y_blend = w * y_pred_lstm + (1 - w) * y_pred_tcn\n",
    "    mae = mean_absolute_error(y_seq_test, y_blend)\n",
    "    rmse = np.sqrt(mean_squared_error(y_seq_test, y_blend))\n",
    "    print(f\"LSTM Weight: {w:.2f} | TCN Weight: {1 - w:.2f} -> MAE: {mae:,.2f}, RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47d6970a-129b-40e6-ab9e-6d8fe07966fc",
   "metadata": {},
   "source": [
    "LSTM Weight: 0.30 | TCN Weight: 0.70 -> MAE: 1,639,789.62, RMSE: 3,844,756.06\n",
    "LSTM Weight: 0.40 | TCN Weight: 0.60 -> MAE: 1,635,023.00, RMSE: 3,841,476.60\n",
    "LSTM Weight: 0.50 | TCN Weight: 0.50 -> MAE: 1,630,924.50, RMSE: 3,838,535.54\n",
    "LSTM Weight: 0.55 | TCN Weight: 0.45 -> MAE: 1,629,190.75, RMSE: 3,837,192.12\n",
    "LSTM Weight: 0.60 | TCN Weight: 0.40 -> MAE: 1,627,679.25, RMSE: 3,835,933.39\n",
    "LSTM Weight: 0.70 | TCN Weight: 0.30 -> MAE: 1,625,340.25, RMSE: 3,833,671.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c0c0c-3f52-47b0-a558-009e29c8c410",
   "metadata": {},
   "source": [
    "### Optimal Blending model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5b9af-d559-498e-b7f9-81ae89dc28f6",
   "metadata": {},
   "source": [
    "### Saving the optimal blending model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f5d10-2761-44d0-bd60-9bf5cc313a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package both models and best weight\n",
    "blend_package = {\n",
    "    \"lstm_model\": lstm_model,         \n",
    "    \"tcn_model\": tcn_model,           \n",
    "    \"best_weight\": best_weight        \n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open(\"dl_best_blend_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_package, f)\n",
    "\n",
    "print(\" Blended model saved as 'dl_best_blend_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59433aa-241a-4b25-82b7-969fbff83012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blended model\n",
    "with open(\"dl_best_blend_model.pkl\", \"rb\") as f:\n",
    "    blend_package = pickle.load(f)\n",
    "\n",
    "lstm_model_loaded = blend_package[\"lstm_model\"]\n",
    "tcn_model_loaded = blend_package[\"tcn_model\"]\n",
    "best_weight_loaded = blend_package[\"best_weight\"]\n",
    "\n",
    "print(f\" Loaded best weight: {best_weight_loaded:.2f}\")\n",
    "\n",
    "# Predict using loaded models\n",
    "y_pred_lstm_loaded = lstm_model_loaded.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn_loaded = tcn_model_loaded.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# Blend predictions\n",
    "y_pred_blend = best_weight_loaded * y_pred_lstm_loaded + (1 - best_weight_loaded) * y_pred_tcn_loaded\n",
    "\n",
    "# Assuming y_seq_test is your true test targets\n",
    "\n",
    "mae_blend = mean_absolute_error(y_seq_test, y_pred_blend)\n",
    "rmse_blend = np.sqrt(mean_squared_error(y_seq_test, y_pred_blend))\n",
    "r2_blend = r2_score(y_seq_test, y_pred_blend)\n",
    "\n",
    "print(f\"Blended Model Test MAE: {mae_blend:,.2f}\")\n",
    "print(f\"Blended Model Test RMSE: {rmse_blend:,.2f}\")\n",
    "print(f\"Blended Model Test R²: {r2_blend:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "787bb770-d544-439f-add7-f79e82d5b7b9",
   "metadata": {},
   "source": [
    "Final Blending Model\n",
    "Blended Model MAE: 1,623,555.47\n",
    "Blended Model RMSE: 3,830,167.53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4b414-86a7-4c35-88e8-8e2168d6460d",
   "metadata": {},
   "source": [
    "## Model Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c721aa-cd56-42fb-9728-8e5102b4b182",
   "metadata": {},
   "source": [
    "### Get validation predictions from base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c088e9-b9fc-4593-92ee-68220d069be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LSTM and TCN predictions on validation data\n",
    "\n",
    "val_preds_lstm = lstm_model.predict([X_num_val, X_branch_val]).flatten()\n",
    "val_preds_tcn  = tcn_model.predict([X_num_val, X_branch_val]).flatten()\n",
    "\n",
    "# Stack base model predictions as features\n",
    "X_meta_train = np.vstack((val_preds_lstm, val_preds_tcn)).T  \n",
    "y_meta_train = y_val  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8889a-4145-46ef-9f4b-17e7e3a0063b",
   "metadata": {},
   "source": [
    "### Get test predictions for final stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebaebd2-48fb-4b8a-87fa-158a7d87091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base model predictions on test set\n",
    "test_preds_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "test_preds_tcn  = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# Stack test predictions\n",
    "X_meta_test = np.vstack((test_preds_lstm, test_preds_tcn)).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643e49c-6c5f-4ebb-815f-d296c3986fde",
   "metadata": {},
   "source": [
    "### Train meta-learner (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffdd4d-dd81-4af9-b579-70ea04aeb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learner\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Final prediction\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluation\n",
    "mae_stack = mean_absolute_error(y_seq_test, y_pred_stack)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_seq_test, y_pred_stack))\n",
    "\n",
    "print(f\" Stacked Model Test MAE: {mae_stack:,.2f}\")\n",
    "print(f\" Stacked Model Test RMSE: {rmse_stack:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8832e57-57a1-4b49-bf92-abfab09e95c3",
   "metadata": {},
   "source": [
    "\n",
    "Ridge Stacking result\n",
    "Stacked Model MAE: 1,643,528.25\n",
    "Stacked Model RMSE: 3,825,765.41\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c874a6-8841-4f4d-a196-d7c6d0c12ef7",
   "metadata": {},
   "source": [
    "### Try other meta-learners - GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92ed41-9eee-49af-967e-f0128a99d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learner\n",
    "meta_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Final prediction\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluation\n",
    "mae_stack = mean_absolute_error(y_seq_test, y_pred_stack)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_seq_test, y_pred_stack))\n",
    "\n",
    "print(f\" Stacked Model Test MAE: {mae_stack:,.2f}\")\n",
    "print(f\" Stacked Model Test RMSE: {rmse_stack:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb480d11-fab6-4d84-82e5-c3730a0b25a5",
   "metadata": {},
   "source": [
    "Stacked Model Test MAE: 1,660,585.16\n",
    "Stacked Model Test RMSE: 3,886,773.34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30579e12-b18c-417e-8f17-45a090662b8d",
   "metadata": {},
   "source": [
    "### Try other meta-learners - XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554457bb-6446-4709-bed8-33f60753ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learner\n",
    "meta_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Final prediction\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluation\n",
    "mae_stack = mean_absolute_error(y_seq_test, y_pred_stack)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_seq_test, y_pred_stack))\n",
    "\n",
    "print(f\" Stacked Model Test MAE: {mae_stack:,.2f}\")\n",
    "print(f\" Stacked Model Test RMSE: {rmse_stack:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5b2cd79-7c1d-45a3-9a8c-1b64e189531f",
   "metadata": {},
   "source": [
    " Stacked Model Test MAE: 1,652,659.50\n",
    " Stacked Model Test RMSE: 3,810,673.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4322a-2e19-4421-8b08-6ec28925ec7e",
   "metadata": {},
   "source": [
    "### Improving Ridge stacker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884a910-c2c3-4e86-88ba-c2c06738ed2a",
   "metadata": {},
   "source": [
    "#### Generate OOF predictions and train Ridge stacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43017a0-804b-4142-a770-68cb1651ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "SEQ_LEN = X_num_train.shape[1]\n",
    "input_shape = (SEQ_LEN, X_num_train.shape[2]) \n",
    "n_branches = int(X_branch_train.max()) + 1\n",
    "\n",
    "# Containers\n",
    "oof_preds_lstm = np.zeros(len(y_train_new))\n",
    "oof_preds_tcn = np.zeros(len(y_train_new))\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "print(\"Generating OOF predictions using TimeSeriesSplit...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_num_train)):\n",
    "    print(f\"Fold {fold+1}/{n_splits}\")\n",
    "\n",
    "    # Split data\n",
    "    X_tr_num, X_val_num = X_num_train[train_idx], X_num_train[val_idx]\n",
    "    X_tr_branch, X_val_branch = X_branch_train[train_idx], X_branch_train[val_idx]\n",
    "    y_tr, y_val_part = y_train_new[train_idx], y_train_new[val_idx]\n",
    "\n",
    "    # Build and fit LSTM\n",
    "    lstm_model = build_bilstm_attention_with_normalization(input_shape, n_branches, best_params)\n",
    "    lstm_model.fit([X_tr_num, X_tr_branch], y_tr,\n",
    "                   validation_data=([X_val_num, X_val_branch], y_val_part),\n",
    "                   epochs=10, batch_size=best_params.get(\"batch_size\", 64),\n",
    "                   callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "                   verbose=0, shuffle=False)\n",
    "\n",
    "    oof_preds_lstm[val_idx] = lstm_model.predict([X_val_num, X_val_branch]).flatten()\n",
    "\n",
    "    # Build and fit TCN\n",
    "    tcn_model = build_dilated_tcn_attention_model(best_params, input_shape, (1,), n_branches)\n",
    "    tcn_model.fit([X_tr_num, X_tr_branch], y_tr,\n",
    "                  validation_data=([X_val_num, X_val_branch], y_val_part),\n",
    "                  epochs=10, batch_size=64,\n",
    "                  callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "                  verbose=0, shuffle=False)\n",
    "\n",
    "    oof_preds_tcn[val_idx] = tcn_model.predict([X_val_num, X_val_branch]).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550d444-d3d5-4206-a842-9bb065d17e7a",
   "metadata": {},
   "source": [
    "#### Build Meta Features, Train Ridge & Predict on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfb9a7-8f32-4e7a-a315-1491736c7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta features for training meta-model\n",
    "X_meta_train = np.vstack((oof_preds_lstm, oof_preds_tcn)).T\n",
    "y_meta_train = y_train_new\n",
    "\n",
    "\n",
    "# Retrain full base models on all training data\n",
    "lstm_model = build_bilstm_attention_with_normalization(input_shape, n_branches, best_params)\n",
    "lstm_model.fit([X_num_train, X_branch_train], y_train_new,\n",
    "               epochs=15, batch_size=best_params.get(\"batch_size\", 64),\n",
    "               callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "               verbose=0, shuffle=False)\n",
    "\n",
    "tcn_model = build_dilated_tcn_attention_model(best_params, input_shape, (1,), n_branches)\n",
    "tcn_model.fit([X_num_train, X_branch_train], y_train_new,\n",
    "              epochs=15, batch_size=64,\n",
    "              callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "              verbose=0, shuffle=False)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lstm_test = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn_test = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "X_meta_test = np.vstack((y_pred_lstm_test, y_pred_tcn_test)).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1e79b-c618-49fc-8812-abec95b8f749",
   "metadata": {},
   "source": [
    "#### Train Ridge Meta-Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700227a-0aeb-4369-ac01-a7ee3d821f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "y_pred_stack = meta_model.predict(X_meta_test)\n",
    "\n",
    "mae_stack = mean_absolute_error(y_seq_test, y_pred_stack)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_seq_test, y_pred_stack))\n",
    "\n",
    "print(f\"\\n Stacked Model Test MAE: {mae_stack:,.2f}\")\n",
    "print(f\" Stacked Model Test RMSE: {rmse_stack:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0575f9f-a9dd-4ab7-bc15-383091b4bf23",
   "metadata": {},
   "source": [
    "Stacked Model Test MAE: 1,725,254.69\n",
    "Stacked Model Test RMSE: 3,850,346.80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb73ce-3882-4125-a237-a7f9f93ab673",
   "metadata": {},
   "source": [
    "### Enrich Meta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2acd9-1e73-4afb-b635-7b18bb916cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Enriched Meta-Feature Set\n",
    "\n",
    "# Train meta features\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Test meta features\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2e90c-6179-490c-9cfb-fafdfd3209e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Ridge Meta Model\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "print(f\" Enriched Stacked MAE: {mae:,.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "print(f\" Enriched Stacked RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d083dfc9-e51f-43a8-8f33-30a136c8b8c0",
   "metadata": {},
   "source": [
    " Enriched Stacked MAE: 1,732,525.00\n",
    " Enriched Stacked RMSE: 3,849,359.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6de15-8ca1-401a-a258-81616653b60f",
   "metadata": {},
   "source": [
    "### Replace Ridge with XGBoost - 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1abff-bb83-4480-aae1-88ebeaea714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train meta features\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Test meta features\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Fit XGBoost meta model\n",
    "meta_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "print(f\" XGBoost Stacked MAE: {mae:,.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "print(f\" XGBoost Stacked RMSE: {rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9c282f4-d5f8-44ee-85eb-432ff1d88178",
   "metadata": {},
   "source": [
    " XGBoost Stacked MAE: 1,850,227.88\n",
    " XGBoost Stacked RMSE: 3,943,966.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed65bf-cada-4699-967a-c9a56c842f53",
   "metadata": {},
   "source": [
    "### More meta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af4dfb-f512-4ba1-89de-c95833532b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Features (Train) — OOF predictions only\n",
    "\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,                                  # Model 1 OOF\n",
    "    oof_preds_tcn,                                   # Model 2 OOF\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),          # Absolute difference\n",
    "    oof_preds_lstm + oof_preds_tcn,                  # Sum of predictions\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2,            # Mean prediction\n",
    "    np.std(np.stack([oof_preds_lstm, oof_preds_tcn], axis=0), axis=0),  # Std dev\n",
    "])\n",
    "\n",
    "\n",
    "# Meta Features (Test) — NO use of y_test\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm,                                     # Model 1 prediction\n",
    "    y_pred_tcn,                                      # Model 2 prediction\n",
    "    np.abs(y_pred_lstm - y_pred_tcn),                # Absolute difference\n",
    "    y_pred_lstm + y_pred_tcn,                        # Sum of predictions\n",
    "    (y_pred_lstm + y_pred_tcn) / 2,                  # Mean prediction\n",
    "    np.std(np.stack([y_pred_lstm, y_pred_tcn], axis=0), axis=0),  # Std dev\n",
    "])\n",
    "\n",
    "\n",
    "# Fit Ridge meta model\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(X_meta_train, y_train_new)  \n",
    "\n",
    "\n",
    "# Predict and Evaluate\n",
    "\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" Final Enriched Stacked MAE: {mae:,.4f}\")\n",
    "print(f\" Final Enriched Stacked RMSE: {rmse:,.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2182047-7b17-49ee-b0c8-88c0653c04b2",
   "metadata": {},
   "source": [
    "Final Enriched Stacked MAE: 1,691,551.4116\n",
    " Final Enriched Stacked RMSE: 3,826,121.4482"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4bceb-08f8-428b-bfc7-094b56f577b0",
   "metadata": {},
   "source": [
    "### Replace Ridge with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce4b11-6515-472f-9308-3cc0aea3852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Features (Train) — OOF predictions only\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    oof_preds_lstm + oof_preds_tcn,\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2,\n",
    "    np.std(np.stack([oof_preds_lstm, oof_preds_tcn], axis=0), axis=0),\n",
    "])\n",
    "\n",
    "\n",
    "# Meta Features (Test)\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm,\n",
    "    y_pred_tcn,\n",
    "    np.abs(y_pred_lstm - y_pred_tcn),\n",
    "    y_pred_lstm + y_pred_tcn,\n",
    "    (y_pred_lstm + y_pred_tcn) / 2,\n",
    "    np.std(np.stack([y_pred_lstm, y_pred_tcn], axis=0), axis=0),\n",
    "])\n",
    "\n",
    "\n",
    "# Fit XGBoost meta model\n",
    "meta_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "meta_model.fit(X_meta_train, y_train_new)\n",
    "\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" XGBoost Stacked MAE: {mae:,.4f}\")\n",
    "print(f\" XGBoost Stacked RMSE: {rmse:,.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "243f8d2f-ea8b-4af9-8141-d672f4a57c34",
   "metadata": {},
   "source": [
    "XGBoost Stacked MAE: 1,780,747.2500\n",
    " XGBoost Stacked RMSE: 3,895,502.3289"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8de557-e10f-4d6a-a09c-0795b13775ba",
   "metadata": {},
   "source": [
    "### Tune hyperparameters (Replace Ridge with XGBoost - 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f7778-61d7-40db-a024-475de819e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=50\n",
    "\n",
    "\n",
    "# Define meta features (already stacked)\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Define MAE scorer (lower is better)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "# Parameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Cross-validation and tuning\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_meta = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_meta,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=mae_scorer,\n",
    "    cv=tscv,\n",
    "    n_iter=50,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "\n",
    "# Use best estimator to predict\n",
    "best_meta_model = random_search.best_estimator_\n",
    "y_meta_pred = best_meta_model.predict(X_meta_test)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" Tuned XGB Meta MAE: {mae:,.2f}\")\n",
    "print(f\" Tuned XGB Meta RMSE: {rmse:,.2f}\")\n",
    "print(f\"Best Params: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6401d6e6-a256-46d6-b9e0-32ecf5e5196c",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    " Tuned XGB Meta MAE: 1,638,201.12\n",
    " Tuned XGB Meta RMSE: 3,886,833.02\n",
    "Best Params: {'subsample': 0.8, 'reg_lambda': 1.5, 'reg_alpha': 0, 'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b82ab5-c0cd-4911-b726-f308d20d6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=100\n",
    "\n",
    "\n",
    "# Define meta features \n",
    "\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Define MAE scorer (lower is better)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "# Parameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Cross-validation and tuning\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_meta = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_meta,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=mae_scorer,\n",
    "    cv=tscv,\n",
    "    n_iter=100,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "\n",
    "# Use best estimator to predict\n",
    "best_meta_model = random_search.best_estimator_\n",
    "y_meta_pred = best_meta_model.predict(X_meta_test)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" Tuned XGB Meta MAE: {mae:,.2f}\")\n",
    "print(f\" Tuned XGB Meta RMSE: {rmse:,.2f}\")\n",
    "print(f\"Best Params: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4b41897-1b7a-4e77-98d7-01afa124793d",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
    " Tuned XGB Meta MAE: 1,671,062.25\n",
    " Tuned XGB Meta RMSE: 3,873,587.09\n",
    "Best Params: {'subsample': 1.0, 'reg_lambda': 1.0, 'reg_alpha': 0, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4ad89-ed97-4624-b24c-7530e8e97347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=150\n",
    "\n",
    "\n",
    "# Define meta features \n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Define MAE scorer (lower is better)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "\n",
    "# Parameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "\n",
    "# Cross-validation and tuning\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "xgb_meta = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_meta,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=mae_scorer,\n",
    "    cv=tscv,\n",
    "    n_iter=150,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "\n",
    "# Use best estimator to predict\n",
    "best_meta_model = random_search.best_estimator_\n",
    "y_meta_pred = best_meta_model.predict(X_meta_test)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" Tuned XGB Meta MAE: {mae:,.2f}\")\n",
    "print(f\" Tuned XGB Meta RMSE: {rmse:,.2f}\")\n",
    "print(f\"Best Params: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c28ca790-7416-47ad-8f0b-2245256c74a4",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
    " Tuned XGB Meta MAE: 1,637,732.00\n",
    " Tuned XGB Meta RMSE: 3,885,839.31\n",
    "Best Params: {'subsample': 0.8, 'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.01, 'colsample_bytree': 0.6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ea6a8-23cd-45e5-b45a-785068884242",
   "metadata": {},
   "source": [
    "### Grid Search Around Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c639028-b15a-44fb-9cb9-75899cf1bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE scorer\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Focused grid around best randomized params\n",
    "param_grid = {\n",
    "    'n_estimators': [40, 50, 60],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.005, 0.01, 0.02],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7],\n",
    "    'reg_alpha': [0.5, 1.0, 1.5],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Model with fixed seed\n",
    "xgb_meta = XGBRegressor(random_state=42, verbosity=0)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_meta,\n",
    "    param_grid=param_grid,\n",
    "    scoring=mae_scorer,\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit to meta training data\n",
    "grid_search.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Best model\n",
    "best_meta_model_grid = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_meta_pred = best_meta_model_grid.predict(X_meta_test)\n",
    "\n",
    "# Final evaluation\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\"Grid-Tuned XGB Meta MAE: {mae:,.2f}\")\n",
    "print(f\"Grid-Tuned XGB Meta RMSE: {rmse:,.2f}\")\n",
    "print(f\"Best Grid Params: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e9c2d4c-5e9c-438b-aa94-3cca77848e21",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
    "Grid-Tuned XGB Meta MAE: 1,634,566.00\n",
    "Grid-Tuned XGB Meta RMSE: 3,895,192.49\n",
    "Best Grid Params: {'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 40, 'reg_alpha': 0.5, 'reg_lambda': 1.5, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60cccde-3f3a-460e-a0e1-ec8f1582eeab",
   "metadata": {},
   "source": [
    "### optimal XGBoost meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc14b8e-48fb-4d37-8a87-5c494afe74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Enriched Meta-Feature Sets\n",
    "X_meta_train = np.column_stack([\n",
    "    oof_preds_lstm,\n",
    "    oof_preds_tcn,\n",
    "    np.abs(oof_preds_lstm - oof_preds_tcn),\n",
    "    (oof_preds_lstm + oof_preds_tcn) / 2\n",
    "])\n",
    "\n",
    "X_meta_test = np.column_stack([\n",
    "    y_pred_lstm_test,\n",
    "    y_pred_tcn_test,\n",
    "    np.abs(y_pred_lstm_test - y_pred_tcn_test),\n",
    "    (y_pred_lstm_test + y_pred_tcn_test) / 2\n",
    "])\n",
    "\n",
    "\n",
    "# Define and Train Optimal XGB Meta Model\n",
    "best_xgb_meta_model = XGBRegressor(\n",
    "    n_estimators=40,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.5,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.5,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_xgb_meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_meta_pred = best_xgb_meta_model.predict(X_meta_test)\n",
    "\n",
    "mae = mean_absolute_error(y_seq_test, y_meta_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_seq_test, y_meta_pred))\n",
    "\n",
    "print(f\" Optimal XGB Meta MAE: {mae:,.2f}\")\n",
    "print(f\" Optimal XGB Meta RMSE: {rmse:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "210cf6c0-3d67-4c73-a0b2-edfa59b3abf6",
   "metadata": {},
   "source": [
    "Optimal XGB Meta MAE: 1,633,965.38\n",
    "Optimal XGB Meta RMSE: 3,893,956.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c1c5b-8f90-4de7-81c0-029bce69992f",
   "metadata": {},
   "source": [
    "### Save the Optimal Stack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97def981-00c5-4250-b793-c8b18d28ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package everything\n",
    "stack_package = {\n",
    "    \"meta_model\": best_xgb_meta_model,      # Trained meta model\n",
    "    \"oof_preds_lstm\": oof_preds_lstm,       # OOF predictions from LSTM\n",
    "    \"oof_preds_tcn\": oof_preds_tcn,         # OOF predictions from TCN\n",
    "    \"X_meta_train\": X_meta_train,           # Meta features (train)\n",
    "    \"X_meta_test\": X_meta_test,             # Meta features (test)\n",
    "    \"y_meta_train\": y_meta_train,           # Meta target\n",
    "    \"y_seq_test\": y_seq_test,               # True test values\n",
    "    \"y_pred_lstm_test\": y_pred_lstm_test,   # LSTM test predictions\n",
    "    \"y_pred_tcn_test\": y_pred_tcn_test      # TCN test predictions\n",
    "}\n",
    "\n",
    "# Save everything in one file\n",
    "with open(\"dl_best_stack_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stack_package, f)\n",
    "\n",
    "print(\"Everything saved in dl_best_stack_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5dc25-768e-466e-8e59-c43aa22f7e28",
   "metadata": {},
   "source": [
    "### Making the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325b8bd-7d03-4855-bc4d-aa5eb742a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "SEQ_LEN = 60\n",
    "\n",
    "# Feature Selection\n",
    "exclude_cols = [\"TXNDATE\", \"BRANCHID\"]\n",
    "numeric_cols = [col for col in X_train_full.columns if col not in exclude_cols]\n",
    "\n",
    "TOP_N_FEATURES = 50\n",
    "X_temp = X_train_full[numeric_cols].select_dtypes(include=['int64', 'float64', 'bool']).copy()\n",
    "y_temp = y_train\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=SEED)\n",
    "xgb_model.fit(X_temp, y_temp)\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_temp.columns).sort_values(ascending=False)\n",
    "top_features = feat_imp.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "print(f\"Selected Top {TOP_N_FEATURES} Features:\\n\", top_features)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full[top_features])\n",
    "X_test_scaled = scaler.transform(X_test_full[top_features])\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=top_features)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=top_features)\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(X_df, y_series, branch_series, window):\n",
    "    X_numeric_seq, X_branch_seq, y_seq = [], [], []\n",
    "    for idx in range(window, len(X_df)):\n",
    "        X_numeric_seq.append(X_df.iloc[idx - window:idx].values)\n",
    "        X_branch_seq.append(branch_series.iloc[idx])\n",
    "        y_seq.append(y_series.iloc[idx])\n",
    "    return (\n",
    "        np.array(X_numeric_seq).astype(np.float32),\n",
    "        np.array(X_branch_seq).astype(np.int32).reshape(-1, 1),\n",
    "        np.array(y_seq).astype(np.float32)\n",
    "    )\n",
    "\n",
    "n_branches = int(X_train_full[\"BRANCHID\"].max()) + 1\n",
    "\n",
    "X_num_seq, X_branch_seq, y_seq = create_sequences(\n",
    "    X_train_scaled.reset_index(drop=True),\n",
    "    y_train.reset_index(drop=True),\n",
    "    X_train_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_seq_test, X_branch_seq_test, y_seq_test = create_sequences(\n",
    "    X_test_scaled.reset_index(drop=True),\n",
    "    #y_test.reset_index(drop=True),\n",
    "    pd.Series(y_test).reset_index(drop=True),\n",
    "    X_test_full[\"BRANCHID\"].reset_index(drop=True),\n",
    "    window=SEQ_LEN\n",
    ")\n",
    "\n",
    "X_num_train, X_num_test, X_branch_train, X_branch_test, y_train_new, y_test = train_test_split(\n",
    "    X_num_seq, X_branch_seq, y_seq, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "input_shape_num = X_num_seq.shape[1:]\n",
    "input_shape_branch = X_branch_seq.shape[1:]\n",
    "vocab_size = int(np.max(X_branch_seq)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3581701-751e-40c6-bfbb-3356902f288a",
   "metadata": {},
   "source": [
    "#### Loading the optimal stack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00d106-f5de-4cd4-9ad1-d631ecf1ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the package\n",
    "with open(\"dl_best_stack_model.pkl\", \"rb\") as f:\n",
    "    loaded_stack = pickle.load(f)\n",
    "\n",
    "meta_model_loaded = loaded_stack[\"meta_model\"]\n",
    "X_meta_test_loaded = loaded_stack[\"X_meta_test\"]\n",
    "y_seq_test_loaded = loaded_stack[\"y_seq_test\"]\n",
    "\n",
    "# Predict with loaded meta model\n",
    "y_meta_pred_loaded = meta_model_loaded.predict(X_meta_test_loaded)\n",
    "\n",
    "# Evaluate\n",
    "mae_loaded = mean_absolute_error(y_seq_test_loaded, y_meta_pred_loaded)\n",
    "rmse_loaded = np.sqrt(mean_squared_error(y_seq_test_loaded, y_meta_pred_loaded))\n",
    "\n",
    "print(f\"Loaded Meta Model MAE: {mae_loaded:,.2f}\")\n",
    "print(f\"Loaded Meta Model RMSE: {rmse_loaded:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f58733a-b0d9-4792-933b-271ead4d0c48",
   "metadata": {},
   "source": [
    "\n",
    "Final Stacking model\n",
    "    Meta Model MAE: 1,633,965.38\n",
    "    Meta Model RMSE: 3,893,956.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba77076-5c3f-49e1-bcd0-51a65ff47395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved models\n",
    "\n",
    "# LSTM & TCN models\n",
    "with open(\"dl_best_blend_model.pkl\", \"rb\") as f:\n",
    "    blend_package = pickle.load(f)\n",
    "\n",
    "lstm_model = blend_package[\"lstm_model\"]\n",
    "tcn_model = blend_package[\"tcn_model\"]\n",
    "best_blend_weight = blend_package[\"best_weight\"]\n",
    "\n",
    "# Stack model\n",
    "with open(\"dl_best_stack_model.pkl\", \"rb\") as f:\n",
    "    stack_package = pickle.load(f)\n",
    "\n",
    "stack_model = stack_package[\"meta_model\"]\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "\n",
    "# Predict with LSTM & TCN\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# Predict with Blend\n",
    "y_pred_blend = best_blend_weight * y_pred_lstm + (1 - best_blend_weight) * y_pred_tcn\n",
    "\n",
    "# Predict with Stack\n",
    "X_meta_test_loaded = stack_package[\"X_meta_test\"]  \n",
    "y_pred_stack = stack_model.predict(X_meta_test_loaded)\n",
    "\n",
    "# Ground truth\n",
    "y_test = stack_package[\"y_seq_test\"]\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "# Evaluate each model\n",
    "results = {\n",
    "    \"Model\": [\"LSTM\", \"TCN\", \"Weighted Blend\", \"Stacked Model\"],\n",
    "    \"RMSE\": [],\n",
    "    \"MAE\": [],\n",
    "   \n",
    "}\n",
    "\n",
    "for preds in [y_pred_lstm, y_pred_tcn, y_pred_blend, y_pred_stack]:\n",
    "    rmse, mae, r2 = evaluate_model(y_test, preds)\n",
    "    results[\"RMSE\"].append(f\"{rmse:,.2f}\")\n",
    "    results[\"MAE\"].append(f\"{mae:,.2f}\")\n",
    "   \n",
    "\n",
    "\n",
    "# Create summary table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4dacd0b-726f-4d5c-a90a-970973af07e5",
   "metadata": {},
   "source": [
    "Model Performance Summary:\n",
    "         Model         RMSE          MAE\n",
    "          LSTM 3,828,927.43 1,624,436.00\n",
    "           TCN 3,856,614.71 1,658,053.50\n",
    "Weighted Blend 3,830,167.53 1,623,555.47\n",
    " Stacked Model 3,893,956.28 1,633,965.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064dd7d-c897-47fe-b670-8f3bab45fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved blend model (contains LSTM, TCN, and best weight)\n",
    "with open(\"dl_best_blend_model.pkl\", \"rb\") as f:\n",
    "    blend_package = pickle.load(f)\n",
    "\n",
    "lstm_model = blend_package[\"lstm_model\"]\n",
    "tcn_model = blend_package[\"tcn_model\"]\n",
    "best_blend_weight = blend_package[\"best_weight\"]\n",
    "\n",
    "# Load saved stack model (contains meta_model, X_meta_test, and y_seq_test)\n",
    "with open(\"dl_best_stack_model.pkl\", \"rb\") as f:\n",
    "    stack_package = pickle.load(f)\n",
    "\n",
    "stack_model = stack_package[\"meta_model\"]\n",
    "X_meta_test_loaded = stack_package[\"X_meta_test\"]\n",
    "y_seq_test = stack_package[\"y_seq_test\"]\n",
    "\n",
    "# Ensure TXNDATE is datetime and reset index\n",
    "test_df = test_df.copy()\n",
    "test_df[\"TXNDATE\"] = pd.to_datetime(test_df[\"TXNDATE\"])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Predict with base models\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# Weighted blended prediction\n",
    "y_pred_blend = best_blend_weight * y_pred_lstm + (1 - best_blend_weight) * y_pred_tcn\n",
    "\n",
    "# Stacked prediction\n",
    "y_pred_stack = stack_model.predict(X_meta_test_loaded)\n",
    "\n",
    "# Align test_df by dropping first SEQ_LEN rows (sequence window)\n",
    "test_df_seq = test_df.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "\n",
    "# Confirm lengths match\n",
    "assert len(test_df_seq) == len(y_seq_test), f\"Mismatch: {len(test_df_seq)} vs {len(y_seq_test)}\"\n",
    "\n",
    "# Merge predictions into aligned DataFrame\n",
    "test_df_seq[\"Actual\"] = y_seq_test\n",
    "test_df_seq[\"LSTM_Pred\"] = y_pred_lstm\n",
    "test_df_seq[\"TCN_Pred\"] = y_pred_tcn\n",
    "test_df_seq[\"Blended_Pred\"] = y_pred_blend\n",
    "test_df_seq[\"Stacked_Pred\"] = y_pred_stack\n",
    "\n",
    "# Plot for a specific BranchID\n",
    "branch_id = 21 \n",
    "branch_df = test_df_seq[test_df_seq[\"BRANCHID\"] == branch_id].sort_values(\"TXNDATE\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\", linewidth=2)\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"LSTM_Pred\"], label=\"LSTM\", color=\"dodgerblue\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"TCN_Pred\"], label=\"TCN\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Blended_Pred\"], label=\"Blended\", color=\"purple\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Stacked_Pred\"], label=\"Stacked\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.title(f\"Actual vs Predicted — BranchID = {branch_id}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0498e-7171-4246-b858-0fbf2acf665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved blend model (contains LSTM, TCN, and best weight)\n",
    "with open(\"dl_best_blend_model.pkl\", \"rb\") as f:\n",
    "    blend_package = pickle.load(f)\n",
    "\n",
    "lstm_model = blend_package[\"lstm_model\"]\n",
    "tcn_model = blend_package[\"tcn_model\"]\n",
    "best_blend_weight = blend_package[\"best_weight\"]\n",
    "\n",
    "# Load saved stack model (contains meta_model, X_meta_test, and y_seq_test)\n",
    "with open(\"dl_best_stack_model.pkl\", \"rb\") as f:\n",
    "    stack_package = pickle.load(f)\n",
    "\n",
    "stack_model = stack_package[\"meta_model\"]\n",
    "X_meta_test_loaded = stack_package[\"X_meta_test\"]\n",
    "y_seq_test = stack_package[\"y_seq_test\"]\n",
    "\n",
    "# Ensure TXNDATE is datetime and reset index\n",
    "test_df = test_df.copy()\n",
    "test_df[\"TXNDATE\"] = pd.to_datetime(test_df[\"TXNDATE\"])\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Predict with base models\n",
    "y_pred_lstm = lstm_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "y_pred_tcn = tcn_model.predict([X_num_seq_test, X_branch_seq_test]).flatten()\n",
    "\n",
    "# Weighted blended prediction\n",
    "y_pred_blend = best_blend_weight * y_pred_lstm + (1 - best_blend_weight) * y_pred_tcn\n",
    "\n",
    "# Stacked prediction\n",
    "y_pred_stack = stack_model.predict(X_meta_test_loaded)\n",
    "\n",
    "# Align test_df by dropping first SEQ_LEN rows (sequence window)\n",
    "test_df_seq = test_df.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "\n",
    "# Confirm lengths match\n",
    "assert len(test_df_seq) == len(y_seq_test), f\"Mismatch: {len(test_df_seq)} vs {len(y_seq_test)}\"\n",
    "\n",
    "# Merge predictions into aligned DataFrame\n",
    "test_df_seq[\"Actual\"] = y_seq_test\n",
    "test_df_seq[\"LSTM_Pred\"] = y_pred_lstm\n",
    "test_df_seq[\"TCN_Pred\"] = y_pred_tcn\n",
    "test_df_seq[\"Blended_Pred\"] = y_pred_blend\n",
    "test_df_seq[\"Stacked_Pred\"] = y_pred_stack\n",
    "\n",
    "# Plot for a specific BranchID with date range\n",
    "branch_id = 21  \n",
    "start_date = \"2025-03-04\"   \n",
    "end_date   = \"2025-03-10\"   \n",
    "\n",
    "branch_df = test_df_seq[test_df_seq[\"BRANCHID\"] == branch_id].copy()\n",
    "branch_df = branch_df.sort_values(\"TXNDATE\")\n",
    "\n",
    "# Filter by date range\n",
    "mask = (branch_df[\"TXNDATE\"] >= pd.to_datetime(start_date)) & (branch_df[\"TXNDATE\"] <= pd.to_datetime(end_date))\n",
    "branch_df = branch_df.loc[mask]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Actual\"], label=\"Actual\", color=\"black\", linewidth=2)\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"LSTM_Pred\"], label=\"LSTM\", color=\"dodgerblue\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"TCN_Pred\"], label=\"TCN\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Blended_Pred\"], label=\"Blended\", color=\"purple\", linestyle=\"--\")\n",
    "plt.plot(branch_df[\"TXNDATE\"], branch_df[\"Stacked_Pred\"], label=\"Stacked\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.title(f\"Actual vs Predicted — BranchID = {branch_id}\\n({start_date} to {end_date})\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"NetCashFlow\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace8165-ec12-4e80-a6de-a78f591ba5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: specify the date and branch to inspect\n",
    "query_date = pd.to_datetime(\"2025-03-07\")  \n",
    "branch_id = 21 \n",
    "\n",
    "# Filter aligned test DataFrame with sequence predictions\n",
    "subset = test_df_seq[\n",
    "    (test_df_seq[\"BRANCHID\"] == branch_id) &\n",
    "    (test_df_seq[\"TXNDATE\"] == query_date)\n",
    "]\n",
    "\n",
    "if subset.empty:\n",
    "    print(f\"No data found for BranchID={branch_id} on {query_date.date()}\")\n",
    "else:\n",
    "    # Extract scalar values (actual and predictions)\n",
    "    actual = subset[\"Actual\"].values[0]\n",
    "    lstm_pred = subset[\"LSTM_Pred\"].values[0]\n",
    "    tcn_pred = subset[\"TCN_Pred\"].values[0]\n",
    "    blended_pred = subset[\"Blended_Pred\"].values[0]\n",
    "    stacked_pred = subset[\"Stacked_Pred\"].values[0]\n",
    "\n",
    "    # Prepare labels and values\n",
    "    labels = [\"Actual\", \"LSTM\", \"TCN\", \"Blended\", \"Stacked\"]\n",
    "    values = [actual, lstm_pred, tcn_pred, blended_pred, stacked_pred]\n",
    "    colors = [\"black\", \"dodgerblue\", \"green\", \"purple\", \"red\"]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Plot points with legend and value annotations\n",
    "    for label, val, color in zip(labels, values, colors):\n",
    "        plt.scatter(label, val, color=color, s=150, label=label)\n",
    "        plt.text(label, val, f\"{val:,.0f}\", ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.title(f\"NetCashFlow on {query_date.date()} for BranchID {branch_id}\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9d214-1a92-46d0-92bd-6eaafb2ecf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: specify the date and branch to inspect\n",
    "query_date = pd.to_datetime(\"2025-01-24\")  \n",
    "branch_id = 21 \n",
    "\n",
    "# Filter aligned test DataFrame with sequence predictions\n",
    "subset = test_df_seq[\n",
    "    (test_df_seq[\"BRANCHID\"] == branch_id) &\n",
    "    (test_df_seq[\"TXNDATE\"] == query_date)\n",
    "]\n",
    "\n",
    "if subset.empty:\n",
    "    print(f\"No data found for BranchID={branch_id} on {query_date.date()}\")\n",
    "else:\n",
    "    # Extract scalar values (actual and predictions)\n",
    "    actual = subset[\"Actual\"].values[0]\n",
    "    lstm_pred = subset[\"LSTM_Pred\"].values[0]\n",
    "    tcn_pred = subset[\"TCN_Pred\"].values[0]\n",
    "    blended_pred = subset[\"Blended_Pred\"].values[0]\n",
    "    stacked_pred = subset[\"Stacked_Pred\"].values[0]\n",
    "\n",
    "    # Prepare labels and values\n",
    "    labels = [\"Actual\", \"LSTM\", \"TCN\", \"Blended\", \"Stacked\"]\n",
    "    values = [actual, lstm_pred, tcn_pred, blended_pred, stacked_pred]\n",
    "    colors = [\"black\", \"dodgerblue\", \"green\", \"purple\", \"red\"]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Plot points with legend and value annotations\n",
    "    for label, val, color in zip(labels, values, colors):\n",
    "        plt.scatter(label, val, color=color, s=150, label=label)\n",
    "        plt.text(label, val, f\"{val:,.0f}\", ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    plt.title(f\"NetCashFlow on {query_date.date()} for BranchID {branch_id}\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d06795-c15d-49e3-ab50-210b99317097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a583ecb-9cf8-4b7c-b3d9-480c8a059472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
